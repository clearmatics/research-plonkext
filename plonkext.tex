% !TeX spellcheck = en_US
\let\accentvec\vec \documentclass[runningheads,10pt]{llncs}
 % \documentclass[runningheads]{amsart}
\let\spvec\vec \let\vec\accentvec

\usepackage{amssymb,amsmath} \let\vec\spvec \usepackage{lmodern}

\usepackage[T1]{fontenc}

\newcommand{\iflipics}[1] {} \newcommand{\iflncs}[1] {#1}

\def\vec#1{\mathchoice{\mbox{\boldmath$\displaystyle#1$}}
{\mbox{\boldmath$\textstyle#1$}} {\mbox{\boldmath$\scriptstyle#1$}}
{\mbox{\boldmath$\scriptscriptstyle#1$}}}

\DeclareFontFamily{U}{mathx}{\hyphenchar\font45}
\DeclareFontShape{U}{mathx}{m}{n}{<-> mathx10}{}
\DeclareSymbolFont{mathx}{U}{mathx}{m}{n}
\DeclareMathAccent{\widebar}{0}{mathx}{"73}

% lncs size (as printed in books, with small margins):
\usepackage[paperheight=23.5cm,paperwidth=15.5cm,text={13.2cm,20.3cm},centering]{geometry}
% \usepackage{fullpage}

\newcommand{\ifamsart}[1] {} \ifamsart{ \newtheorem{theorem}{Theorem}%[section]
		\newtheorem{proposition}[theorem]{Proposition}
		\newtheorem{lemma}[theorem]{Lemma}
		\newtheorem{corollary}[theorem]{Corollary} \theoremstyle{definition}
		\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example} } \usepackage{soul} \usepackage{soulutf8}
\soulregister\cite7 \soulregister\ref7 \soulregister\pageref7
\usepackage{hyperref} \usepackage[color=yellow]{todonotes} \hypersetup{final}
\usepackage{mathrsfs}
\usepackage[advantage,asymptotics,adversary,sets,keys,ff,lambda,primitives,events,operators,probability,logic,mm,complexity]{cryptocode}
%\pcbodylinesep=0.15\baselineskip % MK: got an undefined control sequence error

\usepackage[capitalise]{cleveref}
\crefname{appendix}{Supp.~Mat.}{Sup.~Mat.}
\Crefname{appendix}{Supp.~Mat.}{Sup.~Mat.}
\usepackage{cite} 
\usepackage{booktabs}
\usepackage{paralist}
\usepackage[innerleftmargin=5pt,innerrightmargin=5pt]{mdframed}
%\usepackage{setspace}
\usepackage{caption}
\captionsetup{belowskip=0pt}
%\captionsetup[figure]{font={stretch=2}}
% \usepackage{subcaption}
\usepackage{bm}
\usepackage{url}
\usepackage{dirtytalk}
\include{macros}

\title{On Simulation-Extractability of Universal zkSNARKs}

\author{Markulf Kohlweiss\inst{1,2} \and Michał Zając\inst{3}} \iflncs{
\institute{University of Edinburgh, Edinburgh, UK \and IOHK \\
\email{mkohlwei@inf.ed.ac.uk} \and Clearmatics, London UK \\
\email{m.p.zajac@gmail.com}} }

\allowdisplaybreaks

\begin{document} \sloppy \maketitle

\begin{abstract} 
	In this paper we show that a wide class of (computationally) special-sound
	proofs of knowledge which have unique response property and are
	standard-model zero-knowledge are (non-black-box) simulation-extractable
	when made non-interactive by the Fiat--Shamir transform.  This allows us to
	prove that that two efficient updatable universal
	zkSNARK---\plonk{}~\cite{EPRINT:GabWilCio19} and
	$\sonic$~\cite{CCS:MBKM19}---meet these requirements and conclude by
	showing their simulation-extractability.  As a side result we also show that
	relying security on rewinding and Fiat--Shamir transform often comes at a
	great price of inefficient (yet still polynomial time) knowledge extraction
	and the security loss introduced by these techniques should always be taken
	into account. 
\end{abstract}

\section{Introduction} 
\subsection{Motivation} 
\paragraph{The rise of updatable
zkSNARKs.} 
Recent years shown indisputable progress in building efficient zero-knowledge
proof systems,
e.g.~\cite{AC:Groth10a,TCC:Lipmaa12,EC:GGPR13,SP:PHGR13,AC:Lipmaa13,AC:DFGK14,EC:Groth16,SP:BBBPWM18}
to name a few. Special attention has been devoted to design new
\emph{zero-knowledge succinct non-interactive arguments of knowledge} (zkSNARKs) which are
especially useful for their short (usually constant-length) proofs.
That property makes zkSNARKs especially useful in real-life system deployment,
e.g.~\cite{REPO:Zcash20,ARXIV:RonZaj19,REPO:Zeth20,REPO:Celo20,REPO:Aztec20}.
This is turn risen a question---\emph{Are zkSNARKs' security models useful in
the real life?} This question becomes especially important when one realises that all
zkSNARKs with constant-sized proofs are shown secure in the common
reference string (CRS) model where the CRS used by the parties---i.e.~a prover
that shows veracity of a statement using its private input called
\emph{witness} and a verifier that verifies it---comes with a trapdoor which
can be used to break security of the scheme. That is, convince the verifier to
accept a false statement or a statement which the prover does not
know the witness for. Hence before a real-life deployment of zkSNARKs one has
to answer a number of questions---\emph{How can the parties be sure that the
trapdoor has not leaked?} \emph{Who is the party that generates the CRS?}
\emph{Can this party be trusted?} The last question has to be answered
positively, as otherwise the CRS-generating party may leak the trapdoor. The
questions are especially important in zero knowledge proofs used in
distributed systems, like e.g.~blockchains, where assuming that a trusted
party generates the CRS may be a step too far and subvert the whole purpose of
a decentralised system. 

One of the first who tackled this question were Bellare et al.~\cite{AC:BelFucSca16} who
proposed notions of \emph{subversion zero knowledge} and \emph{subversion
soundness}. The former states that zero knowledge property persists even when
the CRS is produced by a malicious party. The latter states that the proof
system remains sound in the same situation. Importantly, Bellare et al.~shown
that no system can be simultaneously subversion zero-knowledge and subversion-sound.
Abdolmaleki et al.~\cite{AC:ABLZ17}, and independently Fuchsbauer
\cite{PKC:Fuchsbauer18}, shown that the most
efficient zkSNARK for QAP\footnote{QAP stands for Quadratic Arithmetic
Program. QAP is currently the most efficient representation of arithmetic
circuits for showing their validity.} by Groth \cite{EC:Groth16} can be made
subversion zero-knowledge by making minor modifications to its CRS and without
sacrificing its efficiency.

Although efficient, Groth's zkSNARK comes with a drawback---the CRS is
relation-dependent. That is, if one wants to show that two different arithmetic circuits
have been evaluated correctly, it has to do that using two different CRS-s. 
% We say that such a zkSNARK is not universal. 
Since CRS generation is a
troublesome process, it is desired to have it performed once, not every
time a new circuit validity has to be shown. zkSNARKs that utilise a single
CRS for all circuits of given size are called \emph{universal}. One of the first universal
zkSNARK was proposed by Groth et al.~\cite{C:GKMMM18}. This particular proof
system also introduced a novel security property called \emph{updatable
soundness}.
Because of the  \cite{AC:BelFucSca16} impossibility result, one cannot wish for a
system that is simultaneously subversion zero-knowledge and subversion-sound.
Groth et al.~work around this problem by proposing a notion which allows
zkSNARK provers and verifiers to \emph{update} the CRS, i.e.~to take a CRS and modify it
in a well-defined and verifiable way to obtain a new CRS. Updates guarantee 
that if at least one of the CRS-updating
parties is honest then the proof system is (knowledge) sound. Although inefficient, as the CRS length
is quadratic to the size of the proven statements, \cite{C:GKMMM18} set a new
paradigm for designing zkSNARKs.

The first zkSNARK that had updatable and short CRS and was universal was
$\sonic$ proposed by Maller et al.~in \cite{CCS:MBKM19} which was later
made more efficient by Gabizon in \cite{EPRINT:Gabizon19c}. Eventually, Gabizon et
al.~designed $\plonk$ \cite{EPRINT:GabWilCio19} which currently is the
most efficient updatable universal zkSNARK. Independently, Chiesa et
al.~\cite{EC:CHMMVW20} proposed $\textsf{Marlin}$ which efficiency
is comparable to $\plonk$'s.
All these protocols utilise strong cryptographic assumptions like the
algebraic group model (AGM) and random oracle model (ROM) to show their
security. They also use their own representation of circuits instead of the
``standard'' QAP. However, these protocols are also one of the  most interesting schemes
for practitioners for their practicality which comes from efficient
proving and verification, and universality of the CRS, as well as for their
security model that diminishes the requirement of the trusted party to provide
a CRS.

% In this paper we provide a framework which shows that any zero-knowledge proof
% that fulfils some well-defined requirements is simulation-extractable. To show
% that the set of matching protocols is not empty we prove that both $\plonk$
% and $\sonic$ are simulation-extractable. 

\paragraph{On the importance of the simulation extractability.}
Although zkSNARKs satisfy the knowledge soundness definition,
simulation-extractability (SE) is the property that should be required from
zkSNARKs used in practice, i.e.~as efficient zero-knowledge proofs deployed in
the real-life cryptosystems. This is since in the real life one simply cannot
assume that the adversary who tries to break security of the system does not
have access to any proofs provided by other parties using the same
zero-knowledge system. On the contrary, in the most popular applications of
zkSNARKs, like privacy-preserving blockchains, proofs made by all
blockchain-participants are public. Thus, it is only reasonable to require 
a zero-knowledge proof system to be resilient to attacks that utilise proofs
generated by different parties.

There are many results on simulation extractability for
non-interactive zero-knowledge proofs (NIZKs). First, Groth \cite{AC:Groth07}
noticed that a (black-box) simulation extractable NIZK is
universally-composable (UC) 
\cite{EPRINT:Canetti00}. Then Dodis et al.~\cite{AC:DHLW10} introduced a
notion of (black-box) \emph{true simulation extractability} and showed that no
NIZK can be UC-secure if it does not have this property. 
In the context of zkSNARKs it is important to mention such works as the first
simulation-extractable zkSNARK by Groth and Maller \cite{C:GroMal17} and SE
zkSNARK for QAP by Lipmaa \cite{EPRINT:Lipmaa19a}. 
Kosba's et
al.~\cite{EPRINT:KZMQCP15} shown a general transformation from a NIZK to a
black-box SE NIZK. Although their transformation works for zkSNARKs as well,
succinctness of the proof system is not preserved as the statement's witness is encrypted.
Recently, Abdolmaleki et al.~\cite{CCS:AbdRamSla20} shown another transformation that
obtains non-black-box simulation extractability but also preserves
succinctness of the argument.

Independently, some authors focused on obtaining simulation extractability of
known zkSNARKs, like $\groth$ \cite{EC:Groth16}, by introducing minor modifications and using
stronger assumptions \cite{EPRINT:BowGab18,EPRINT:AtaBag19}. Interestingly,
although such modifications hurt performance of the proof system, the resulting
zkSNARKs are still more efficient than the first SE zkSNARK \cite{C:GroMal17},
see \cite{EPRINT:AtaBag19}. Recently, \cite{EPRINT:BKSV20} shown that the
original Groth's proof system from \cite{EC:Groth16} is weakly SE and
randomisable.  

\paragraph{State of the art---simulation-extractable updatable universal zkSNARKs.} 
Up to our best knowledge, there are zkSNARKs that are simulation-extractable
and zkSNARKs that are universal, however there are no known zkSNARKs that enjoy
both of these properties out-of-the-box. Obviously, given a universal zkSNARK
one could lift it to be simulation-extractable using techniques described
e.g.~in \cite{EPRINT:KZMQCP15,EPRINT:AbdRamSla20}, but such lift comes with
inevitably efficiency loss.  On the other hand, there is no known lift that
would take a zkSNARK and make it universal. Since updatable zkSNARKs are quite
restrictive regarding format of their CRSs, making such lift seems as a very
difficult task. 
The same applies for updatable zkSNARKs. No SE updatable zkSNARKs are
currently known and there is no transformation that could take a SE zkSNARK
and make it updatable.

\subsection{Our contribution}
First of all, we show that a class of computationally special-sound proofs of
knowledge that are zero-knowledge in the standard model and has unique
response property \emph{is simulation extractable out-of-the box} when made
non-interactive by the Fiat--Shamir transform. Although this problem has been
already tackled by Faust et al.~\cite{INDOCRYPT:FKMV12}, we extend it to a
much wider class of protocols, e.g.~our result is not restricted to
$3$-message protocols only.

To show that our result is useful we prove that one of the most efficient updatable and universal
zkSNARKs---$\plonk$ and $\sonic$---are simulation-extractable.
Although we do not change these protocols at all, we had to solve a number of
problems in order to show that which are explained below. 

Before we continue, we note that \plonk{} and \sonic{}---as originally
presented in \cite{EPRINT:GabWilCio19} and \cite{CCS:MBKM19}---are interactive
proofs of knowledge 
made non-interactive by the Fiat--Shamir transform.  In the following, we denote the underlying
interactive protocols by $\plonkprot$ (for $\plonk$) and $\sonicprot$ (for $\sonic$) and
the resulting, non-interactive ones, by $\plonkprotfs$ and $\sonicprotfs$,
respectively. 

\subsubsection{Special soundness.} 
To be able to follow \cite{INDOCRYPT:FKMV12} it had to be shown that
$\plonkprot$ and $\sonicprot$ are special-sound.  However the standard definition of special
soundness could not be met.  First of
all, the  definition requires extraction of a witness from any two
transcripts, each containing three messages and that share the first message. For 
$\plonkprot$ and $\sonicprot$ that gives nothing. The definition had to be tuned to cover
protocols that have more rounds than just three.  Furthermore, the number of
transcripts required is much greater---$(\numberofconstrains + 3)$---where
$\numberofconstrains$ is the number of constrains in the proven circuit---for
$\plonkprot$ and $(\multconstr + \linconstr + 1)$--where $\multconstr$ and
$\linconstr$ are the numbers of multiplicative and linear constraints---
for $\sonicprot$. Hence, we do not have a \emph{pair of transcripts}, but a \emph{tree of transcripts}.

Secondly, both protocols rely on common reference strings which come with trapdoors
that allow an adversary who knows them to produce multiple valid proofs even without
knowing the witness or for false statements. Recall that the standard special soundness definition
requires witness extraction from \emph{any} pair of acceptable transcripts
that share a common root. Thus, the definition cannot be met. Fortunately, one
could define a weaker version of special soundness, i.e.~a computational
special soundness. That is, we show that either it is possible to
extract a witness from a tree of acceptable transcripts or one could use the
adversary that produced the tree to break some underlying computational
assumption.

\subsubsection{Unique response property.} Another property that
has to be proven is the unique response property
which states, as expressed in \cite{C:Fischlin05}, that except for the first round, all messages sent
by the prover are deterministic. As previously, we also could not use this
definition right out of the box simply because $\plonkprot$ does not follow it---both first
and the second prover's messages are randomised. We thus propose a
generalisation of the definition which states that a protocol is $\ur{i}$ if the
prover is deterministic after sending its $i$-th message. This property is
fulfilled by $\plonkprot$ with $i = 2$. (Although we show that for $i = 3$, we
also prove that this is sufficient).

To be able to show the unique response property (for both of the protocols) we
also had to show that the modified KZG polynomial commitment scheme \cite{AC:KatZavGol10}
proposed in \cite{EPRINT:GabWilCio19,CCS:MBKM19} have a \emph{unique opening
property} which states that for a polynomial $\p{f}(X)$ evaluated at some
point $z$ it should be infeasible for any PPT adversary to provide
two different (even honest) but acceptable openings of the commitment. 

\subsubsection{HVZK.}%
In order to show our result we also show that (interactive) $\plonkprot$ and
$\sonicprot$ are
honest verifier zero-knowledge in the standard model, i.e.~the simulator is
able to produce a transcript indistinguishable from a transcript produced by a
honest prover and verifier without any additional knowledge, esp.~without
knowing the CRS trapdoor.
Although both $\sonic$ and $\plonk$ are shown to be zero-knowledge, the proofs
provided by their authors rely on trapdoors. For our reduction to work,
we need simulators that provide indistinguishable proofs relying only on
rearranging the order of messages and picking suitable verifier's challenges. 
That is, any PPT
party should be able to produce a simulated proof by its own. (Note that this property
does not necessary break soundness of the protocol as the simulator is
required only to produce a transcript and is not involved in a real
conversation with a real verifier).
This property allows us to build simulators for $\plonkprotfs$ and
$\sonicprotfs$ that rely only on programmability of the random oracle.

\subsubsection{Generalisation of the general forking lemma.}
Consider an interactive $3$-round special-sound protocol $\ps$ and its
non-interactive version $\ps_\fs$ obtained by the Fiat--Shamir
transform.  The general forking lemma provides an instrumental lower bound for
probability of extracting a witness from two proofs for the same statement
that share the first message. Since $\plonkprot$ and $\sonicprot$ have more
than $3$ rounds and are not special-sound, the forking lemma, as known from
\cite{CCS:BelNev06}, cannot be used
directly. We thus propose a modification that covers multi-round protocols
and where witness extraction require more transcripts than merely two.
Unfortunately, we also observe that the security gap grows with the number of transcripts and the
probability that the
extractor succeeds diminishes significantly. (That said, we have to note that
the security loss is polynomial, albeit big.)

We note that some modern zkSNARKs, like
\cite{SP:BBBPWM18,CCS:MBKM19}, rely on the Fiat--Shamir
transform and the forking lemma heavily. First, an interactive protocol is
proposed and
its security and special-soundness analysed; second, one uses an argument that
the Fiat--Shamir transform can be used to get a protocol that is
non-interactive and shares the same security properties. 

\markulf{03.11.2020}{I rephrased this a bit, as it seemed to hurt us as well.
  Does knowledge soundness have the same tightness?}
	\michals{4.11.20}{Thanks it looks better now, re ks -- that depends. For
		plonk there is no such issue with ks as its ks is shown in the algebraic
		group model; sonic uses both witness extended emulation and the AGM, thus
	I guess it is affected}
We see our generalized forking lemma as contributing to a critical assessment of
this approach. The analysis of the interactive protocol is not enough and one
has to consider the security loss implied by the generalisation of the forking
lemma or disclose a transformation that does not suffer from the generalisation
inefficiency. We note that the security loss may also apply when knowledge
soundness is proven. That is the case for $\sonic$, which security proof
relies on so-called witness-extended emulation. Authors of $\plonk$ worked
around this problem by showing their protocol secure directly in the algebraic
group model.

\subsubsection{Towards simulation-extractability.} Given our modified, less
restrictive, definition for special-soundness and the unique response property, and
our generalised forking lemma we are able to show the announced result---simulation
extractability of $\plonkprotfs$ and $\sonicprotfs$. The proof is inspired by
simulation-extractability and simulation-soundness proofs from
\cite{INDOCRYPT:FKMV12}, with major modifications, which were required as
\cite{INDOCRYPT:FKMV12} considers (Fiat--Shamir transformed) $\Sigma$-protocols
that are undoubtedly simpler protocols than the considered proof systems.
% Since the proof highly relies on the (generalised) forking lemma, the security
% lost it introduces is considerable.

% \subsubsection{Efficient simulation-soundness.}
% Given that the security reduction for simulation-extractability introduces a security gap we also present a proof for $\plonkprot_\fs$ simulation soundness which utilises the algebraic group model and is tight. 
% It remains an open question how to show simulation extractability tightly, e.g.~using AGM.

\subsubsection{Interactive zero knowledge vs non-interactive zero knowledge.}
\michals{12.11}{I am not sure that section is needed}Another issue we tackle is a question whether NIZK proof systems are in
fact zero-knowledge, see e.g.~\cite{C:Pass03}. This problem was raised as one
could observe that a NIZK proof system has a property alien to interactive
proofs---a verifier who obtains a proof $\zkproof$ for a statement $\inp \in
\LANG$ inevitably learns how to prove this statement. More precisely, it can
just reuse the obtained proof $\zkproof$. This makes the verifier learn
undoubtedly more than simply the veracity of the proven statement. On the other
hand, the verifier learns a particular proof $\zkproof$ for a concrete CRS
$\crs$ only. However, the CRS generator is considered trusted, thus the proof
$\zkproof$ could be considered as a proof for $\inp$ generally, regardless of the
CRS.

Simulation-extractable updatable NIZKs---and zkSNARKs in particular---tighten
the gap between interactive zero knowledge and non-interactive zero knowledge in
the CRS model. First, simulation-extractability assures that no adversary can
maul an existing proof and make a fresh one, what limits replay attacks.
Second, updatable NIZKs does not assume that there is a trusted party that
provides a CRS, but rather a sequence of parties (called updaters) that modify
it. The security model is radically different as now one believes in the
veracity of a proof only if it trusts that there is at least one honest party
between the updaters. 

We believe that our result may be useful for designing new zkSNARKs, especially those based on a polynomial commitment schemes~\cite{AC:KatZavGol10}, as it shows that a careful protocol design may give it a strong security notion for free.

\section{Preliminaries}
Let $\ppt$ denote probabilistic polynomial-time and $\secpar \in \NN$ be the
security parameter.  All adversaries are stateful.  For an algorithm $\adv$,
let $\image (\adv)$ be the image of $\adv$ (the set of valid outputs of
$\adv$), let $\RND{\adv}$ denote the set of random tapes of correct length for $\adv$ (assuming the given
value of $\secpar$), and let $r \sample \RND{\adv}$ denote the random choice
of the randomiser $r$ from $\RND{\adv}$.  We denote by $\negl$ ($\poly$) an
arbitrary negligible (resp.~polynomial) function.

Probability ensembles $X = \smallset{X_\secpar}_\secpar$ and
$Y = \smallset{Y_\secpar}_\secpar$, for distributions $X_\secpar, Y_\secpar$, have \emph{statistical distance} $\SD$ equal
$\epsilon(\secpar)$ if $\sum_{a \in \supp{X_\secpar \cup Y_\secpar}}
\abs{\prob{X_\secpar = a} - \prob{Y_\secpar = a}} = \epsilon(\secpar)$.  We
write $X \approx_\secpar Y$ if $\SD(X_\secpar, Y_\secpar) \leq \negl$.  
For values $a(\secpar)$ and $b(\secpar)$ we write $a(\secpar) \approx_\secpar
b(\secpar)$ if $\abs{a(\secpar) - b(\secpar)} \leq \negl$.  
\markulf{03.11.2020}{This requires probability ensembles and
	$\secpar$ dependent
  values. Which notation do you
  want to use? I am ok with light, just would like to hint at us being aware of this.}
	\michals{4.11}{Check now}

Denote by $\RELGEN = \smallset{\REL}$ a family of relations. We assume that if
$\REL$ comes with any auxiliary input, it is benign. Directly from the
description of $\REL$ one learns security parameter $\secpar$ and other
necessary information like public parameters $\pp$ containing description of a
group $\GRP$, if the relation is a relation of group elements (as it usually
is in case of zkSNARKs).

\paragraph{Bilinear groups.}
A bilinear group generator $\pgen (\secparam)$ returns public parameters $ \pp
= (p, \GRP_1, \GRP_2, \GRP_T, \pair, \gone{1}, \gtwo{1})$, where $\GRP_1$,
$\GRP_2$, and $\GRP_T$ are additive cyclic groups of prime order $p =
2^{\Omega (\secpar)}$, $\gone{1}, \gtwo{1}$ are generators of $\GRP_1$,
$\GRP_2$, resp., and $\pair: \GRP_1 \times \GRP_2 \to \GRP_T$ is a
non-degenerate $\ppt$-computable bilinear pairing.  We assume the bilinear
pairing to be Type-3, i.e., that there is no efficient isomorphism from
$\GRP_1$ to $\GRP_2$ or from $\GRP_2$ to $\GRP_1$.  We use the by now standard
bracket notation, i.e., we write $\bmap{a}{\gi}$ to denote $a g_{\gi}$ where
$g_{\gi}$ is a fixed generator of $\GRP_{\gi}$.  We denote $\pair (\gone{a},
\gtwo{b})$ as $\gone{a} \bullet \gtwo{b}$.  Thus, $\gone{a} \bullet \gtwo{b} =
\gtar{a b}$.  We freely use the bracket notation with matrices, e.g., if
$\vec{A} \vec{B} = \vec{C}$ then $\vec{A} \grpgi{\vec{B}} = \grpgi{\vec{C}}$
and $\gone{\vec{A}}\bullet \gtwo{\vec{B}} = \gtar{\vec{C}}$.
Since every algorithm $\adv$ takes as input the public parameters we
skip them when describing $\adv$'s input. Similarly, we do not explicitly
state that each protocol starts with generating these parameters by $\pgen$.

\subsection{Computational assumptions.}
Security of $\plonk$ and $\sonic$ rely on two discrete-log based security
assumptions---$(q_1, q_2)$-$\dlog$ assumption and its extended with
negative exponents version $(q_1,
q_2)$-$\ldlog$ assumption\footnote{Note that \cite{CCS:MBKM19} dubs their
	assumption \emph{a dlog assumption}. We changed that name to distinct it
from the more standard dlog assumption used in \cite{EPRINT:GabWilCio19}.
``l'' in \emph{ldlog} relates to use of Laurent polynomials in the assumption.}.
\begin{definition}[$(q_1, q_2)\mhyph\dlog$ assumption]
	Let $\adv$ be a $\ppt$ adversary that gets as input $\gone{1, \chi, \ldots, \chi^{q_1}}, \gtwo{1, \chi, \ldots, \chi^{q_2}}$, for some randomly picked $\chi \in \FF_p$, then
	\[
		\condprob{\chi \gets \adv(\gone{1, \chi, \ldots, \chi^{q_1}}, \gtwo{1, \chi, \ldots, \chi^{q_2} })}{\chi \sample \FF_p} \leq \negl.
	\]
\end{definition}

\begin{definition}[$(q_1, q_2)\mhyph\ldlog$ assumption]
		Let $\adv$ be a $\ppt$ adversary that gets as input $\gone{\chi^{-q_1},
		\ldots, 1, \chi, \ldots, \chi^{q_1}}, \gtwo{\chi^{-q_2}, \ldots, 1, \chi, \ldots, \chi^{q_2}}$, for some randomly picked $\chi \in \FF_p$, then
	\[
			\condprob{\chi \gets \adv(\gone{\chi^{-q_1}, \ldots, 1, \chi, \ldots,
			\chi^{q_1}}, \gtwo{\chi^{-q_2}, \ldots, 1, \chi, \ldots, \chi^{q_2} })}{\chi \sample \FF_p} \leq \negl.
	\]
\end{definition}

\paragraph{BBG uber assumption}
Also, to be able to show computational honest verifier zero knowledge of
$\plonk$ in the standard model, what is required by our reduction, we use the
\emph{uber assumption} introduced by Boneh et
al.~\cite{EC:BonBoyGoh05} as presented by Boyen in \cite{PAIRING:Boyen08}.
% Here we present not in its whole generality but rather fitted to our purpose.

Let $r, s, t, c \in \NN \setminus \smallset{0}$, Consider tuples of
polynomials $\pR
\in \FF_p[X_1, \ldots, X_c]^r$, $\pS \in \FF_p[X_1, \ldots, X_c]^s$ and
$\pT 
\in \FF_p[X_1, \ldots, X_c]^t$. Write $\pR = \left( \p{r}_1, \ldots,
\p{r}_r \right)$,
$\pS = \left( \p{s}_1, \ldots, \p{s}_s \right)$ and $\pT = \left( \p{t}_1, \ldots,
\p{t}_r \right)$ for polynomials $\p{r}_i, \p{s}_j, \p{t}_k$. 

For a function $f$ and a vector $(x_1, \ldots, x_c)$ we write $f(\pR)$ to
denote application of $f$ to each element of $\pR$, i.e.
\[
	f(\pR) = \left( f(\p{r}_1 (x_1, \ldots, x_c), \ldots, f(\p{r}_r
	(x_1, \ldots, x_c) \right).
\]
Similarly for applying $f$ to $\pS$ and $\pT$.

\begin{definition}[Independence of $\pR, \pS, \pT$]
	\label{def:independence}
	Let $\pR, \pS, \pT$ be defined as above. We say that polynomial
	$\p{f} \in \FF_p[X_1, \ldots, X_c]$ is \emph{dependent} on $\pR, \pS,
	\pT$ if there exists $rs + t$ constants $a_{i, j}, b_k$ such that 
	\[
		\p{f} = \sum_{i = 1}^{r} \sum_{j = 1}^{s} a_{i, j} \p{r}_i \p{s}_j +
		\sum_{k = 1}^{t} b_k \p{t}_k.
	\]
	We say that $\p{f}$ is \emph{independent} if it is not dependent.
\end{definition}

The original (decisional) uber assumption by Boneh et al.~states that for
type-III pairings:

\begin{definition}[$(\pR, \pS, \pT, \p{f})$-uber assumption
	\cite{EC:BonBoyGoh05}]
	\label{def:uber_assumption_orig}
	Let $\pR, \pS, \pT$, $(x', x_1, \ldots, x_c) \sample \FF_p^{c + 1}$ and let
	$\p{f}$ be independent on $(\pR, \pS, \pT)$.
	Then, for any $\ppt$ adversary $\adv$
	\begin{multline*}
		\prob{\adv(\gone{\pR(x_1, \ldots x_c)}, \gtwo{\pS(x_1, \ldots, x_c)},
		\gtar{\pT(x_1, \ldots, x_c)}, \gtar{\p{f}(x_1, \ldots, x_c)}) = 1} \approx_\secpar \\ 
		\prob{\adv(\gone{\pR(x_1, \ldots x_c)}, \gtwo{\pS(x_1, \ldots, x_c)},
		\gtar{\pT(x_1, \ldots, x_c)}, \gtar{x'}) = 1}.  
	\end{multline*}
\end{definition}

For the sake of this paper we modify the assumption slightly, i.e.~we require
not target group $\GRP_T$ elements to be indistinguishable, but elements of
$\GRP_1$, more precisely we require: 

\begin{definition}[$(\pR, \pS, \pT, \p{f})$-uber assumption]
	\label{def:uber_assumption}
	Let $\pR, \pS, \pT$, $(x', x_1, \ldots, x_c) \sample \FF_p^{c + 1}$ and let
	$\p{f}$ be independent on $(\pR, \pS, \pT)$.
	Then, for any $\ppt$ adversary $\adv$
	\begin{multline*}
		\prob{\adv(\gone{\pR(x_1, \ldots x_c)}, \gtwo{\pS(x_1, \ldots, x_c)},
		\gtar{\pT(x_1, \ldots, x_c)}, \gone{\p{f}(x_1, \ldots, x_c)}) = 1} \approx_\secpar \\ 
		\prob{\adv(\gone{\pR(x_1, \ldots x_c)}, \gtwo{\pS(x_1, \ldots, x_c)},
		\gtar{\pT(x_1, \ldots, x_c)}, \gone{x'}) = 1}.  
	\end{multline*}
\end{definition}

\paragraph{Proofs by Game-Hoping.}
Proofs by \emph{game hoping} is a method of writing proofs popularised by e.g.~Shoup \cite{EPRINT:Shoup04} and Dent \cite{EPRINT:Dent06c}. The method relies on the following lemma.

\begin{lemma}[Difference lemma,  cf.~{\cite[Lemma 1]{EPRINT:Shoup04}}]
	\label{lem:difference_lemma}
	Let $\event{A}, \event{B}, \event{F}$ be events defined in some probability
	space, and suppose that $\event{A} \land \nevent{F} \iff \event{B}
		\land \nevent{F}$.  Then 
	\[
		\abs{\prob{\event{A}} - \prob{\event{B}}} \leq \prob{\event{F}}\,.
	\]
\end{lemma}
\markulf{03.11.2020}{This somewhat relies on both games being defined over the
  same probability space, e.g. only their winning conditions but none of the
  sampling differs.}
	\michals{4.11.20}{Changed "probability distribution" to "probability space".
	"Prob. dist" was used in the original Schoup's paper, but I think
prob.~space makes more sense here.}

\subsection{Polynomial commitment.}
\label{sec:poly_com}
In the polynomial commitment scheme $\PCOM = (\kgen, \com, \open, \verify)$
the committer $\committer$ can prove to the receiver $\receiver$ that some
polynomial $\p{f}$ that $\committer$ committed to evaluates to $s$ as some point
$z$ chosen by $\receiver$.

$\plonk$ and $\sonic$ use variants of the KZG polynomial commitment scheme. We denote the first by $\PCOMp$ and present in \cref{fig:pcomp} and the latter by $\PCOMs$ and present in \cref{fig:pcoms}.

\begin{figure}[t!]
	\begin{pcvstack}[center,boxed]
		\begin{pchstack}
			\procedure{$\kgen(\secparam)$}
			{
			\chi \sample \FF^2_p \\ [\myskip]
			\pcreturn \gone{1, \ldots, \chi^{\numberofconstrains + 2}}, \gtwo{\chi}\\ [\myskip]
				\hphantom{\hspace*{5.5cm}}	
				%\hphantom{\pcind \p{o}_i(X) \gets \sum_{j = 1}^{t_i} \gamma_i^{j - 1} \frac{\p{f}_{i,j}(X) - \p{f}_{i, j}(z_i)}{X - z_i}}
			}
			
			\pchspace
			
			\procedure{$\com(\crs, \vec{\p{f}}(X))$}
			{ 
				\pcreturn \gone{\vec{c}} = \gone{\vec{\p{f}}(\chi)}\\ [\myskip]
				\hphantom{\pcind \pcif 
					\sum_{i = 1}^{\abs{\vec{z}}} r_i \cdot \gone{\sum_{j = 1}^{t_j}
					\gamma_i^{j - 1} c_{i, j} - \sum{j = 1}^{t_j} s_{i, j}} \bullet
				\gtwo{1} + }
			}
		\end{pchstack}
		% \pcvspace
    
		\begin{pchstack}
			\procedure{$\open(\crs, \vec{\gamma}, \vec{z}, \vec{s}, \vec{f}(X))$}
			{
			\pcfor i \in \range{1}{\abs{\vec{z}}} \pcdo\\ [\myskip]
			\pcind \p{o}_i(X) \gets \sum_{j = 1}^{t_i} \gamma_i^{j - 1} \frac{\p{f}_{i,j}(X) - \p{f}_{i, j}(z_i)}{X - z_i}\\ [\myskip]
				\pcreturn \vec{o} = \gone{\vec{\p{o}}(\chi)}\\ [\myskip]
				\hphantom{\hspace*{5.5cm}}	
			}
			
			\pchspace
			
			\procedure{$\verify(\crs, \gone{c}, \vec{z}, \vec{s}, \gone{\p{o}(\chi)})$}
			{
				\vec{r} \gets \FF_p^{\abs{\vec{z}}}\\ [\myskip]
				\pcfor i \in \range{1}{\abs{\vec{z}}} \pcdo \\ [\myskip]
				\pcind \pcif 
					\sum_{i = 1}^{\abs{\vec{z}}} r_i \cdot \gone{\sum_{j = 1}^{t_j} \gamma_i^{j - 1} c_{i, j} - \sum{j = 1}^{t_j} s_{i, j}} \bullet \gtwo{1} + \\ [\myskip]
					\pcind \sum_{i = 1}^{\abs{\vec{z}}} r_i z_i o_i
					\bullet \gtwo{1} \neq 
					\gone{- \sum_{i = 1}^{\abs{\vec{z}}} r_i o_i } \bullet \gtwo{\chi} \pcthen  \\
					\pcind \pcreturn 0\\ [\myskip]
					\pcreturn 1.
			}
		\end{pchstack}
	\end{pcvstack}
	\caption{$\PCOMp$ polynomial commitment scheme}
	\label{fig:pcomp}
\end{figure}

\begin{figure}[t!]
	\begin{pcvstack}[center,boxed]
		\begin{pchstack}
			\procedure{$\kgen(\secparam)$}
			{
				\alpha, \chi \sample \FF^2_p \\ [\myskip]
				\pcreturn \gone{\smallset{\chi^i}_{i = -\multconstr}^{\multconstr},
				\smallset{\alpha \chi^i}_{i = -\multconstr, i \neq
			0}^{\multconstr}},\\
			\pcind \gtwo{\smallset{\chi^i, \alpha \chi^i}_{i =
				-\multconstr}^{\multconstr}}, \gtar{\alpha}\\
				%\markulf{03.11.2020}{} \\
			%	\hphantom{\pcind \p{o}_i(X) \gets \sum_{j = 1}^{t_i} \gamma_i^{j - 1} \frac{\p{f}_{i,j}(X) - \p{f}_{i, j}(z_i)}{X - z_i}}
				\hphantom{\hspace*{5.5cm}}	
		}
			
			\pchspace
			
			\procedure{$\com(\crs, \maxconst, \p{f}(X))$}
			{
				\p{c}(X) \gets \alpha \cdot X^{\dconst - \maxconst} \p{f}(X) \\ [\myskip]
				\pcreturn \gone{c} = \gone{\p{c}(\chi)}\\ [\myskip]
				\hphantom{\pcind \pcif 
					\sum_{i = 1}^{\abs{\vec{z}}} r_i \cdot \gone{\sum_{j = 1}^{t_j}
					\gamma_i^{j - 1} c_{i, j} - \sum{j = 1}^{t_j} s_{i, j}} \bullet
				\gtwo{1} + }
			}
		\end{pchstack}
		% \pcvspace
    
		\begin{pchstack}
			\procedure{$\open(\crs, z, s, f(X))$}
			{
				\p{o}(X) \gets \frac{\p{f}(X) - \p{f}(z)}{X - z}\\ [\myskip]
				\pcreturn \gone{\p{o}(\chi)}\\ [\myskip]
				\hphantom{\hspace*{5.5cm}}	
			}
			
			\pchspace
			
			\procedure{$\verify(\crs, \maxconst, \gone{c}, z, s, \gone{\p{o}(\chi)})$}
			{
				\pcif \gone{\p{o}(\chi)} \bullet \gtwo{\alpha \chi} + \gone{s - z \p{o}(\chi)} \bullet \gtwo{\alpha} = \\ [\myskip]
				\pcind \gone{c} \bullet \gtwo{\chi^{- \dconst + \maxconst}} \pcthen  \pcreturn 1\\ [\myskip]
				\rlap{\pcelse \pcreturn 0.}
				\hphantom{\pcind \pcif 
					\sum_{i = 1}^{\abs{\vec{z}}} r_i \cdot \gone{\sum_{j = 1}^{t_j}
					\gamma_i^{j - 1} c_{i, j} - \sum{j = 1}^{t_j} s_{i, j}} \bullet
				\gtwo{1} + }
			}
		\end{pchstack}
	\end{pcvstack}
	
	\caption{$\PCOMs$ polynomial commitment scheme}
	\label{fig:pcoms}
\end{figure}

% We require $\PCOM$ to have the following properties:
We emphasize the following properties of a secure polynomial commitment
$\PCOM$:
\begin{description}
	\item[Evaluation binding] a PPT adversary $\adv$ who output a commitment $c$
		to a polynomial $\p{f}$ and is given an evaluation point $z$ has at most
negligible chances to correctly open the commitment to a value different than
$\p{f}(z)$.

\markulf{03.11.2020}{Make sure we mean the right thing. In the following
  definition based on KZG, the adversary does not necessarily need to know $f$:}
	\michals{4.11}{Right, I think the knowledge component has been shown
	independently in Sonic and Plonk (for their versions of the polcom).}
	Let 
	$k \in \NN$ be the number of committed polynomials,
	$l \in \NN$ number of evaluation points,
	$\vec{c} \in \GRP^k$ be the commitments,  
	$\vec{z},\vec{z}' \in \FF_p^l$ be the attributes the polynomials are evaluated at, $\vec{s},\vec{s}' \in \FF_p^k$ the evaluations, and 
	$\vec{o},\vec{o}' \in \FF_p^l$ be the commitment openings. 
	Then for every $\ppt$ adversary $\adv$	
	\[
		\Pr
			\left[
			\begin{aligned}
				& \verify(\crs, \vec{c}, \vec{z}, \vec{s}, \vec{o}),  \\ 
				& \verify(\crs, \vec{c}, \vec{z}', \vec{s}', \vec{o}'), \\
				& \vec{s} \neq \vec{s}'
			\end{aligned}
			\,\left|\,
			\begin{aligned}
				& \crs \gets \kcrs(\secparam),\\
				& (\vec{c}, \vec{z}, \vec{s}, \vec{s}', \vec{o}, \vec{o}') \gets \adv(\crs)
			\end{aligned}
			\right.\right] \leq \negl\,.
	\]

\end{description}
	
We say that $\PCOM$ has the unique opening property if the following holds:
\begin{description}
	\item[Opening uniqueness] 
	Let 
	$k \in \NN$ be the number of committed polynomials,
	$l \in \NN$ number of evaluation points,
	$\vec{c} \in \GRP^k$ be the commitments,  
	$\vec{z} \in \FF_p^l$ be the attributes the polynomials are evaluated at, $\vec{s} \in \FF_p^k$ the evaluations, and 
	$\vec{o} \in \FF_p^l$ be the commitment openings. 
	Then for every $\ppt$ adversary $\adv$	
	\[
		\Pr
			\left[
			\begin{aligned}
				& \verify(\crs, \vec{c}, \vec{z}, \vec{s}, \vec{o}),  \\ 
				& \verify(\crs, \vec{c}, \vec{z}, \vec{s'}, \vec{o'}), \\
				& \vec{o} \neq \vec{o'}
			\end{aligned}
			\,\left|\,
			\begin{aligned}
				& \crs \gets \kcrs(\secparam),\\
				& (\vec{c}, \vec{z}, \vec{s}, \vec{s'}, \vec{o}, \vec{o'}) \gets \adv(\crs)
			\end{aligned}
			\right.\right] \leq \negl\,.
	\]
\end{description}
Intuitively, opening uniqueness assures that there is only one valid opening
for the committed polynomial and given evaluation point. This property is
crucial in showing simulation-extractability of $\plonk$ and $\sonic$. We show
that the $\plonk$'s and $\sonic$'s polynomial commitment schemes satisfy this
requirement in \cref{lem:pcomp_unique_op} and \cref{lem:pcoms_unique_op}
respectively.
\markulf{03.11.2020}{The two definitions could be combined into what I would
  call strong evaluation binding.}
	\michals{4.11}{Noted, let me check whether it is more convenient to use
	one or two separate properties later}

\subsection{Algebraic Group Model}
The algebraic group model (AGM) introduced in \cite{C:FucKilLos18} lies
between the standard model and generic bilinear group model. In the AGM it is
assumed that an adversary $\adv$ can output a group element $\gnone{y} \in
\GRP$ if $\gnone{y}$ has been computed by applying group operations to group
elements given to $\adv$ as input. It is further assumed, that $\adv$ knows
how to ``build'' $\gnone{y}$ from that elements. More precisely, the AGM
requires that whenever $\adv(\gnone{\vec{x}})$ outputs a group element
$\gnone{y}$ then it also outputs $\vec{c}$ such that $\gnone{y} = \vec{c}^\top
\cdot \gnone{\vec{x}}$.  It is worth to note that both $\plonk$ and $\sonic$
have been shown secure using the AGM. 

\subsection{Zero knowledge}
In a zero-knowledge proof system, a prover convinces the verifier of
the veracity of a statement without leaking any side information except that the
statement is true.
Here, we focus on proof systems that guarantee soundness against a $\ppt$
cheating prover.
The zero-knowledge property is proven by constructing a simulator that can
simulate the view of a cheating verifier without knowing the secret
information---witness---of the prover.

More precisely, let $\RELGEN(\secparam) = \smallset{\REL}$ be a family of
$\npol$ relations.
Denote by $\LANG_\REL$ the language determined by $\REL$.
Let $\prover$ and $\verifier$ be $\ppt$ algorithms, the former called \emph{prover}
and the latter \emph{verifier}. We allow our proof system to have a setup,
i.e.~there is a $\kgen$ algorithm that takes as input the relation $\REL$ and
outputs a common reference string $\crs$.
We denote by $\ip{\prover(\REL, \crs, \inp, \wit)}{\verifier(\REL,
\crs,\inp)}$ a transcript $\trans$ (or, proof $\pi$--we use these two
alternatively) of a conversation between a $\prover$ with input
$(\REL, \crs, \inp, \wit)$ and $\verifier$ with input $(\REL, \crs, \inp)$.
We write $\ip{\prover (\REL, \crs, \inp, \wit)}{\verifier(\REL, \crs, \inp)} =
1$ if in the end of the transcript the verifier $\verifier$ returns $1$ and
say that $\verifier$ accepts the transcript. We sometimes abuse notation and
write $\verifier(\trans) = 1$ to denote a fact that $\trans$ is accepted by
the verifier. 

\markulf{09.07.20}{Could also all quantify for Soundness and Zero knowledge?
  Yes, I meant the first, forall $\inp, \wit \in \REL$. But I also agree that
  for $\REL \gets \RELGEN$ the probabilities do not define a negligible
  function, but a specific value in $[0,1]$. We could call $\RELGEN$ an NP
  relations generator from which we sample. Does the notation get too heavy?}
\michals{09.07.20}{I am not sure what you meant here. You want to have for all $\inp, \wit \in \REL$ or you doubt these properties make sense for all $\REL \gets \RELGEN$?}
\michals{4.11.20}{Update: now we have $\REL \in \RELGEN(\secparam)$}
A proof system $\proofsystem = (\kgen, \prover, \verifier, \simulator)$ for $\RELGEN$ is required to have three properties: completeness, soundness and zero knowledge, which are defined as follows:
\begin{description}
	\item[Completeness] An interactive proof system $\proofsystem$ is
			\emph{complete} if an honest prover always convinces an honest verifier,
			that is for all $\REL \in \RELGEN(\secparam)$ and $(\inp, \wit) \in \REL$
	\[
		\condprob{\ip{\prover (\REL, \crs, \inp, \wit)}{\verifier (\REL, \crs,
		\inp)} = 1}{\crs \gets \kgen(\secparam)} = 1\,.
	\]
	\item[Soundness] We say that $\proofsystem$ for $\RELGEN$ is \emph{sound} if
			no $\ppt$ prover $\adv$ can convince an honest verifier $\verifier$ to
			accept a proof for a false statement, i.e~for $\inp \not\in\LANG$. More
			precisely, for all $\REL \in \RELGEN(\secparam)$
	\[
		\condprob{\ip{\adv(\REL, \crs, \inp)}{\verifier(\REL, \crs, \inp)} =
		1}{\crs \gets \kgen(\secparam), \inp \gets \adv(\REL, \crs); \inp \not\in \LANG_\REL} \leq \negl\,;
	\]
	\item[Zero knowledge] We call an interactive proof system $\proofsystem$
		\emph{zero-knowledge} if for any $\REL \in \RELGEN(\secparam)$, $(\inp,
		\wit) \in \REL$, and adversary $\adv$ there exists a $\ppt$ simulator $\simulator$ such that
	\begin{multline*}
	  \left\{\ip{\prover(\REL, \crs, \inp, \wit)}{\adv(\REL, \crs, \inp, \wit)}
			\,\left|\, \crs \gets \kgen(\secparam)\COMMENT{, (\inp, \wit) \gets \adv(\REL,
\crs)}\vphantom{\simulator^\adv}\right.\right\} \approx_\secpar
		\\
		\left\{\simulator^{\adv}(\REL, \crs, \inp)\,\left|\, \crs \gets
			\kgen(\secparam)\COMMENT{, (\inp, \wit) \gets \adv(\REL,
\crs)}\vphantom{\simulator^\adv}\right.\right\}\,.  
\end{multline*}
	%
	We call zero knowledge \emph{perfect} if the distributions are equal and
	\emph{computational} if they are indistinguishable for any $\nuppt$ distinguisher.

	Occasionally, a weaker version of zero knowledge is required, so called
	\emph{honest verifier zero knowledge} (HVZK), where it is assumed that the
	verifier's challenges are picked at random from some predefined set.
	Although weaker, this definition suffices in many applications. Especially,
	an interactive zero-knowledge proof that is HVZK and \emph{public-coin}
	(i.e.~the verifier outputs as challenges its random coins) can be made
	non-interactive in the random oracle model by using the Fiat--Shamir
	transformation.
\end{description}
	
	Sometimes a stronger notion of soundness is required---except requiring that the verifier rejects proofs of statements outside the language, we request from the prover to know a witness corresponding to the proven statement. This property is formalised by the following notion:
\begin{description}
	\item[Knowledge soundness] We call an interactive proof system $\proofsystem$
			\emph{knowledge-sound} if for any $\REL \in \RELGEN(\secparam)$ and a $\ppt$ adversary $\adv$
	% \begin{multline*}
	\[
	\Pr\left[
		\begin{aligned}
			& \verifier(\REL, \crs, \inp, \trans) = 1 \\
			& \land \REL(\inp, \wit) = 0
	 \end{aligned}
	  \,\left|\,
	 \begin{aligned}
		 & \crs \gets \kcrs(\REL), \inp \gets \adv(\REL, \crs), \\
		 & (\wit, \trans) \gets \ext^{\ip{\adv(\REL, \crs, \inp)}{\verifier(\REL, \crs, \inp)}}(\REL, \inp)
	 \end{aligned}
	 \vphantom{\begin{aligned}
		 \adv (\trans) = 1, \\
		 \text{if $\trans{}$ is accepting} \\
		 \pcind \text{then $\REL(\inp, \wit)$}
	 \end{aligned}}\right.
	 \right] \leq \negl\,,
 % \end{multline*}
 \]
\end{description}

\paragraph{NIZKs in the Random Oracle Model.}
In NIZKs in the Random Oracle Model we distinguish, for the sake of clarity, two simulators, one denoted by $\simulator_\zkproof$ that is responsible for providing simulated proofs and $\simulator_\ro$ that picks a random oracle instantiation and takes care of all parties' queries to $\ro$.
% \michals{9.06}{Should we distinguish two simulators or just pack everything into a single one?}
For the sake of consistency (with random oracle-free NIZKs) we use $\simulator$
to denote the pair of state-sharing simulators $\simulator_\zkproof,
\simulator_\ro$.

\paragraph{Sigma protocols.}
A sigma protocol $\sigmaprot = (\prover, \verifier, \simulator)$  for a relation
$\REL \in \RELGEN(\secparam)$  is a special case of an interactive proof which transcript compounds of three messages $(a, b, z)$, the middle being a challenge provided by the verifier.
Sigma protocols are honest verifier zero-knowledge and specially-sound. That
is, there exists an extractor $\ext$ which given two accepting transcripts $(a, b, z)$, $(a, b', z')$ for a statement $\inp$ can recreate the corresponding witness if $b \neq b'$. Formally,
\begin{description}
	\item[Special soundness] A sigma protocol $\sigmaprot$ is \emph{specially-sound} if for any adversary $\adv$ the probability
	\[
		\Pr\left[
		\begin{aligned}
				& \wit \gets \ext(\REL, \inp, (a, b, z), (a, b', z')),\\
				& \REL(\inp, \wit) = 0
		\end{aligned}
		\,\left|\,
		\begin{aligned}
			& (\inp, (a, b, z), (a, b', z')) \gets \adv(\REL), \\
			& \verifier(\REL, \inp, (a, b, z)) = \\
			& \qquad = \verifier(\REL, \inp, (a, b', z')) = 1, \\
		\end{aligned}
		\right.\right]
	\]
	is negligible in $\secpar$.
\end{description}

% Furthermore sigma protocols are honest verifier zero-knowledge.
%That is the zero-knowledge property holds only for honest verifiers, what is
%formalized as follows: \begin{description}
%	\item[Honest verifier zero knowledge] A sigma protocol $\sigmaprot$ is \emph{honest verifier zero-knowledge} if for all adversaries $\adv$ holds
%	\begin{multline*}
%		\left\{\ip{\prover(\REL, \crs, \inp, \wit)}{\verifier(\REL, \inp)} \,\left|\, (\inp, \wit) \gets \adv(\REL)\vphantom{\simulator_\zkproof^\adv}\right.\right\} \approx_\secpar
%		\left\{\simulator_\zkproof^{\verifier}(\REL, \inp)\,\left|\, (\inp, \wit) \gets \adv(\REL)\vphantom{\simulator^\adv}\right.\right\}\,.
%	\end{multline*}
%\end{description}
%Although this notion is weaker than a standard zero knowledge it is often sufficient. Furthermore, a HVZK interactive proof system transformed by the Fiat--Shamir transformation is zero-knowledge.

Another property that sigma protocols sometimes have is, introduced by Fischlin \cite{C:Fischlin05}, a unique response property which states that no $\ppt$ adversary  can produce two accepting transcripts that differ only on the last element.
More precisely,
\begin{description}
	\item[Unique response property] Let $\sigmaprot = (\prover, \verifier,
			\simulator)$ be a sigma-protocol for $\REL \in \RELGEN(\secparam)$ which
			proofs compound of three messages $(a, b, z)$. We say that
			$\sigmaprot$ is has a unique response property if for all $\ppt$
			algorithms $\adv$ holds 
			\[
				\condprob{\verifier (a, b, z) = \verifier (a, b, z')  = 1}{(a, b, z,
				z') \gets \adv(\REL)} \leq \negl\,.
			\]
\end{description}
(If the unique response property holds even against unbounded adversaries, we call it \emph{strict}, cf.~\cite{INDOCRYPT:FKMV12}.)
Later on we often call protocols that follows this notion \emph{ur-protocols}.
For the sake of completeness we note that many sigma-protocols, like e.g.~Schnorr's protocol \cite{C:Schnorr89}, fulfil this requirement.

% \paragraph{Zero knowledge proof system in the CRS model.}
% Many proof systems additionally compounds of a setup algorithm $\kcrs$ that on input $\REL$ outputs a common reference string (CRS) $\crs$. The common reference string comes with a corresponding trapdoor $\td$ that allows the simulator to simulate a proof.

\subsection{Simulation extractable NIZKs from sigma protocols}
Real life applications often require from a NIZK proof system to be
non-malleable. That is, no adversary seeing a proof $\zkproof$ for a statement
$\inp$ should be able to provide a new proof $\zkproof'$ related to
$\zkproof$.  A strong version of non-malleability is formalised by simulation
extractability.  This notion states that no adversary can produce valid proof
without knowing the corresponding witness. This must hold even if the
adversary is allowed to see polynomially many simulated proofs for any
statements it wishes.

\begin{definition}[Simulation-extractable NIZK]
	\label{def:simext}
	Let $\ps = (\kgen, \prover, \verifier, \simulator)$ be a
	computationally special-sound HVZK proof and $\ps_\fs = (\kgen_\fs,
	\prover_\fs, \verifier_\fs, \simulator_\fs)$ be $\ps$ transformed by the
	Fiat--Shamir transform.  We say that $\ps_\fs$ is simulation extractable
	with \emph{extraction error} $\nu$ if for any $\ppt$ adversary $\adv$ that
	is given oracle access to a random oracle $\ro$ and simulator
	$\simulator_\fs$, and produces an accepting transcript of $\ps$ with
	probability
	$\accProb$, that is
	\[
		\accProb = \Pr \left[
		\begin{aligned}
			& \verifier_\fs(\REL, \crs, \inp_\advse, \zkproof_\advse) = 1,\\
			& (\inp_\advse, \zkproof_\advse) \not\in Q
		\end{aligned}
		\, \left| \,
		\begin{aligned}
			& \crs \gets \kgen(\secparam),\\
			& (\inp_\advse, \zkproof_\advse) \gets \advse^{\simulator_\fs,
			\ro} (\REL, \crs) 
		\end{aligned}
		\right.\right]\,,
	\]
	probability
	\[
		\frkProb = \Pr \left[
		\begin{aligned}
			& \verifier_\fs(\REL, \crs, \inp_\advse, \zkproof_\advse) = 1,\\
			& (\inp_\advse, \zkproof_\advse) \not\in Q,\\
			& \REL(\inp_\advse, \wit_\advse) = 0
		\end{aligned}
		\, \left| \,
		\begin{aligned}
			& \crs \gets \kgen(\secparam),\\
			& (\inp_\advse, \zkproof_\advse) \gets \advse^{\simulator_\fs,
			\ro} (\REL, \crs) \\
			& \wit_\advse \gets \ext_\ss (\trans_{\advse}) 
		\end{aligned}
		\right.\right]
	\]
	is at at least 
	\[
		\frkProb \geq \frac{1}{\poly} (\accProb - \nu)^d - \eps(\secpar)\,,
	\]
	for some polynomial $\poly$, constant $d$ and negligible $\eps$ whenever
	$\accProb \geq \nu$.
\end{definition}

Consider a sigma protocol $\sigmaprot = (\prover, \verifier, \simulator)$ that
is specially sound and has a unique response property. Let $\sigmaprot_\fs = (\prover_\fs, \verifier_\fs, \simulator_\fs)$ be a NIZK obtained by applying the Fiat--Shamir transform to $\sigmaprot$.
Faust et al.~\cite{INDOCRYPT:FKMV12} show that every such $\sigmaprot_\fs$ is simulation-extractable.

\begin{theorem}[Simulation extractability of the Fiat--Shamir transform \cite{INDOCRYPT:FKMV12}]
	Let $\sigmaprot = (\prover, \verifier, \simulator_\zkproof)$ be a non-trivial sigma protocol with unique responses for a language $\LANG \in \npol$.
	In the random oracle model, the NIZK proof system $\sigmaprot_\fs = (\prover_\fs, \verifier_\fs, \simulator_{\fs})$ resulting by applying the Fiat--Shamir transform to $\sigmaprot$ is simulation extractable with extraction error $\eta = q/h$ for the simulator $\simulator$. Here, $q$ is the number of random oracle queries and $h$ is the number of elements in the range of $\ro$.
	% Furthermore, the extractor $\ext_\adv$ needs to run $\adv^{\simulator_{\fs, \ro}}, \adv^{\simulator_{\fs, \zkproof}}$ twice.
\end{theorem}

The theorem relies on the following classical lemma, called \emph{General forking lemma} \cite{JC:PoiSte00}.

\begin{lemma}[General forking lemma, cf.~\cite{INDOCRYPT:FKMV12,CCS:BelNev06}]
	\label{lem:forking_lemma}
	Fix $q \in \ZZ$ and a set $H$ of size $h > 2$. Let $\adv$ be a $\ppt$ algorithm that on input $y, h_1, \ldots, h_q$ returns $(i, s)$, where $i \in\range{0}{q}$ and $s$ is called a \emph{side output}.
	Denote by $\ig$ a randomised instance generator.
	We denote by $\accProb$ the probability
	\[
		\condprob{i > 0}{y \gets \ig; h_1, \ldots, h_q \sample H; (i, s) \gets \adv(y, h_1, \ldots, h_q)}\,.
	\]
	Let $\forking_\adv(y)$ denote the algorithm described in \cref{fig:forking_lemma}, then the probability $\frkProb$ defined as
	$
		\frkProb := \condprob{b = 1}{y \gets \ig; (b, s, s') \gets \forking_{\adv}(y)}
	$
	holds
	\[
		\frkProb \geq \accProb \brak{\frac{\accProb}{q} - \frac{1}{h}}\,.
	\]
	%
	\begin{figure}[t]
		\centering
		\fbox{
		\procedure{$\forking_\adv (y)$}
		{
			r \sample \RND{\adv}\\
			h_1, \ldots, h_q \sample H\\
			(i, s) \gets \adv(y, h_1, \ldots, h_q; r)\\
			\pcif i = 0\ \pcreturn (0, \bot, \bot)\\
			h'_{i}, \ldots, h'_{q} \sample H\\
			(i', s') \gets \adv(y, h_1, \ldots, h_{i - 1}, h'_{i} h'_{q}; r)\\
			\pcif (i = i') \land (h_{i} \neq h'_{i})\ \pcreturn (1, s, s')\\
			\pcind \pcelse \pcreturn (0, \bot, \bot)
		}}
		\caption{Forking algorithm $\forking_\adv$}
		\label{fig:forking_lemma}
\end{figure}
\end{lemma}
%
In case of a sigma protocol, the probability $\frkProb$ can be interpreted as
a lower bound for a successful witness extraction from two transcripts.  Let
$\trans_1 = (\inp, a, b, z)$ and $\trans_2 = (\inp, a, b', z')$ be the
transcripts.  Both $\trans_1$ and $\trans_2$ have to be \emph{acceptable},
i.e.~$i > 0$ and the probability that $\adv$ makes an acceptable transcript is
denoted by $\accProb$.  Index $i$ can be interpreted as an index of $h_i$
which was sent as a challenge for $(\inp, a)$, this index has to be guessed by
the security reduction.  For the sake of extractability, both transcripts have
to have the same index $i$, i.e.~the same instance $\inp$ and the first
message $a$, but the actual challenges $b = h_i$ and $b' = h'_{i}$ have to
differ.

\section{Towards simulation extractability for multi-round
protocols, definitions and lemmas}
Unfortunately, Faust et al.'s result cannot be directly applied in our case
since the protocols we consider have more than three rounds of interaction,
require more than just two transcript for the extractor to work and are not special sound.

\subsection{Generalised forking lemma.}
First of all, although dubbed ``general'', \cref{lem:forking_lemma} is not
general enough for our purpose as it is useful only for protocols where witness
can be extracted from just two transcripts.  To be able to extract a witness
from, say, an execution of $\plonkprot$ we need to obtain at least
$\numberofconstrains + 3$ valid proofs.  
Here we propose a generalisation of the general forking lemma that given
probability of producing an accepting transcript $\waccProb$ lower-bounds the
probability of generating a \emph{tree of accepting transcripts} $\tree$,
which allows to extract a witness. 

\begin{definition}[Tree of accepting transcripts]
	\label{def:tree_of_accepting_transcripts}
	Consider a $(2\mu + 1)$-round interactive proof system $\ps$. A $(n_1,
	\ldots, n_\mu)$-tree of accepting transcript is a tree where each node on
	depth $i$, for $i \in \range{1}{\mu}$, is an $i$-th prover's message in an
	acceptable transcripts; edges between the nodes are labeled with verifier's
	challenges; and each node on depth $i$ has $(n_{i} - 1)$ siblings and $n_{i + 1}$ children.
	Altogether, the tree consists of $N = \prod_{i = 1}^\mu n_i$ branches, which
	makes $N$ acceptable transcripts. We require $N = \poly$.
\end{definition}

We note that the general forking lemma proposed in
\cref{lem:generalised_forking_lemma} works for protocols which have
extractable witness from a $(1, \ldots, 1, n_i, 1, \ldots, 1)$-tree of
acceptable transcripts. This limitation however does not affect the main
result of this paper, i.e.~showing that both $\plonk$ and $\sonic$ are
simulation extractable. 

\begin{lemma}[General forking lemma II]
	\label{lem:generalised_forking_lemma}
	Fix $q \in \ZZ$ and set $H$ of size $h \geq m$. 
	Let $\adv$ be a $\ppt$ algorithm that on input $y, h_1, \ldots, h_q$ returns $(i, s)$ where $i \in \range{0}{q}$ and $s$ is called a side output. 
	Denote by $\ig$ a randomised instance generator. We denote by $\accProb$ the probability
	\[
		\condprob{i \neq 0}{ y \gets \ig;\ h_1, \ldots, h_q \sample H;\ (i, s) \gets \adv(y, h_1, \ldots, h_q)}\,.
	\]
	Let $\genforking_{\adv}^{m}$ 
	denote the algorithm described in
	\cref{fig:genforking_lemma} then the probability $\frkProb := \condprob{b =
	1}{y \gets \ig;\ (b, \vec{s}) \gets \genforking_{\adv}^{m}(y)}$ is at least 
	\[
		\frac{\accProb^m}{q^{m - 1}} - \accProb \cdot \left(1 - \frac{h!}{(h - m)! \cdot h^m}\right)
	\]
		
	\begin{figure}[t]
		\centering
		\fbox{
		\procedure{$\genforking_{\adv}^{m} (y)$}
		{
			r \sample \RND{\adv}\\
			h_1^{1}, \ldots, h_{q}^{1} \sample H\\
			(i_1, s_1) \gets \adv(y, h_1^{1}, \ldots, h_{q}^{1}; r)\\
			\pcif i_1 = 0\ \pcreturn (0, \bot)\\
			\pcfor j \in \range{2}{m}\\
			\pcind h_{1}^{j}, \ldots, h_{i - 1}^{j} \gets h_{1}^{j - 1}, \ldots,
			h_{i - 1}^{j - 1}\\
			\pcind h_{i}^{j}, \ldots, h_{q}^{j} \sample H\\
			\pcind (i_j, s_j) \gets \adv(y, h_1^{j}, \ldots, h_{i - 1}^{j}, h_{i}^{j},
			\ldots, h_{q}^{j}; r)\\
			\pcind \pcif i_j = 0 \lor i_{j} \neq i_{j - 1}\ \pcreturn (0, \bot)\\
			\pcif \exists (k, j), (k', j') \in \range{1}{q} \times
		\range{1}{m} \land (h_{k}^{j} = h_{k'}^{j'})\
		\pcreturn (0, \bot)\\
			\pcelse \pcreturn (1, \vec{s})
		}}
		\caption{Generalised forking algorithm $\genforking_{\adv}^{m}$
      \markulf{03.11.2020}{How is $i$ picked. Shall we improve this algorithm together?}}
			\michals{4.11}{Yes, that would be great. (also, currently working on
			that}
			\michals{4.11}{Check now}
		\label{fig:genforking_lemma}
\end{figure}
\end{lemma}
\begin{proof}
	\michals{3.08.20}{DISCLAIMER: This proof is a naive generalisation of the general forking lemma from Bellare and Neven 06. Need to check that all (in)equalities hold!}
	We proceed similarly as in \cite{CCS:BelNev06} (with some parts taken almost verbatim).
	
	First let denote by $\accProb(y)$ and $\frkProb(y)$ the following probabilities
	\begin{align*}
	\accProb(y) & =  \condprob{i \neq 0}{h_1, \ldots, h_q \sample H;\ (i, s) \gets \adv(y, h_1, \ldots, h_q)}\,.\\
		\frkProb(y) & = \condprob{b = 1}{(b, \vec{s}) \gets
	\genforking_{\adv}^{m}(y)}\,.
	\end{align*}
	
	We start by claiming that for all $y$ 
	\begin{equation}\label{eq:frkProb_y}
		\frkProb(y) \geq 
		\frac{\accProb(y)^m}{q^{m - 1}} - \accProb(y) \cdot \left(1 - \frac{h!}{(h - m)! \cdot h^m}\right)
		\end{equation}
	Then with the expectation taken over $y \sample \ig$, we have
	\begin{align}
		\frkProb & = \expected{\frkProb(y)} \geq
		\expected{\frac{\accProb(y)^m}{q^{m - 1}} -  \accProb(y) \cdot \left(1 -
		\frac{h!}{(h - m)! \cdot h^m}\right)} \label{eq:use_eq1}\\
						 & = \frac{\expected{\accProb(y)^m}}{q^{m - 1}} -
						 \expected{\accProb(y)} \cdot  \left(1 - \frac{h!}{(h - m)! \cdot h^m}\right) \\
		& \geq \frac{\expected{\accProb(y)}^m}{q^{m - 1}} -
		\expected{\accProb(y)} \cdot \left(1 - \frac{h!}{(h - m)! \cdot h^m}\right) \label{eq:by_lemma_jensen}\\
		& = \frac{\accProb^m}{q^{m - 1}} -  \accProb \cdot \left(1 - \frac{h!}{(h - m)! \cdot h^m}\right)\label{eq:by_accProb}\,.
	\end{align}
	Where \cref{eq:use_eq1} comes from \cref{eq:frkProb_y};
	\cref{eq:by_lemma_jensen} comes from \cref{lem:jensen}; and
	\cref{eq:by_accProb} holds by the fact that $\expected{\accProb(y)} =
	\accProb$.
	
	We now show \cref{eq:frkProb_y}.
	Denote by $J = \range{1}{m}^2 \setminus \smallset{(j, j)}_{j \in \range{1}{m}}$. 
	For any input $y$, with probabilities taken over the coin tosses of
$\genforking_{\adv}^{m}$ we have
	\begin{align*}
		\frkProb (y) & = \prob{i_j = i_{j'} \land i_j \geq 1 \land h_{i_j} \neq h_{i_{j'}} \text{ for } (j, j') \in J}	\\
		& \geq \prob{i_j = i_{j'} \land i_j \geq 1 \text{ for } (j, j') \in J} \\
		& \qquad - \prob{i_j \geq 1 \land h_{i_j} = h_{i_{j'}} \text{ for some } (j, j') \in J}\\
		& = \prob{i_j = i_{j'} \land i_j \geq 1 \text{ for } (j, j') \in J} -
		\prob{i_j \geq 1} \cdot 
		\left(1 - \frac{h!}{(h - m)! \cdot h^m}\right) \\ 
		& = \prob{i_j = i_{j'} \land
		i_j \geq 1 \text{ for } (j, j') \in J} - \accProb(y) \cdot \left(1 - \frac{h!}{(h - m)! \cdot h^m}\right)\,.
	\end{align*}
	It remains to show that $\prob{i_j = i_{j'} \land i_j \geq 1 \text{ for }
	(j, j') \in J} \geq \infrac{\accProb(y)^m}{q^{m - 1}}$.
	Let $\RND{\adv}$ denote the set from which $\adv$ picks its coins at random.
	For each $\iota \in \range{1}{q}$ let $X_\iota \colon \RND{\adv} \times
	H^{\iota - 1} \to [0, 1]$ be defined by setting $X_\iota(\rho, h_1, \ldots,
h_{\iota - 1})$ to \[
		\condprob{i = \iota}{h_\iota, \ldots, h_q \sample H; (i, s) \gets \adv(y, h_1, \ldots, h_q; \rho)} 
	\] 
	for all $\rho \in \RND{\adv}$ and $h_1, \ldots, h_{\iota - 1} \in H$. Consider $X_\iota$ as a random variable over the uniform distribution on its domain. 
	Then
	\begin{align*}
		& \prob{i_j = i_{j'} \land i_j \geq 1 \text{ for } (j, j') \in J} 
		 = \sum_{\iota = 1}^{q} \prob{i_1 = \iota \land \ldots \land i_m = \iota} \\
		& = \sum_{\iota = 1}^{q} \prob{i_1 = \iota} \cdot \condprob{i_2 = \iota}{i_1 = \iota} \cdot \ldots \cdot \condprob{i_m = \iota}{i_1 = \ldots = i_{m - 1} = \iota} \\
		& = \sum_{\iota = 1}^{q} \sum_{\rho, h_1, \ldots, h_{\iota - 1}} X_{\iota} (\rho, h_1, \ldots, h_{\iota - 1})^{m} \cdot \frac{1}{\abs{\RND{\adv}} \cdot \abs{H}^{\iota - 1}}
		= \sum_{\iota = 1}^{q} \expected{X_\iota^m} \,.
	\end{align*}
	Importantly, $\sum_{\iota = 1}^q \expected{X_{\iota}} = \accProb(y)$.
	
	By \cref{lem:jensen} we get
	\[
		\sum_{\iota = 1}^{q} \expected{X_\iota^m} \geq \sum_{\iota = 1}^{q} \expected{X_\iota}^m\,.
	\]
	Note that for e.g.~$X_i = 1$, $i \in \range{1}{q}$ the inequality becomes equality, that is, it is tight.
	 
	We now use the H\"older inequality from \cref{lem:holder} where we set  $x_i = \expected{X_i}$, $y_i = 1$, $p = m$, and $q = m/(m - 1)$ obtaining
	\begin{gather}
		\sum_{i = 1}^{q} \expected{X_i}  \leq \left(\sum_{i = 1}^{q} \expected{X_i}^m\right)^{\frac{1}{m}} \cdot \left(\sum_{i = 1}^{q} 1^\frac{m}{m - 1}\right)^{\frac{m - 1}{m}} \label{eq:tightness} \\
		\left(\sum_{i = 1}^{q} \expected{X_i}\right)^{m}  \leq \left(\sum_{i = 1}^{q} \expected{X_i}^m\right) \cdot q^{m - 1}\\
		\frac{1}{q^{m - 1}} \cdot \accProb(y)^{m} \leq \sum_{i = 1}^{q} \expected{X_i}^m\,.
	\end{gather}
	Finally, we get
	\[
		\frkProb(y) \geq \frac{\accProb(y)^m}{q^{m - 1}} - 
		 \accProb(y) \cdot \left(1 - \frac{h!}{(h - m)! \cdot h^m}\right)\,.
	\]
	\qed
\end{proof}

\begin{remark}[Tightness of \cref{eq:tightness}]
	In is important to note that Inequality (\ref{eq:tightness}) is tight. More precisely, for $\expected{X_i} = x$, $i \in \range{1}{q}$ we have
	\begin{gather*}
		\sum_{i = 1}^q x = \left(\sum_{i = 1}^{q} x^m\right)^\frac{1}{m} \cdot \left(\sum_{i = 1}^{q} 1^{\frac{m}{m - 1}}\right)^{\frac{m - 1}{m}} \\
		qx = \left(qx^m\right)^\frac{1}{m} \cdot q^{\frac{m - 1}{m}} \\
		(qx)^m = qx^m \cdot q^{m - 1} \\
		(qx)^m = (qx)^m\,.
	\end{gather*}
\end{remark}

\begin{lemma}\label{lem:jensen}
	Let $\RND{\adv}$ denote the set from which $\adv$ picks its coins at random.
	For each $\iota \in \range{1}{q}$ let $X_\iota \colon \RND{\adv} \times
	H^{\iota - 1} \to [0, 1]$ be defined by setting $X_\iota(\rho, h_1, \ldots,
h_{\iota - 1})$ to 
	\[
		\condprob{i = \iota}{h_\iota, \ldots, h_q \sample H; (i, s) \gets \adv(y, h_1, \ldots, h_q; \rho)} 
	\] 
	for all $\rho \in \RND{\adv}$ and $h_1, \ldots, h_{\iota - 1} \in H$. Consider $X_\iota$ as a random variable over the uniform distribution on its domain. 
	Then $\expected{X_\iota^m} \geq \expected{X_\iota}^m$.
\end{lemma}
\begin{proof}
	First we recall the Jensen inequality \cite{W:Weissten20}, if for some random variable $X$ holds $\abs{\expected{X}} \leq \infty$ and $f$ is a Borel convex function then 
	\[
		f(\expected{X}) \leq \expected{f(X)}\,.
	\] 
	Finally, we note that $\abs{\expected{X}} \leq \infty$ and taking to the $m$-th power is a Borel convex function on $[0, 1]$ interval. 
	\qed
\end{proof}

\begin{lemma}[H\"older's inequality. Simplified.]\label{lem:holder}
	Let $x_i, y_i$, for $i \in \range{1}{q}$, and $p, q$ be real numbers such that $1/p + 1/q = 1$. Then
	\[
		\sum_{i = 1}^{q} x_i y_i \leq \left(\sum_{i = 1}^{q} x_i^p\right)^{\frac{1}{p}} \cdot \left(\sum_{i = 1}^{q} y_i^p\right)^{\frac{1}{q}}\,.
	\]
\end{lemma}

\subsection{Unique-response protocols.}
Another problem comes with another assumption required by Faust et al. That is, the unique response property of the transformed sigma protocol.
Fischlin's formulation, although perfectly fine for applications presented in \cite{C:Fischlin05}, is not enough in our case.
First of all, the property assumes that the protocol has three rounds, with
the middle being the challenge from the verifier. That is not the case we
consider here. Second, it is not entirely clear how to generalize the
property. Should one require that after the first challenge from the verifier
the prover's responses are fixed? That could not work since if there is more
challenges the prover's messages has to respond to them.  Another problem rises when the protocol
contains some round---obviously, except the first one---where the prover
randomises its message. In that case unique-responsiveness can not hold as
well.  Last but not least, the protocols we consider here are not designed to
be in the standard model, but utilises CRS. That also complicates things
considerably.

We walk around these obstacles by providing a generalised notion of the unique
response property.  More precisely, we say that a $(2\mu + 1)$-round protocol
has \emph{unique responses after $i$} and is called a $\ur{i}$-protocol if it
follows the definition below:

\begin{definition}[$\ur{i}$-protocol]
	\label{def:wiur}
	Let $\proofsystem$ be a $2\mu$-round proof system $\ps = (\kgen, \prover,
	\verifier, \simulator)$.
	Denote by $a_0, b_0, \ldots, a_{\mu - 1}, b_{\mu - 1}, a_{\mu}$ the consecutive messages exchanged in the protocol, where messages $a_i$ come from the prover and $b_i$ from the verifier.
	We say that $\proofsystem$ has \emph{unique responses after $i$}
	if 
	\[
		\prob{
			\begin{aligned}
			&	(a_0, b_0, \ldots, a_j, b_j, a_\mu), (a'_0, b'_0, \ldots, a'_j,
		b'_j a'_\mu) \gets \adv(\REL, \crs) \\
			& b_k = b'_k, k \in \range{1}{\mu - 1},\ a_l = a'_l,\ l \in
		\range{1}{j},\ j > i 
			\end{aligned}
			\ \left|\  
		\vphantom{\begin{aligned}
		&	(a_0, b_0, \ldots, a_j, b_j, a_\mu), (a'_0, b'_0, \ldots, a'_j,
	b'_j a'_\mu) \gets \adv(\REL, \crs) \\
		& b_k = b'_k, k \in \range{1, \mu - 1}, a_l = a'_l, l \in
	\range{1}{j}, j > i 
		\end{aligned}}
	\crs \gets \kgen(\secparam) \right.
	} \leq \negl
	\]
% 	if after submitting its $i$-th message $a_i$ the prover is a deterministic
% 	function. That is, it does not use its randomness tape and deterministically answers verifier's challenges.
\end{definition}
\begin{example}
	The Schnorr protocol is $\ur{1}$. That is, after submitting its first message $a$, the prover is a deterministic function of the instance, $a$, and the verifier's challenge.
\end{example}

We note that the definition above is independent on whether the proof system $\proofsystem$ utilises CRS (and compounds of the CRS-generating $\kgen$ algorithm) or not.
% \michals{08.07.20}{Should we change it to "deterministic prover property"?}

\subsection{Computational special soundness}
Note that the special soundness property (as usually defined) holds for all--- even computationally unbounded---adversaries. Since a simulation trapdoor for $\plonkprot$ exists, it is not special sound in that regard---as an unbounded adversary could reveal the trapdoor and build a number of simulated proofs for a fake statement. 
Hence, we provide a weaker, yet sufficient, definition of \emph{computational special soundness}. More precisely, we state that an adversary that is able to answer correctly multiple challenges either knows the witness or can be used to break some computational assumption. 

\begin{definition}[Computational special soundness]
	Let $\proofsystem = (\kgen, \prover, \verifier, \simulator)$ be an $(2 \mu + 1)$-round proof system for a relation $\REL$. 
	We say that $\proofsystem$ is $(n_1, \ldots, n_\mu)$-\emph{special sound} if
	for every $\ppt$ adversary $\adv(\REL, \crs)$, where $\crs \gets
	\kgen(\secparam)$, that produces an accepting $(n_1, \ldots, n_\mu)$-tree of transcripts $\tree$ for a statement $\inp$ there exists an extractor $\ext$ that given $\tree$ extracts $\wit$ such that $\REL(\inp, \wit) = 1$ with an overwhelming probability.
\end{definition}
\markulf{03.11.2020}{Do we need to provide more details on the execution
  environment of $\adv$? It is given an honestly generated CRS and a relation as
  input but nothing else?}
	\michals{4.11}{check now}

Since we do not utilise the classical special soundness (that holds for all, even unbounded, adversaries) all references to that property should be understood as references to its computational version.

% \subsection{Unique opening property}
% here we show that both the polynomial commitment scheme $\PCOMp$ used in
% $\plonk$ and the polynomial commitment scheme $\PCOMs$ used in $\sonic$ have
% the unique opening property.

\section{Simulation-extractability---the general result}
\michals{25.09}{The generalisation of the forking lemma we have may work only for schemes that require rewinding of a single round. That limits the generalisation of the result}

\begin{theorem}[Simulation-extractable multi-round protocols]
	\label{thm:se}
	Let $\ps = (\kgen, \prover, \verifier, \simulator)$ be an interactive $2
	\mu$-round proof system that is honest verifier zero-knowledge in the
	standard model\footnote{Crucially, we require that one can provide an
			indistinguishable simulated proof without any additional knowledge, as
	e.g~knowledge of a CRS trapdoor.}, has $\ur{k}$ property with security $\epsur$ that is $(1, \ldots, 1, n_j, 1, \ldots, 1)$-special sound, where $j > k$.
	Assume that the simulator $\simulator$ does not get as input any trapdoor related to the CRS.
	Let $\ro\colon \bin^{*} \to \bin^{\secpar}$ be a random oracle. 
	Then $\psfs$ is simulation-extractable with extraction error $\epsur$
	against $\ppt$ adversaries that makes up to $q$ random oracle queries and
	returns an acceptable proof with probability at least $\accProb$. 
	The extraction probability $\extProb$ is at least
	\[
		\extProb \geq \frac{1}{q^{n_j - 1}} (\accProb - \epsur)^{n_j} -\eps\,,
	\]
	for some negligible $\eps$.	
\end{theorem}
\begin{proof}		
	The proof goes by game hoping. The games are controlled by an environment $\env$ that internally runs a simulation extractability adversary $\advse$,  provides it with access to a random oracle and simulator, and when necessary rewinds it.
	The games differ by various breaking points, i.e.~points where the environment decides to abort the game. 

	Denote by $\zkproof_{\advse}, \zkproof_{\simulator}$ proofs
	returned by the adversary and the simulator respectively. We use $\zkproof[i]$
	to denote prover's message in the $i$-th round of the proof, $\zkproof[i].\ch$
	to denote the challenge that is given to the prover after $\zkproof[i]$, and
	$\zkproof\range{i}{j}$ to denote all messages of the proof including challenges between rounds $i$ and $j$.
	
	Without loss of generality, we assume that whenever the accepting proof contains a response to a challenge from a random oracle, we assume that the adversary queried the oracle to get it. 
	It is straightforward to transform any adversary that violates this condition into an adversary that makes these additional queries to the random oracle and wins with the same probability.
	
	\ngame{0} 
	This is a simulation extraction game played between an adversary $\advse$ who has given access to a random oracle $\ro$ and simulator $\psfs.\simulator$. 
	There is also an extractor $\ext$ that, from the proof $\zkproof_\advse$ for instance $\inp_\advse$ output by the adversary and from a transcripts of $\advse$'s operations, is tasked to extract a witness $\wit_\advse$ such that $\REL(\inp_\advse, \wit_\advse)$ holds.
	$\advse$ wins if it manages to produce an acceptable proof and the extractor fails to reveal the corresponding witness.
	In the following game hops we upper-bound the probability that this happens.
	
	\ngame{1}
	This is identical to $\game{0}$ except that now the game is aborted if there is a simulated proof $\zkproof_\simulator$ such that $\zkproof_\simulator\range{1}{k} = \zkproof_\advse\range{1}{k}$. That is, the adversary in its final proof reuses a part of a simulated proof it saw before and the proof is acceptable.
	Denote that event by $\event{\errur}$.
	
	\ncase{$\game{0} \mapsto \game{1}$}	
	We have, 
	\[
		\prob{\game{0} \land \nevent{\errur}} = \prob{\game{1} \land \nevent{\errur}}
	\]
	and, from the difference lemma, cf.~\cref{lem:difference_lemma}
	\[
		\abs{\prob{\game{0}} - \prob{\game{1}}} \leq \prob{\event{\errur}}\,.
	\]
	Thus, to show that the transition from one game to another introduces only minor change in probability of $\advse$ winning it should be shown that $\prob{\event{\errur}}$ is small.
	
	Assume that $\advse$ queried the simulator on $\inp_{\advse}$---the instance which $\advse$ outputs. 
	We show a reduction $\rdvur$ that utilises $\advse$, who outputs a valid proof for $\inp_\advse$, to break the $\ur{3}$ property of $\ps$. 

	Consider an algorithm $\rdvur$ that runs $\advse$ internally as a black-box:
	\begin{itemize}
		\item The reduction answers both queries to the simulator $\psfs.\simulator$ and to the random oracle. 
		It also keeps lists $Q$, for the simulated proofs, and $Q_\ro$ for the random oracle queries. 
		\item When $\advse$ outputs a fake proof $\zkproof_{\advse}$ for  $\inp_\advse$, $\rdvur$ looks through lists $Q$ and $Q_\ro$ until it finds 
		$\zkproof_{\simulator}\range{1}{k}$ such that $\zkproof_{\advse}\range{1}{k} = \zkproof_{\simulator}\range{1}{k}$ and a random oracle query $\zkproof_{\simulator}[k].\ch$ on $\zkproof_{\simulator}\range{1}{k}$.
		\item $\rdvur$ returns two proofs for $\inp_\advse$:
		\begin{align*}
			\zkproof_1 = (\zkproof_{\simulator}\range{1}{k}, \zkproof_{\simulator}[k].\ch, \zkproof_{\simulator}\range{k + 1}{\mu})\\
			\zkproof_2 = (\zkproof_{\simulator}\range{1}{k}, \zkproof_{\simulator}[k].\ch, \zkproof_{\advse}\range{k + 1}{\mu})
		\end{align*}
		\end{itemize}  
		If $\zkproof_1 = \zkproof_2$, then $\advse$ fails to break simulation extractability, as $\zkproof_2 \in Q$.
		On the other hand, if the proofs are not equal, then $\rdvur$ breaks $\ur{k}$-ness of $\ps$, what may happen with some negligible probability $\epsur$ only, hence
		\[
			\prob{\event{\errur}} \leq \epsur\,.
		\]
		
	\ngame{2}
	This is identical to $\game{1}$ except that now the environment aborts also
	when it fails to build a $(1, 1, 1, n_j, 1)$-tree of accepting transcripts
	$\tree$ by rewinding $\advse$. Denote that event by $\event{\errfrk}$.
	Note that for every acceptable proof $\zkproof_{\advse}$, we may
	assume that whenever $\advse$ outputs in Round $i$, for $i > k$, a message
	$\zkproof_{\advse}[i]$, then $\zkproof_{\advse}\range{1}{i}$ is a query to
	the random oracle that was made by the adversary, not the
	simulator\footnote{\cite{INDOCRYPT:FKMV12} calls these queries
	\emph{fresh}.}.  That is, assume that is not true and for some query
	$\zkproof_{\advse}\range{1}{i}$ holds $\zkproof_{\advse}\range{1}{i} =
	\zkproof_\simulator\range{1}{i'}$, for $i, i' > k$, the proof is acceptable
	and not in $Q$, then the unique response property would be broken.
	
	\ncase{$\game{1} \mapsto \game{2}$}	
	As previously, 
	\[
		\abs{\prob{\game{1}} - \prob{\game{2}}} \leq \prob{\event{\errfrk}}\,.
	\]
	Denote by $\waccProb$ the probability that $\advse$ outputs a proof that is
  accepted and does not break $\ur{k}$-ness of $\ps$.
  Assuming that $\advse$ does not break the unique response property is
	necessary to be able to use the forking lemma. Otherwise, if the adversary
	could, say, rerandomise a proof $\zkproof$ it obtained from a simulator,
	then it would be able to provide a tree of accepting transcripts out of a
	single $\zkproof$. Since $\zkproof$ comes from a simulator expecting that an
	extractor can extract from it a valid witness is futile. The unique
	response property takes care of that. If an adversary would like to reuse
	a simulated proof it saw, it would end up with exactly the same proof
	which can not break simulation extractability.
	\markulf{03.11.2020}{Recall here why we need UR to use forking lemma.}
	\michals{4.11}{Check the explanation above}
	From the generalised forking lemma,
	cf.~\cref{lem:generalised_forking_lemma}, \[
		\prob{\event{\errfrk}} \leq 1 -
		\waccProb \cdot \left(\frac{\waccProb^{n_j - 1}}{q^{n_j - 1}} +
		\frac{(2^\secpar) !}{(2^\secpar - n_j)! \cdot
	(2^\secpar)^{n_j}} - 1\right)\,.
\]
For the sake of simplicity we loose this approximation a bit and state
\[
		\prob{\event{\errfrk}} \leq 1 -
		\left(\frac{\waccProb^{n_j}}{q^{n_j - 1}} +
			\waccProb \cdot \left(\frac{2^\secpar - n_j}{2^\secpar}\right)^{n_j} -
		\waccProb\right)\,.
\]
	\ngame{3}
	This is identical to $\game{2}$ except that now the environment uses the
	tree $\tree$ to extract the witness for the proven statement and aborts when
	it fails. Denote that event by $\event{\errss}$.
	
	\ncase{$\game{2} \mapsto \game{3}$}	
	As previously, 
	\[
		\abs{\prob{\game{2}} - \prob{\game{3}}} \leq \event{\errss}\,.
	\]
	Since $\ps$ is special-sound the probability that $\env$ fails in extracting
	the witness is upper-bounded by some negligible $\eps_\ss$.
	
	In the last game, Game $\game{3}$, the environment aborts when it fails to
	extract the correct witness, hence the adversary $\advse$ cannot win.  Thus,
	by the game-hoping argument, 
	\[
		\abs{\prob{\game{0}} - \prob{\game{3}}} \leq 1 -
		\left(\frac{\waccProb^{n_j}}{q^{n_j - 1}} + \waccProb \cdot
		\left(\frac{2^\secpar - n_j}{2^\secpar}\right)^{n_j} - \waccProb\right) + \epsur + \epsss\,.
	\]
	Thus the probability that extractor $\extss$ succeeds is at least
	\[
		\frac{\waccProb^{n_j}}{q^{n_j - 1}} + 
		\waccProb \cdot
		\left( \frac{2^\secpar - n_j}{2^\secpar}\right)^{n_j} -
	\waccProb - \epsur - \epsss\,.
	\]
	Since $\waccProb$ is probability of $\advse$ outputting acceptable
	transcript that does not break $\ur{3}$-ness of $\ps$, then $\accProb \leq
	\waccProb + \epsur$, where $\accProb$ is the probability of $\advse$ outputing an acceptable proof as defined in \cref{def:simext}. It thus holds
	\[
 		\label{eq:frk}
 		\extProb \geq \frac{(\accProb - \epsur)^{n_j}}{q^{n_j - 1}} -
		\underbrace{(\accProb - \epsur) \cdot \left( 1 -
			\left(\frac{2^\secpar - n_j}{2^\secpar}\right)^{n_j}\right)
- \epsur - \epsss}_{\eps}\,.
 	\]
 	Note that the part of \cref{eq:frk} denoted by $\eps$ is negligible as
	$\epsur, \epsss$ are negligible, and $\left(\infrac{(2^\secpar
	- n_j)}{2^\secpar}\right)^{n_j}$ is overwhelming.
	Thus, 
 	\[
 		\extProb \geq \frac{1}{q^{n_j - 1}} (\accProb - \epsur)^{n_j} -\eps\,.
 	\] 
 	thus
 	$\psfs$ is simulation extractable with extraction error $\epsur$.
 	\qed
\end{proof}

\section{Simulation extractability of $\plonkprotfs$} 
In this section we show that $\plonkprotfs$
is simulation-extractable. To that end, we
proceed as follows. 
First we show that the version of the KZG polynomial commitment scheme that is
proposed in the \plonk{} paper has the unique opening property,
cf.~\cref{sec:poly_com} and \cref{lem:pcomp_unique_op}. This is then used to
show that $\plonkprot$ has the $\ur{3}$ property, cf.~\cref{lem:plonkprot_ur}.

Next, we show that 
$\plonkprot$ is computational special-sound. That is, given a number of acceptable
transcripts which match on the first 3 rounds of the protocol we can either
reveal a correct witness for the proven statement or use one of the transcripts
to break the $\dlog$ assumption. The latter requires the AGM. More
precisely, we assume that each group element included in the
transcripts comes with a vector of coefficients that represents the element in
the basis compound of the input group elements, i.e.~$\plonkprot$'s CRS. See \cref{lem:plonkprot_ss}.

Given special-soundness of $\plonkprot$, we use the fact that it is also $\ur{3}$ and show, in a similar fashion to \cite{INDOCRYPT:FKMV12}, that it is simulation-extractable. That is, we build reductions that given a simulation extractability adversary $\advse$ either breaks the protocol's unique response property or breaks the $\dlog$ assumption, if extracting a valid witness from a tree of transcripts is impossible. See \cref{thm:plonkprotfs_se}.

\subsection{Unique opening property of $\PCOMp$}
\begin{lemma}
	\label{lem:pcomp_unique_op}
	Let $\PCOMp$ be a batched version of a KZG polynomial commitment
	\cite{AC:KatZavGol10} as described in \cite{EPRINT:GabWilCio19} then $\PCOMp$ has the unique opening property. 
\end{lemma}
\begin{proof}
	Let 
	$\vec{z} = (z, z') \in \FF_p^2$ be the two points the polynomials are evaluated at,
	$k \in \NN$ be the number of the committed polynomials to be evaluated at $z$, and $k' \in \NN$ be the number of the committed polynomials to be evaluated at  $z'$,
	$\vec{c} \in \GRP^k, \vec{c'} \in \GRP^{k'}$ be the commitments,  
	$\vec{s} \in \FF_p^k, \vec{s'} \in \FF_p^{k'}$ the evaluations, and 
	$\vec{o} = (o, o') \in \FF_p^2$ be the commitment openings. We need to show
  that for every $\ppt$ adversary $\adv$ the probability
	\[
		\Pr
			\left[
				\begin{aligned}
					& \verify(\crs, \vec{c}, \vec{c'}, (z, z'), \vec{s}, \vec{s'}, \vec{o}), \\
					& \verify(\crs, \vec{c}, \vec{c'}, (z, z'), \vec{\tilde{s}}, \vec{\tilde{s}'}, \vec{\tilde{o}}) \\
					& \vec{o} \neq \vec{\tilde{o}}
				\end{aligned}
			\,\left|\,
			\vphantom{\begin{aligned}
				& \verify(\crs, \vec{c}, \vec{c}, \vec{z}, \vec{s}, \vec{s'}, \vec{o}), \\
				& \verify(\crs, \vec{c}, \vec{c}, \vec{z}, \vec{s}, \vec{s'}, \vec{\tilde{o}}) \\
				&\vec{o} \neq \vec{\tilde{o}})
			\end{aligned}}
			\begin{aligned}
				& \crs \gets \kcrs(\secparam), \\
				&	(\vec{c}, \vec{c'}, \vec{z}, \vec{s}, \vec{s'}, \vec{\tilde{s}}, \vec{\tilde{s}'}, \vec{o}, \vec{\tilde{o}}) \gets \adv(\crs)
			\end{aligned}
			\right.\right]
		 % \leq \negl.
	\]
	is at most negligible.
	
	\ncase{Step 1} First, consider a case where the commitment is limited to commit to multiple polynomials which are evaluated at the same point $z$. 
	As noted in \cite[Lemma 2.2]{EPRINT:GabWilCio19} it is enough to upper bound
  the probability of the adversary succeeding using the idealised verification
	equation---which considers equality between polynomials---instead of the
	real verification equation---which consider equality of the polynomials'
	evaluations. This holds since an adversary that manages to provide a
	commitment opening that holds for the real verifier, but does not hold for
	the idealised verifier can be used to break the dlog assumption and reveal
	the secret trapdoor used to produce the commitment's CRS. 
	
	For polynomials $\vec{f} = f_1, \ldots, f_k$, evaluation point $z$, evaluation result $\vec{s} = s_1, \ldots, s_k$, random $\gamma$, and opening $o(X)$ the idealised check verifies that
	\begin{equation}
		\sum_{i = 1}^k \gamma^{i - 1} f_i(X) - \sum_{i = 1}^{k} \gamma^{i - 1} s_i \equiv o(X) (X - z)\,.
		\label{eq:pcom_idealised_check}
	\end{equation}
	Since $o(X)(X - z) \in \FF_p[X]$ then from the uniqueness of polynomial composition, there is only one $o(X)$ that fulfils the equation above.

	\ncase{Step 2} Second, consider a case when the polynomials are evaluated on two points $\vec{z} = (z, z')$ and the adversary is asked to provide two openings $\vec{o} = (o, o')$.
	Similarly, we analyse the case of the ideal verification. In that scenario, the verifier checks whether the following equality, for $\gamma, r'$ picked at random, holds:
	\begin{multline}
		\label{eq:ver_eq_poly}
		\sum_{i = 1}^{k} \gamma^{i - 1} \cdot f_i(X) - \sum_{i = 1}^{k} \gamma^{i - 1} \cdot s_i  + r' \left(\sum_{i = 1}^{k'} \gamma'^{i - 1} \cdot f'_i(X) - \sum_{i = 1}^{k'} \gamma'^{i - 1} \cdot s'_i \right)\\
		\equiv o(X)(X - z) + r' o'(X)(X- z')
	\end{multline}
	Since $r'$ has been picked at random, \cref{eq:ver_eq_poly} holds while either
	\[
		\sum_{i = 1}^{k} \gamma^{i - 1} \cdot f_i(X) - \sum_{i = 1}^{k}  \gamma^{i - 1} \cdot s_i \equiv o(X)(X - z)
	\]
	or 
	\[
		\sum_{i = 1}^{k'} \gamma'^{i - 1} \cdot f'_i(X) - \sum_{i = 1}^{k'} \gamma'^{i - 1} \cdot s'_i \equiv o'(X)(X - z')
	\]
	does not is negligible~\cite{EPRINT:GabWilCio19}. This brings the proof back to Step 1 above. 
	\qed
\end{proof}

\subsection{Unique response property}

\begin{lemma}
	\label{lem:plonkprot_ur}
	If a polynomial commitment scheme $\PCOM$ is evaluation binding with parameter $\epsbind$ and has unique openings property with parameter $\epsop$, then $\plonkprot$ is $\ur{3}$\footnote{An attentive reader may note that $\plonkprot$ is $\ur{2}$. However, in the case presented here, less is required.} 
	with parameter $\epsur \leq \epsbind + \epsop$.
\end{lemma}
\begin{proof}
	Let $\adv$ be an adversary that breaks $\ur{3}$-ness of
  $\plonkprot$. 
	% \markulf{30.08}{This is fine. But sounds as if the law of the
    % excluded middle is needed here. I am not sure it is. :) Mostly a
    % philosophical point.}
	We consider two cases, depending on which round $\adv$ is able to provide at
  least two different outputs such that the resulting transcripts are acceptable.
  % \markulf{30.08}{Rewrote this text: that in each of them $\adv$ breaks the
    % evaluation binding property of $\PCOM$.}
  For the first case we show that $\adv$ breaks the evaluation binding property of $\PCOM$, while for the
  second case we show that it breaks the unique opening property of $\PCOM$.
	
	\case{1}
	In Round 4 the prover is asked to give evaluations of predefined polynomials at some point $\chz$. Naturally, for the given polynomials only one value at $\chz$ is correct.
	Assume $\adv$ is able to produce two different outputs in that round: $\vec{r_4} = (\ev{\p{a}}, \ev{\p{b}}, \ev{\p{c}}, \ev{\p{S_{\sigma 1}}}, \ev{\p{S_{\sigma 2}}}, \ev{\p{r}}, \ev{\p{z}})$ and 
	$\vec{r_4} = (\ev{\p{a}}', \ev{\p{b}}', \ev{\p{c}}', \ev{\p{S_{\sigma 1}}}', \ev{\p{S_{\sigma 2}}}', \ev{\p{r}}', \ev{\p{z}}')$
	which suppose to be evaluations at $\chz$ of polynomials $\p{a}, \p{b}, \p{c}, \p{S_{\sigma 1}}, \p{S_{\sigma 2}}, \p{r}$ and an evaluation at $\chz \omega$ of $\p{z}$.
	Clearly, at least one of $\vec{r_4}$, $\vec{r'_4}$ has to be incorrect, thus if both evaluations are acceptable by the $\PCOM.\verify$ then the evaluation binding property of $\PCOM$ is broken. This happens with probability upper-bounded by $\epsbind$.
	
	\case{2}
	In the last round of the protocol the prover provides openings for the
  polynomial commitment evaluations done before. 
	Assume $\adv$ is able to produce two different polynomial commitment openings pairs: 
	$\vec{r_5} = (\ev{\p{W_\chz}}, \ev{\p{W_{\chz \omega}}})$ and 
	$\vec{r'_5} = (\ev{\p{W_\chz}}', \ev{\p{W_{\chz \omega}}}')$.
	% Since \cref{lem:pcomp_unique_op}, 
	Since $\PCOM$ has unique opening property, 
	one of the openings has to be incorrect and should be rejected by the
  polynomial commitment verifier. This happens except with probability $\epsop$
	
	\conclude
	Hence the probability that $\adv$ breaks $\ur{3}$-property of $\PCOM$ is upper-bounded by $\epsbind + \epsop$.
	\qed
\end{proof}


\subsection{Computational special soundness}
\begin{lemma}
	\label{lem:plonkprot_ss}
	Let $\adv$ be a $\ppt$ algebraic adversary. The probability $\epsss$ that $\adv$ breaks 
	 $(1, 1, 1, \numberofconstrains + 3, 1)$-computational special soundness of $\plonkprot$ is upper-bounded as
	 \[
	 	\epsss \leq \epsbatch + \epsdlog\,,
	 \] 
	 where $\epsbatch$ is (negligible) probability that $\plonkprot$'s idealised verification equation $\vereq(X)$ accepts an invalid proof because of batching and $\epsdlog$ is a probability that a $\ppt$ algorithm can break $(\numberofconstrains + 2, 1)$-$\dlog$ assumption.
\end{lemma}
\begin{proof}
	Let $\crs$ be $\plonkprot$'s CRS and denote by $\crs_1$ all CRS's $\GRP_1$-elements; that is, $\crs_1 = \gone{1, \chi, \ldots, \chi^{\numberofconstrains + 2}}$. 
	Let $\adv$ be an algebraic adversary that for a statement $\inp$ produces a $(1, 1, 1, \numberofconstrains + 3, 1)$-tree of acceptable transcripts $\tree$. % with non-negligible probability $\eta_\tree$. 
	Note that in all transcripts the instance $\inp$, proof elements $\gone{\p{a}(\chi), \p{b}(\chi), \p{c}(\chi), \p{z}(\chi), \p{t}(\chi)}$ and challenges $\alpha, \beta, \gamma$ are common as the transcripts share the first three rounds. 
	
	We consider two mutually disjunctive events. 
	First, $\event{E}$ holds when all of the transcripts are acceptable by the idealised verification equation, i.e.~$\vereq(X) = 0$, cf.~\cref{eq:ver_eq}.
	Second, $\nevent{E}$ holds when there is a transcript that is acceptable, yet 
	for some transcript $\vereq(\chi) = 0$, but $\vereq(X) \neq 0$.
	We build a special extractor $\extss$ which given the tree of transcripts $\tree$ reveals the witness with an overwhelming probability when $\event{E}$ happens. 
	We show a reduction $\rdvdlog$ that, when $\nevent{E}$ happens, breaks the $\dlog$ assumption. 
	
	\ncase{When $\event{E}$ happens}  Since the protocol $\plonkprot$,
	instantiated with the idealised verification equation, is perfectly sound,
	except with probability of batching failure $\epsbatch$, for a valid proof
	$\zkproof$ of a statement $\inp$ there exists a witness $\wit$, such that
	$\REL(\inp, \wit)$ holds.  Note that the polynomials $\p{a}(X),
	\p{b}(X), \p{c}(X)$, which contain witness in their coefficients, have degree $(\numberofconstrains + 2)$ and since $\adv$
	answered honestly on $(\numberofconstrains + 3)$ different challenges $\chz$
	then $(\numberofconstrains + 3)$ evaluations of these polynomials (at
	different points) are known. The extractor $\extss$ interpolates the
	polynomials and reveals the corresponding witness $\wit$.
  \markulf{03.11.2020}{What is definition of witness-carrying and what
    guarantees this property?}
		\michals{4.11}{Witness-carrying is not a well defined term, I used it to
			express that the coefficients of the polynomials constitutes the witness
		for the relation. I will rephrase if you find it misleading.}
		\michals{4.11}{Rephrased}
	\ncase{When $\nevent{E}$ happens} Consider a transcript that such that
	$\vereq(X) \neq 0$, but $\vereq(\chi) = 0$.
	Since the adversary is algebraic, all group elements included in the tree of
  transcripts are extended by their representation as a combination of the input
  $\GRP_1$-elements i.e.~$\gone{1, \chi, \ldots, \chi^{\numberofconstrains +
      2}}$. Hence all coefficients of the verification equation polynomial
  $\vereq(X)$ are known and $\rdvdlog$ can find its zero points. Since
  $\vereq(\chi) = 0$, the targeted discrete log value $\chi$ is among them.
	\qed
\end{proof}

\subsection{Honest verifier zero-knowledge}
\begin{lemma}
	$\plonk$ is honest verifier zero-knowledge and its simulator $\simulator$ does not
	require a CRS trapdoor.\footnote{The simulator works as a simulator for
	proofs that are zero-knowledge in the standard model. However, we do not
say that $\plonk$ is HVZK in the standard model as proof of that
\emph{requires} the CRS simulator.} More precisely, assume that $\plonk$'s CRS
simulator $\simulator_\chi$ produces a proof that is at most $\epszk$ far from
a real proof, and $(\pR, \pS, \pT, \pf)$-uber assumption for $\pR, \pS, \pT,
\pf$ as defined in \cref{eq:uber} is $\epsuber$-secure. Then for any $\ppt$
adversary $\adv$ its advantage in telling a proof produced by $\simulator$
from a real proof is upper-bounded by $\epszk + \epsuber$.
\end{lemma}
\begin{proof}
	The proof goes by game-hoping. The environment that controls the games
	provides the adversary with a CRS $\crs$, then the adversary outputs an
	instance--witness pair $(\inp, \wit)$ and, depending on the game, is
	provided with either real or simulated proof for it. In the end of the game
	the adversary outputs either $0$ if it believes that the proof it saw was
	provided by the simulator and $1$ in the other case.

	\ngame{0} In this game $\adv(\REL, \crs)$ picks an instance--witness
	pair $(\inp, \wit)$ and gets a real proof $\zkproof$ for it.
	
	\ngame{1} In this game for $\adv(\REL, \crs)$ picks an instance--witness
	pair $(\inp, \wit)$ and gets a proof $\zkproof$ that is simulated by a simulator
	$\simulator_\chi$ that utilises for the simulation the CRS trapdoor and
	proceeds as follows.
	In the first round the simulator $\simulator_\chi$ picks randomisers $b_1, \ldots b_9$, sets
	$\wit_i$, for $i \in \range{1}{3 \numberofconstrains}$, computes polynomials
	$\pa(X), \pb(X), \pc(X)$ and outputs $\gone{\pa(\chi), \pb(\chi), \pc(\chi)}$. 
	Then it picks Round 1 challenge $\beta, \gamma$ honestly.

	In the second round $\simulator_\chi$ computes the polynomial $\pz(X)$ and
	outputs $\gone{\pz(\chi)}$. Then it picks randomly Round 2 challenge
	$\alpha$.

	In the third round the simulator computes polynomial $\pt(X)$ and evaluates it
	at $\chi$, then outputs $\gone{\ptlo(\chi), \ptmid(\chi),
	\pthi(\chi)}$. Note that this evaluation is possible only since
	$\simulator_\chi$ knows the trapdoor.

	In the last two rounds the simulator proceeds as an honest prover would
	proceed and picks corresponding challenges at random as an honest verifier
	would.
	
	\ncase{$\game{0} \mapsto \game{1}$} 
	Since $\plonk$ is zero-knowledge, probability that $\adv$ outputs a
	different bit in both games is negligible. Hence
	\[
		\abs{\prob{\game{0}} - \prob{\game{1}}} \leq \epszk.
	\]

	\ngame{2} In this game $\adv(\REL, \crs)$ picks an
	instance--witness pair $(\inp, \wit)$ and gets a proof $\zkproof$ simulated
	by the simulator $\simulator$ which proceeds as follows:

	Since the simulator $\simulator$ does not know a witness $\wit$ for the
	proven statement $\inp$, it cannot compute the output of Round 1 accordingly
	to the protocol. Instead, it picks randomly both the randomisers $b_1,
	\ldots, b_6$ and sets $\wit_i = 0$ for $i \in
	\range{1}{3\numberofconstrains}$. Then $\simulator$ outputs $\gone{\p{a}(\chi),
	\p{b}(\chi), \p{c}(\chi)}$.  For the first round challenge, the simulator
	picks permutation argument challenges $\beta, \gamma$ randomly.

	In the second round, the simulator cannot the simulator computes $\p{z}$
	from the newly picked randomisers $b_7, b_8, b_9$ and coefficients of
	polynomials $\p{a}, \p{b}, \p{c}$. Then it evaluates $\p{z}$ honestly and
	outputs $\gone{\p{z}(\chi)}$.  Challenge $\alpha$ that should be sent by the
	verifier after Round 2 is picked by the simulator at random.

	The next round starts by the simulator picking at random a challenge $\chz$,
	which in the real proof comes as a challenge from the verifier sent after
	Round 3.  Then $\simulator$ computes evaluations \[\p{a}(\chz), \p{b}(\chz),
		\p{c}(\chz), \p{S_{\sigma 1}}(\chz), \p{S_{\sigma 2}}(\chz),
	\pubinppoly(\chz), \lag_1(\chz), \p{Z_H}(\chz),\allowbreak
	\p{z}(\chz\omega)\] and computes $\p{t}(X)$ honestly. Since for a random
	$\p{a}, \p{b}, \p{c},
	\p{z}$ the constraint system is (with overwhelming probability) not
	satisfied and the constraints-related polynomials are not divisible by
	$\p{Z_H}$,
	$\p{t}(X)$ is a rational function, not a polynomial. Then, the simulator
	evaluates $\p{t}(X)$ at $\chz$ and picks randomly a
	degree-$(\numberofconstrains + 2)$ polynomial $\p{\tilde{t}}(X)$ such that
	$\p{t}(\chz) = \p{\tilde{t}}(\chz)$ and publishes a commitment
	$\gone{\p{\tilde{t}}(\chi)}$.  After this round the simulator outputs $\chz$
	as a challenge.

In the next round, the simulator computes polynomial $\p{r}(X)$ as an honest
prover would, i.e.  
\begin{align*}
		\p{r}(X) = 
			& \p{a}(\chz) \p{b}(\chz) \selmulti(X) + \p{a}(\chz) \selleft(X) + \p{b}(\chz) \selright(X) + \p{c}(\chz) \seloutput(X) + \selconst(X) \\
			& + \alpha \cdot \left( (\p{a}(\chz) + \beta \chz + \gamma) (\p{b}(\chz) + \beta k_1 \chz + \gamma)(\p{c}(\chz) + \beta k_2 \chz + \gamma) \cdot \p{z}(X)\right) \\
			& - \alpha \cdot \left( (\p{a}(\chz) + \beta \p{S_{\sigma 1}}(\chz) + \gamma) (\p{b}(\chz) + \beta \p{S_{\sigma 2}}(\chz) + \gamma)\beta \p{z}(\chz\omega) \cdot \p{S_{\sigma 3}}(X)\right) \\
			& + \alpha^2 \cdot \lag_1(\chz) \cdot \p{z}(X)
\end{align*}
and evaluates $\p{r}(X)$ at $\chz$. 

The rest of the evaluations are already computed, thus 
$\simulator$ simply outputs 
\[
		\p{a}(\chz), \p{b}(\chz), \p{c}(\chz), \p{S_{\sigma 1}}(\chz), \p{S_{\sigma 2}}(\chz), \p{t}(\chz), \p{z}(\chz \omega)\,.
\]
After that it picks randomly the challenge $v$, proceeds in the last round as
an honest prover would proceed and outputs the final challenge, $u$, by
picking it at random as well.

\ncase{$\game{1} \mapsto \game{2}$}
We now describe the reduction $\rdv$ which relies on the $(\pR, \pS, \pT,
\pf)$-uber assumption where $\pR, \pS, \pT, \pf$ are polynomials over
variables $\vB = B_1, \ldots B_9$ and are defined as follows.
Let $E = \smallset{\smallset{1, 2}, \smallset{3, 4}, \smallset{5, 6},
\smallset{7, 8, 9}}$.
Let 
\begin{align}
	\label{eq:uber}
	\pR(\vB) & = \smallset{B_i \mid i \in A,\ A \in E} \cup \smallset{B_i B_j \mid i \in
A, j \in B,\ A \neq B,\ A, B \in E} \cup \\ 
			& \smallset{B_i B_j B_k \mid i \in A,\ j \in
B,\ k \in C,\ A \neq B \neq C \neq A,\ A, B, C \in E} \cup \notag \\
		& \smallset{B_i B_j B_k B_l \mid i \in A,\ j \in B,\ k \in C,\ l \in D,\
			A, B, C, D \text{ all different and in } E} \notag \\
		& \setminus \smallset{B_1
B_3 B_5 B_7}\,,\notag \\
	\pS(\vB) & = \emptyset \notag\,, \\
	\pT(\vB) & = \emptyset \notag\,, \\
	\pf(\vB) & = B_1 B_3 B_5 B_7 \notag\,.
\end{align}
That is, the elements of $\pR$ are all singletons, pairs and triplets of $B_i$
variables that occur in polynomial $\pt(\vB)$ except the
challenge element $\pf(\vB) = B_1
B_3 B_5 B_7$.
Variables $\vB$ are evaluated to randomly picked $\vb = b_1, \ldots
b_9$.

The reduction $\rdv$ learns $\gone{\pR}$ and challenge $\gone{w}$ where $w$ is
either $\pf(\vb) = b_1 b_3 b_5 b_7$ or a random value.
Then it picks $\chi$, $\chz$ and computes the CRS $\crs$ from $\chi$ 
Elements  $b_i$ are
interpreted as polynomials in $X$ that are evaluated at $\chi$, i.e. $b_i =
b_i(\chi)$
Next, $\rdv$ sets 
\[ \gone{\p{\tb}_i}(X) =
	(X - \chz)(X - \ochz) \gone{b_i}(X) + \xi_i (X - \chz) \gone{1} +
	\zeta_i (X - \ochz) \gone{1}, % \text{ for } i \in % \range{1}{9}, u_1
\sample \FF_p 
\] 
for $i \in \range{1}{9}$ and $\xi_i, \zeta_i \sample \FF_p$. Denote by $\tb_i$
evaluations of $\p{\tb}_i$ at $\chi$.
The reduction computes all $\gone{\tb_i \tb_j}, \gone{\tb_i \tb_j \tb_k},
\gone{\tb_i \tb_j \tb_k \tb_l}$ such that
$\gone{B_i B_j, B_i B_j B_k B_l} \in \pR$.
This is possible since $\rdv$ knows all singletons $\gone{b_1, \ldots,
b_9}$ and pairs $\gone{b_i b_j} \in \pR$ which can be used to compute all
required pairs $\gone{\tb_i \tb_j}$: 
\begin{align*}
	\gone{\tb_i \tb_j} 
	& = ((\chi - \chz)(\chi - \ochz)\gone{b_i} + \xi_i (\chi - \chz)\gone{1} +
	\zeta_i (\chi - \ochz) \gone{1}) 
	\cdot \\
	 & ((\chi - \chz)(\chi - \ochz)\gone{b_j} + \xi_j (\chi - \chz)\gone{1} +
	\zeta_j (\chi - \ochz) \gone{1}) = \\
	 & ((\chi - \chz)(\chi - \ochz))^2 \gone{b_i b_j} +  ((\chi - \chz)(\chi -
	 \ochz)\gone{b_i} (\xi_j (\chi - \chz) \gone{1} + \zeta_j (\chi - \ochz)
	 \gone{1}) + \\
	 & ((\chi - \chz)(\chi -
	 \ochz)\gone{b_j} (\xi_i (\chi - \chz) \gone{1} + \zeta_i (\chi - \ochz)
	 \gone{1}) + \psi,
\end{align*}
where $\psi$ compounds of $\xi_i, \xi_j, \zeta_i, \zeta_j, \chz, \ochz, \chi$ which
are all known by $\rdv$ and no $b_i$ nor $b_j$.
Analogously for the triplets and quadruplets. 

For the challenge $\gone{w}$ $\rdv$ sets $\gone{\tw} = \gone{\tb_1 \tb_3
\tb_5 \tb_7}$, where $\gone{b_1 b_3 b_5 b_7}$ is substituted by $\gone{w}$.
Next it runs the adversary $\adv(\REL, \crs)$ and obtains from $\adv$ an
instance--witness pair $(\inp, \wit)$.  $\rdv$ now prepares a simulated proof.

\begin{description} 
	\item[Round 1] $\rdv$ computes $\gone{\pa(\chi)}$ using as
	randomisers $\gone{\tb_1}, \gone{\tb_2}$ and setting $\wit_i = 0$, for $i
	\in \range{1}{3 \numberofconstrains}$. Similarly it computes
	$\gone{\pb(\chi)}, \gone{\pc(\chi)}$.  $\rdv$ publishes the obtained values
	and picks a Round 1 challenge $\beta, \gamma$ at random.  
\item[Round 2]
	$\rdv$ computes $\gone{\pz(\chi)}$ using $\tb_7, \tb_8, \tb_9$ and publishes
	it. Then it picks randomly the challenge $\alpha$.  
\item[Round 3] The
	reduction computes $\gone{\pt(\chi)}$ using $\tw$ as it was equal $\tb_1
	\tb_3 \tb_5 \tb_7$. That is, if $w = b_1 b_3 b_5 b_7$ then $\pt(\chi)$ is as
	computed by the simulator $\simulator_\chi$, otherwise, if $w$ is random
	then $\pt(\chi)$ is random as well, thus it is computed as $\simulator$
	would compute. The reduction computes and outputs $\gone{\ptlo(\chi),
	\ptmid(\chi), \pthi(\chi)}$ such that $\pt(X) = \ptlo(X) +
	X^\numberofconstrains \ptmid(X) + X^{2\numberofconstrains} \pthi(X)$.
	Eventually, $\rdv$ outputs $\chz$.  
\item[Round 4] The reduction outputs
	$\pa(\chz), \pb(\chz), \pc(\chz), \p{S_{\sigma 1}}(\chz), \p{S_{\sigma 2}
		(\chz)}, \pt(\chz), \pz(\ochz)$.  For the sake of concreteness, denote by
		$S = \smallset{\pa, \pb, \pc, \pt, \pz}$. Although for a polynomial $\p{p}
		\in S$, reduction $\rdv$ does not know $\p{p}(\chi)$ or even do not know
		all the coefficients of $\p{p}$, the polynomials in $S$ was computed such
		that the reduction always knows their evaluation at $\chz$ and $\ochz$.
\item[Round 5] $\rdv$ computes the openings of the polynomial commitments
	assuring that evaluations at $\chz$ it provided were computed honestly.
	\end{description} Is the adversary $\adv$'s output distribution differ in
	Game $\game{1}$ and $\game{2}$ then the reduction uses it to distinguish
	between $w = b_1 b_3 b_5 b_7$ and $w$ being random, thus \[
		\abs{\prob{\game{1}} - \prob{\game{2}}} \leq \epsuber.  \] Eventually, \[
		\abs{\prob{\game{0}} - \prob{\game{2}}} \leq \epszk + \epsuber.  \] \qed
	\end{proof}

\subsection{From special-soundness and unique response property to simulation extractability of $\plonkprotfs$}

Since \cref{lem:plonkprot_ur,lem:plonkprot_ss} hold, $\plonkprot$ is $\ur{3}$
and computationally special sound. We now 
\markulf{03.11.2020}{cut: follow \cite{INDOCRYPT:FKMV12}. Don't give those
guys too much credit, instead: }\michals{4.11}{I am fine with that} make use of \cref{thm:se} and show that
$\plonkprot_\fs$ is simulation-extractable as defined in \cref{def:simext}.

% \michals{13.10}{The theorem below is going to have a much shorter proof -- for it being just a corollary of Sec.~4}
\begin{corollary}[Simulation extractability of $\plonkprot_\fs$]
	\label{thm:plonkprotfs_se}
	Assume that  $\plonkprot$ is $\ur{3}$ with security $\epsur(\secpar)$, and computational special-sound with security $\epsss(\secpar)$.
	Let $\ro\colon \bin^* \to \bin^\secpar$ be a random oracle. 
	Let $\advse$ be a $\ppt$ adversary that can make up to $q$ random oracle
	queries and outputs an acceptable proof for $\plonkprotfs$ with probability
	at least $\accProb$.
	Then $\plonkprotfs$ is simulation-extractable with extraction error $\eta = \epsur$. The extraction probability $\extProb$ is at least
	\[
		\extProb \geq \frac{1}{q^{\numberofconstrains + 2}} (\accProb - \epsur)^{\numberofconstrains + 3} -\eps\,,
	\]
	for some negligible $\eps$ and $\numberofconstrains$ being the number of contrains in the proven circuit.
\end{corollary}

\markulf{03.11.2020}{Comment again at this being huge? Maybe mention
  simulation-soundness result?}

%\begin{proof}
% 	The theorem holds as a corollary to \cref{thm:se}.
	% The proof goes by game hoping. The games are controlled by an environment $\env$ that internally runs a simulation extractability adversary $\advse$, simulates its with access to a random oracle and simulator, and when necessary rewinds it.
	% The games differ by various breaking points, i.e.~points where the environment decides to abort the game. 
	% 
	% Denote by $\zkproof_{\advse}, \zkproof_{\simulator}$ proofs
	% returned by the adversary and the simulator respectively. We use $\zkproof[i]$
	% to denote prover's message in the $i$-th round of the proof, $\zkproof[i].\ch$
	% to denote the challenge that is given to the prover after $\zkproof[i]$, and
	% $\zkproof\range{i}{i'}$ to denote all messages of the proof exchanged between rounds $i$ and $i'$, i.e.~$\zkproof[i], \zkproof[i].\ch, \ldots, \zkproof[i']$.
	% 
	% Without loss of generality, we assume that whenever the accepting proof contains a response to a challenge from a random oracle, we assume that the adversary queried the oracle to get it. 
	% It is straightforward to transform any adversary that violates this condition into an adversary that makes these additional queries to the random oracle and wins with the same probability.
	% 
	% \ngame{0}
	% This is a simulation extraction game played between an adversary $\advse$ who
	% has given access to a random oracle $\ro$ and simulator
	% $\plonkprotfs.\simulator$.  There is also an extractor $\ext$ that, from the
	% proof $\zkproof_\advse$ for instance $\inp_\advse$ output by the adversary and
	% from a transcripts of $\advse$'s operations, is tasked to extract a witness
	% $\wit_\advse$ such that $\REL(\inp_\advse, \wit_\advse)$ holds. $\advse$ wins
	% if it manages to produce an acceptable proof and the extractor fails to reveal
	% the corresponding witness. In the following game hops we upper-bound
	% probability of this happening.
	% 
	% \ngame{1}
	% This is identical to $\game{0}$ except that now the game is aborted if there
	% is a simulated proof $\zkproof_\simulator$ such that
	% $\zkproof_\simulator\range{1}{3} = \zkproof_\advse\range{1}{3}$. That is, the
	% adversary in its final proof reuses a part of a simulated proof it saw before
	% and the proof is acceptable. Denote that event by $\event{\errur}$.
	% 
	% \ncase{$\game{0} \mapsto \game{1}$}	
	% We have, 
	% \[
	% 	\prob{\game{0} \land \nevent{\errur}} = \prob{\game{1} \land \nevent{\errur}}
	% \]
	% and, from \cref{lem:difference_lemma}
	% \[
	% 	\abs{\prob{\game{0}} - \prob{\game{1}}} \leq \event{\errur}\,.
	% \]
	% Thus, to show that the transition from one game to another introduces only minor change in probability of $\advse$ winning it should be shown that $\prob{\event{\errur}}$ is small.
	% 
	% Assume that $\advse$ queried the simulator on $\inp_{\advse}$---the instance which $\advse$ outputs. 
	% We show a reduction $\rdvur$ that utilises $\advse$, who outputs a valid proof for $\inp_\advse$, to break the $\ur{3}$ property of $\plonkprot$. 
	% 
	% Consider an algorithm $\rdvur$ that runs $\advse$ internally as a black-box:
	% \begin{itemize} \item The reduction answers both queries to the simulator
	% $\plonkprotfs.\simulator$ and to the random oracle.  It also keeps lists $Q$,
	% for the simulated proofs, and $Q_\ro$ for the random oracle queries.  \item
	% When $\advse$ outputs a fake proof $\zkproof_{\advse}$ for  $\inp_\advse$,
	% $\rdvur$ looks through lists $Q$ and $Q_\ro$ until it finds
	% $\zkproof_{\simulator}\range{1}{3}$ such that $\zkproof_{\advse}\range{1}{3} =
	% \zkproof_{\simulator}\range{1}{3}$ and a random oracle query
	% $\zkproof_{\simulator}[3].\ch$ on $\zkproof_{\simulator}\range{1}{3}$. \item
	% $\rdvur$ returns two proofs for $\inp_\advse$: \begin{align*} \zkproof_1 =
	% (\zkproof_{\simulator}\range{1}{3}, \zkproof_{\simulator}[3].\ch,
	% \zkproof_{\simulator}\range{4}{5})\\ \zkproof_2 =
	% (\zkproof_{\simulator}\range{1}{3}, \zkproof_{\simulator}[3].\ch,
	% \zkproof_{\advse}\range{4}{5}) \end{align*} \end{itemize}   If $\zkproof_1 =
	% \zkproof_2$, then $\advse$ fails to break simulation extractability, as
	% $\zkproof_2 \in Q$. On the other hand, if the proofs are not equal, then
	% $\rdvur$ breaks $\ur{3}$-ness of $\plonkprot$, what may happen with some
	% negligible probability $\epsur$ only, hence 
	% \[ 
	% \prob{\event{\errur}} \leq \epsur\,. 
	% \]
	% 
	% \ngame{2}
	% This is identical to $\game{1}$ except that now the environment aborts also when it fails to build a $(1, 1, 1, \numberofconstrains + 3, 1)$-tree of accepting transcripts $\tree$ by rewinding $\advse$. Denote that event 
	% by $\event{\errfrk}$.
	% Note that for every acceptable proof $\zkproof_{\advse}$, we may
	% assume that whenever $\advse$ outputs in Round $i$, for $i \in \smallset{4, 5}$, a message $\zkproof_{\advse}[i]$, then
	% $\zkproof_{\advse}\range{1}{i}$ is a query to the random oracle that was made by the adversary, not the simulator\footnote{\cite{INDOCRYPT:FKMV12} calls these queries \emph{fresh}.}. 
	% That is, assume that is not true and for some query $\zkproof_{\advse}\range{1}{i}$ holds $\zkproof_{\advse}\range{1}{i} = \zkproof_\simulator\range{1}{i'}$, for $i, i' \in \smallset{4, 5}$, the proof is acceptable and not in $Q$, then the unique response property would be broken.
	% 
	% \ncase{$\game{1} \mapsto \game{2}$}	
	% As previously, 
	% \[
	% 	\abs{\prob{\game{1}} - \prob{\game{2}}} \leq \event{\errfrk}\,.
	% \]
	% Denote by $\accProb$ the probability that $\advse$ outputs a proof which is acceptable and does not break $\ur{3}$-ness of $\plonkprot$. 
	% From the generalised forking lemma, cf.~\cref{lem:generalised_forking_lemma}, 
	% \[
	% 	\prob{\event{\errfrk}} \leq 1 - \left(\frac{\accProb^{\numberofconstrains + 3}}{q^{\numberofconstrains + 2}} - \frac{\accProb \cdot (\numberofconstrains + 2)}{2^\secpar}\right)\,.
	% \]
	% 
	% \ngame{3}
	% This is identical to $\game{2}$ except that now the environment uses the tree $\tree$ to extract the witness for the proven statement and aborts when it fails. Denote that event by $\event{\errss}$.
	% 
	% \ncase{$\game{2} \mapsto \game{3}$}	
	% As previously, 
	% \[
	% 	\abs{\prob{\game{2}} - \prob{\game{3}}} \leq \event{\errss}\,.
	% \]
	% Since $\plonkprot$ is special-sound the probability that $\env$ fails in extracting the witness is upper-bounded by some negligible $\eps_\ss$.
	% 
	% In the last game, Game $\game{3}$, the environment aborts when it fails to extract the correct witness, hence the adversary $\advse$ cannot win. 
	% Thus, by the game-hoping argument, 
	% \[
	% 	\abs{\prob{\game{0}} - \prob{\game{3}}} \leq 1 - \left(\frac{\accProb^{\numberofconstrains + 3}}{q^{\numberofconstrains + 2}} - \frac{\accProb \cdot (\numberofconstrains + 2)}{2^\secpar}\right) + \epsur + \epsss\,.
	% \]
	% Thus the probability that extractor $\extss$ succeeds is at least
	% \[
	% 	\left(\frac{\accProb^{\numberofconstrains + 3}}{q^{\numberofconstrains + 2}} - \frac{\accProb \cdot (\numberofconstrains + 2)}{2^\secpar}\right) - \epsur - \epsss\,.
	% \]
	% Since $\accProb$ is probability of $\advse$ outputting acceptable
	% transcript that does not break $\ur{3}$-ness of $\plonkprot$, then
	% $\accProb \leq \accProb + \epsur$, where $\accProb$ is the probability of $\advse$ outputing an acceptable proof as defined in \cref{def:simext}. It thus holds
	% \[
 	% 	\label{eq:frk}
 	% 	\extProb \geq \frac{(\accProb - \epsur)^{\numberofconstrains +
	% 	3}}{q^{\numberofconstrains + 2}} - \underbrace{\frac{(\accProb - \epsur) \cdot (\numberofconstrains + 2)}{2^\secpar} - \epsur - \epsss}_{\eps}\,.
 	% \]
 	% Note that the part of \cref{eq:frk} denoted by $\eps$ is negligible and 
 	% \[
 	% 	\extProb \geq \frac{1}{q^{\numberofconstrains + 2}} (\accProb - \epsur)^{\numberofconstrains + 3} -\eps\,.
 	% \] 
 	% thus
 	% $\plonkprot_\fs$ is simulation extractable with extraction error $\epsur$.
 % 	\qed
 % \end{proof}

\section{Simulation extractability of $\sonicprotfs$}
\subsection{Unique opening property of $\PCOMs$}
\begin{lemma}
	\label{lem:pcoms_unique_op}
	Let $\PCOMs$ be a batched version of a KZG polynomial commitment
	\cite{AC:KatZavGol10} as described in \cite{CCS:MBKM19} then $\PCOMs$ has the unique opening property. 
\end{lemma}
\begin{proof}
	Let 
	$z \in \FF_p$ be the attribute the polynomial is evaluated at,
	$c \in \GRP$ be the commitment,  
	$s \in \FF_p$ the evaluation, and 
	$o \in \FF_p$ be the commitment opening. 
	We need to show that for every $\ppt$ adversary $\adv$ probability
	\[
		\Pr
			\left[
				\begin{aligned}
					& \verify(\crs, c, z, s, o), \\
					& \verify(\crs, c, z, \tilde{s}, \tilde{o})
				\end{aligned}
			\,\left|\,
			\vphantom{\begin{aligned}
				& \verify(\crs, c, z, s, o),\\
				& \verify(\crs, c, z, \tilde{s}, \tilde{o}) \\
				&o \neq \tilde{o})
			\end{aligned}}
			\begin{aligned}
				& \crs \gets \kcrs(\secparam), \\
				&	(c, z, s, \tilde{s}, o, \tilde{o}) \gets \adv(\crs)
			\end{aligned}
			\right.\right]
		 % \leq \negl.
	\]
	is at most negligible.
	
	As noted in \cite[Lemma 2.2]{EPRINT:GabWilCio19} it is enough to upper bound
  the probability of the adversary succeeding using the idealised verification equation---which considers equality between polynomials---instead of the real verification equation---which consider equality of the polynomials' evaluations.
	
	For a polynomial $f$, its degree upper bound $\maxconst$, evaluation point $z$, evaluation result $s$, and opening $o(X)$ the idealised check verifies that
	\begin{equation}
		\alpha (X^{\dconst - \maxconst}f(X) \cdot X^{-\dconst + \maxconst} -  s) \equiv \alpha \cdot o(X) (X - z)\,,
	\end{equation}
	what is equivalent to 
	\begin{equation}
		f(X) -  s \equiv o(X) (X - z)\,.
		\label{eq:pcoms_idealised_check}
	\end{equation}
	Since $o(X)(X - z) \in \FF_p[X]$ then from the uniqueness of polynomial composition, there is only one $o(X)$ that fulfils the equation above.

	\qed
\end{proof}


\subsection{Unique response property}
The unique response property of $\sonicprot$ follows from the unique opening
property of the used polynomial commitment scheme $\PCOMs$.
\begin{lemma}
	\label{lem:sonicprot_ur}
	If a polynomial commitment scheme $\PCOMs$ is evaluation binding with
	parameter $\epsbind$ and has unique openings property with parameter
	$\epsop$, then $\sonicprot$ is $\ur{1}$ with parameter $\epsur \leq
	\epsbind + \epsop$.  
\end{lemma}
\begin{proof}
	Let $\adv$ be an adversary that breaks $\ur{1}$-ness of $\sonicprot$. 
	We consider two cases, depending on which round $\adv$ is able to provide at
	least two different outputs such that the resulting transcripts are
	acceptable.  For the first case we show that $\adv$ breaks the evaluation
	binding property of $\PCOMs$, while for the
  second case we show that it breaks the unique opening property of $\PCOMs$.
	
	The proof goes similarly to the proof of \cref{lem:plonkprot_ur} thus we
	provide only draft of it here. 
	In each Round $i$, for $i > 1$, the prover either commits to some
	well-defined polynomials (deterministically), evaluates these on some
	randomly picked points, or shows that the evaluations were performed
	correctly. 
	Naturally for a committed polynomial $\p{p}$ evaluated at point $x$ only one
	value $y = \p{p}(x)$ is correct. If the adversary was able to provide two
	different values $y$ and $\tilde{y}$ that would be accepted as an evaluation
	of $\p{p}$ at $x$ then the $\PCOMs$'s evaluation binding would be broken.
	Alternatively, if $\adv$ was able to provide two openings $\p{W}$ and
	$\p{\tilde{W}}$ for $y = \p{x}$ then the unique opening property would be
	broken.

	Hence the probability that $\adv$ breaks $\ur{1}$-property of $\PCOMs$ is
	upper-bounded by $\epsbind + \epsop$. \michals{22.X}{Do we need to multiply
	the epsilons by some constant that relates to the number of chances
	the adversary can break the binding and op properties?} 
	\qed
	
\end{proof}

\subsection{Computational special soundness}
\begin{lemma}
		\label{lem:sonicprot_ss}
		Let $\adv$ be a $\ppt$ algebraic adversary. The probability $\epsss$ that
		$\adv$ breaks computational special soundness of $\sonicprot$ is upper-bounded as
		\[
				\epsss \leq \epss + \epsldlog\,,
		\]
		where $\epss$ is a soundness error of the protocol, and $\epsldlog$ is a probability that a $\ppt$ algorithm can break the
		$(\dconst, \dconst)$-$\ldlog$ assumption.
\end{lemma}
\begin{proof}
		The proof goes similarly to the proof of \cref{lem:plonkprot_ss}.
%
		Let $\adv$ be an adversary that produces a $(1, 1, \multconstr + \linconstr + 1, 1, 1)$-tree of acceptable
		transcripts $\tree$ for a statement $\inp$. We consider two disjunctive
		events $\event{E}$ and $\nevent{E}$. The first corresponds to a case when
		all transcripts in $\tree$ are acceptable for the ideal verifier,
		i.e.~$\vec{\vereq}(X) = \vec{0}$. In that case we show an extractor $\extss$ that
		from $\tree$ extracts a valid witness $\wit$. The second, corresponds to
		a case when $\tree$ contains a transcript that is acceptable by the
		real verifier but is not acceptable by the ideal verifier. In that case we
		show a reduction $\rdvldlog$ that uses $\adv$ to break the $(\dconst,
		\dconst)$-$\ldlog$
		assumption with at least the same probability.

		\ncase{When $\event{E}$ happens}
		Since $\sonicprot$ is perfectly sound regarding the ideal verifier, for an
		acceptable proof $\zkproof$ for a statement $\inp$ there exists a witness
		$\wit$ such that $\REL(\inp, \wit)$ holds and the polynomial $\p{r}(X, Y)$
		contains witness at its coefficients.  Note that the witness-carrying
		polynomial $\p{r}(X, y)$ has degree at most $(\multconstr + \linconstr)$
		and since $\adv$ answered correctly on $(\multconstr + \linconstr + 1)$
		different challenges $z$ (for the sake of concreteness let us call them
		$z_1, \ldots, z_{\multconstr + \linconstr + 1}$) then $(\multconstr +
		\linconstr + 1)$ evaluations $\p{r}(z_1, y), \ldots, \p{r}(z_{\multconstr
		+ \linconstr + 1}, y)$ of $\p{r}(X, y)$ are known. The extractor $\extss$
		interpolates $\p{r}(X, y)$ and reveals the corresponding witness $\wit$.

		\ncase{When $\nevent{E}$ happens} Consider a transcript such that for some verification equation
		$\vereq_i(X) \neq 0$, but $\vereq_i(\chi) = 0$.
		Since the adversary is algebraic, all group elements included in the tree of
  	transcripts are extended by their representation as a combination of the input
  	$\GRP_1$ or $\GRP_2$-elements. Hence all coefficients of the verification equation polynomial
  	$\vereq_i(X)$ are known and $\rdvdlog$ can find its zero points. Since
  	$\vereq_i(\chi) = 0$, the targeted discrete log value $\chi$ is among them.
		\qed		
\end{proof}

\subsection{Honest verifier zero-knowledge}
\begin{lemma}
	\label{lem:sonic_hvzk}
	$\sonic$ is honest verifier
	zero-knowledge.	
\end{lemma}
% Here we show that $\sonic$ is trapdoor-less zero-knowledge.
\begin{proof}
The simulator proceeds as follows.
In the first round, it picks randomly vectors $\vec{a}$, $\vec{b}$ and sets
\begin{equation}
		\label{eq:ab_eq_c}
		\vec{c} = \vec{a} \cdot \vec{b}. 
\end{equation}
Then it pick randomisers $c_{\multconstr + 1}, \ldots, c_{\multconstr
+ 4}$, honestly computes polynomials
$\p{r}(X, Y), \p{r'}(X, Y), \p{s}(X, Y)$ and $\p{t}(X, Y)$ and concludes the
first round as an honest prover would. Because of the randomisers
polynomial $\pr(X, Y)$ computed by the simulator is indistinguishable from a
polynomial provided by an honest user for a distinguisher that only learns
$R,T,a,b$ from the proof (the other proof elements are fixed by these elements
and the public information). See~\cref{sec:sonic_constraint_system}.
%less than three evaluations of $\pr(X, Y)$. 

Next, $\simulator$ computes the first verifier's challenge $y$ such that $\p{t}(X,
y)$ is a polynomial that has $0$ as a coefficient next to $X^0$. I.e.~$\pt(0,
y) = 0$.
As noted in \cite{CCS:MBKM19}, the coefficient next to $X^0$ in $\p{t}(X, Y)$
equals
\begin{equation}
		\label{eq:x0}
		t(0,Y) = 
		\vec{a} \cdot \vec{\p{u}}(Y) + 
		\vec{b} \cdot \vec{\p{v}}(Y) + 
		\vec{c} \cdot \vec{\p{w}}(Y) +	
		\sum_{i = 1}^{\multconstr} a_i b_i (Y^i + Y^{-i}) - \p{k}(Y), 
\end{equation}
for public $\vec{\p{u}}(Y), \vec{\p{v}}(Y), \vec{\p{w}}(Y), \p{k}(Y)$ as defined in \cref{sec:sonic_constraint_system}
% \begin{align*}
% 		\p{u_i}(Y) & = \sum_{q = 1}^\linconstr Y^{q + \multconstr} u_{q, i}\\
% 		\p{v_i}(Y) & = \sum_{q = 1}^\linconstr Y^{q + \multconstr} v_{q, i}\\
% 		\p{w_i}(Y) & = -Y^i - Y^{-i}  + \sum_{q = 1}^\linconstr Y^{q +
% 		\multconstr} w_{q, i}\\
% 		\p{k}(Y) & = \sum_{q = 1}^\linconstr Y^{q + \multconstr} k_{q}.
% \end{align*}
(Vectors $\vec{u_q}, \vec{v_q}, \vec{w_q}$ are $\multconstr$-elements long
and correspond to the $Q$ linear constrains of the proof system. Field element
$k_{q}$ is
the instance value). When the proven instance is correct, $t(0,Y)$ is a zero
polynomial. See \cite{CCS:MBKM19} for  details.
Also, when \cref{eq:ab_eq_c} holds, that polynomial simplifies to
\begin{equation}
		\label{eq:x0_simpl}
		t(0,Y) = \vec{a} \cdot \vec{\p{u}}(Y) + 
		\vec{b} \cdot \vec{\p{v}}(Y) + 
		\vec{c} \cdot \vec{\p{\tilde{w}}}(Y)	
		- \p{k}(Y), 
\end{equation}
where $\vec{\p{\tilde{w}}}(Y)$ is defined as
\[
		\p{\tilde{w}_i}(Y) = \sum_{q = 1}^\linconstr Y^{q +
		\multconstr} w_{q, i}\,.
\]
Note that the polynomial from \cref{eq:x0_simpl} is a ``classical'',
i.e.~non-Laurent, polynomial. Also,  $t(0,Y)$ is a polynomial of degree $(\linconstr +
\multconstr)$ with $0$ coefficients for $Y^{i}$, for $i \in
\range{0}{\multconstr}$. Also, since $\vec{a}, \vec{b}$ were picked at
random, $\pt(0,Y) / Y^{\multconstr + 1}$ is
a degree-$(\linconstr - 1)$ polynomial of random coefficients. Recall, that the
view of the adversary is independent of $\vec{a}, \vec{b}$ because of randomisers $c_{\multconstr + 1}, \ldots, c_{\multconstr
  + 4}$.

The probability that a random degree-$(\linconstr - 1)$ polynomial over
$\FF_p[Y]$ has a root is at least $\infrac{1}{(\linconstr - 1) !}$, see
\cref{lem:root_prob} for a proof of that bound. Since we assume that $\linconstr
= \poly$, we can say that the polynomial $\pt(0,Y)$ as computed by the simulator
has roots with non-negligible probability. Furthermore, these roots can be found
and the simulator picks fresh $\vec{a}, \vec{b}$ until $\pt(0,Y)$ has a root. As
the roots of a random polynomial are themselves random $\FF_p$ element, the
challenge $y$ picked by the simulator comes from the same distribution as if it
was picked by an honest verifier.

The simulator continues building the transcript by honestly computing the
prover's messages and by picking verifier's challenges at random. This and the
fact that $\pt(0,y) = 0$ guarantees that the transcript provided by the simulator is
acceptable and comes from the same distribution as a transcript of an honest
prover and verifier. \qed
\end{proof}

\begin{remark} 
	As noted in \cite{CCS:MBKM19}, $\sonic$ is statistically
	subversion-zero knowledge (Sub-ZK). As noted in \cite{AC:ABLZ17}, one way to
	achieve subversion zero knowledge is to utilise an extractor that extracts a
	CRS trapdoor from a CRS-generator. Unfortunately, a NIZK made subversion
	zero-knowledge by this approach cannot achieve perfect Sub-ZK as one has to
	count in the probability of extraction failure. However, with the simulation
	presented in \cref{lem:sonic_hvzk}, the trapdoor is not required for the
	simulator as it is able to simulate the execution of the protocol just by
	picking appropriate (honest) verifier's challenges. This result transfers to
	$\sonicprotfs$, where the simulator can program the random oracle to provide
	challenges that fits it.
\end{remark}

\subsection{From special-soundness and unique response property to simulation extractability of $\sonicprotfs$}
Since \cref{lem:sonicprot_ur,lem:sonicprot_ss} hold, $\sonicprot$ is $\ur{1}$
and computationally special sound. We now make use
of \cref{thm:se} and show that $\sonicprotfs$ is simulation-extractable as defined in \cref{def:simext}.

\begin{corollary}[Simulation extractability of $\sonicprotfs$]
	\label{thm:sonicprotfs_se}
	Assume that  $\sonicprot$ is $\ur{1}$ with security $\epsur(\secpar)$, and
	computational special-sound with security $\epsss(\secpar)$.  Let $\ro\colon
	\bin^* \to \bin^\secpar$ be a random oracle. 
	Let $\advse$ be a $\ppt$ adversary that can make up to $q$ random oracle
	queries and outputs an acceptable proof for $\sonicprotfs$ with probability
	at least $\accProb$.  Then $\sonicprotfs$ is simulation-extractable with
	extraction error $\eta = \epsur$. The extraction probability $\extProb$ is at least
	\[
			\extProb  \geq \frac{1}{q^{\multconstr + \linconstr}} (\accProb - \epsur)^{\multconstr +
			\linconstr + 1} - \eps.
	\]
	for some negligible $\eps$, $\multconstr$ and $\linconstr$ being,
	respectively, the multiplicative and linear constrains of the system.
\end{corollary}

\section{Further work}

\bibliographystyle{alpha}
\bibliography{cryptobib/abbrev1,cryptobib/crypto,additional_bib}
\clearpage
\appendix
{\Huge{Supplementary Material}}
\section{$\plonk$ protocol rolled out}
\label{sec:plonk_explained}
\ncase{$\plonk$ CRS generating algorithm $\kgen(\secparam)$}
For the sake of simplicity of security reductions presented in this paper, we
describe here a simplified CRS generating algorithm which produces only
these CRS elements that cannot be computed without knowing the secret trapdoor
$\chi$. The rest of the preprocessed input, as it is called in
\cite{EPRINT:GabWilCio}, can be computed using these CRS elements thus we leave
them to be computed by the prover, verifier, and simulator. 

The CRS generating algorithm picks at random $\chi \sample \FF_p$, computes
and outputs
\[
	\crs = \left(\gone{\smallset{\chi^i}_{i = 0}^{\numberofconstrains + 2}},
	\gtwo{\chi} \right).
\]

\ncase{$\plonk$ prover $\prover(\REL, \crs, \inp, \wit = (\wit_i)_{i \in \range{1}{3 \cdot \numberofconstrains}})$}
\begin{description}
	\item[Round 1] 
	Sample $b_1, \ldots, b_9 \sample \FF_p$; compute $\p{a}(X), \p{b}(X), \p{c}(X)$ as 
	\begin{align*}
		\p{a}(X) &= (b_1 X + b_2)\p{Z_H}(X) + \sum_{i = 1}^{\numberofconstrains} \wit_i \lag_i(X) \\
		\p{b}(X) &= (b_3 X + b_4)\p{Z_H}(X) + \sum_{i = 1}^{\numberofconstrains} \wit_{\numberofconstrains + i} \lag_i(X) \\
		\p{c}(X) &= (b_5 X + b_6)\p{Z_H}(X) + \sum_{i = 1}^{\numberofconstrains} \wit_{2 \cdot \numberofconstrains + i} \lag_i(X) 
	\end{align*}
	Output $\gone{\p{a}(\chi), \p{b}(\chi), \p{c}(\chi)}$.
	
	\item[Round 2]
	Get challenges $\beta, \gamma \in \FF_p$
	\[
		\beta = \ro(\trans, 0)\,, \qquad \gamma = \ro(\trans, 1)\,.
	\]
	Compute permutation polynomial $\p{z}(X)$
	\begin{multline*}
		\p{z}(X) = (b_7 X^2 + b_8 X + b_9)\p{Z_H}(X) + \lag_1(X) + \\
			+ \sum_{i = 1}^{\numberofconstrains - 1} 
			\left(\lag_{i + 1} (X) \prod_{j = 1}^{i} 
			\frac{
			(\wit_j +\beta \omega^{j - 1} + \gamma)(\wit_{\numberofconstrains + j} + \beta k_1 \omega^{j - 1} + \gamma)(\wit_{2 \numberofconstrains + j} +\beta k_2 \omega^{j- 1} + \gamma)}
			{(\wit_j+\sigma(j) \beta + \gamma)(\wit_{\numberofconstrains + j} + \sigma(\numberofconstrains + j)\beta + \gamma)(\wit_{2 \numberofconstrains + j} + \sigma(2 \numberofconstrains + j)\beta + \gamma)}\right)
	\end{multline*}
	Output $\gone{\p{z}(\chi)}$
		
	\item[Round 3]
	Get the challenge $\alpha = \ro(\trans)$, compute the quotient polynomial 
	\begin{align*}
	& \p{t}(X)  = \\
	& (\p{a}(X) \p{b}(X) \selmulti(X) + \p{a}(X) \selleft(X) + 
	\p{b}(X)\selright(X) + \p{c}(X)\seloutput(X) + \pubinppoly(X) + \selconst(X)) 
	\frac{1}{\p{Z_H}(X)} +\\
	& + ((\p{a}(X) + \beta X + \gamma) (\p{b}(X) + \beta k_1 X + \gamma)(\p{c}(X) 
	+ \beta k_2 X + \gamma)\p{z}(X)) \frac{\alpha}{\p{Z_H}(X)} \\
	& - (\p{a}(X) + \beta \p{S_{\sigma 1}}(X) + \gamma)(\p{b}(X) + \beta 
	\p{S_{\sigma 2}}(X) + \gamma)(\p{c}(X) + \beta \p{S_{\sigma 3}}(X) + 
	\gamma)\p{z}(X \omega))  \frac{\alpha}{\p{Z_H}(X)} \\
	& + (\p{z}(X) - 1) \lag_1(X) \frac{\alpha^2}{\p{Z_H}(X)}
	\end{align*}
	Split $\p{t}(X)$ into degree less then $\numberofconstrains$ polynomials $\p{t_{lo}}(X), \p{t_{mid}}(X), \p{t_{hi}}(X)$, such that
	\[
		\p{t}(X) = \p{t_{lo}}(X) + X^{\numberofconstrains} \p{t_{mid}}(X) + X^{2 \numberofconstrains} \p{t_{hi}}(X)\,.
	\]
	Output $\gone{\p{t_{lo}}(\chi), \p{t_{mid}}(\chi), \p{t_{hi}}(\chi)}$.
	
	\item[Round 4]
	Get the challenge $\chz \in \FF_p$, $\chz = \ro(\trans)$.
	Compute opening evaluations
	\begin{align*}
			\p{a}(\chz), \p{b}(\chz), \p{c}(\chz), \p{S_{\sigma 1}}(\chz), \p{S_{\sigma 2}}(\chz), \p{t}(\chz), \p{z}(\chz \omega),
	\end{align*}
	Compute the linearisation polynomial
	\[
		\p{r}(X) = 
		\begin{aligned}
			& \p{a}(\chz) \p{b}(\chz) \selmulti(X) + \p{a}(\chz) \selleft(X) + \p{b}(\chz) \selright(X) + \p{c}(\chz) \seloutput(X) + \selconst(X) \\
			& + \alpha \cdot \left( (\p{a}(\chz) + \beta \chz + \gamma) (\p{b}(\chz) + \beta k_1 \chz + \gamma)(\p{c}(\chz) + \beta k_2 \chz + \gamma) \cdot \p{z}(X)\right) \\
			& - \alpha \cdot \left( (\p{a}(\chz) + \beta \p{S_{\sigma 1}}(\chz) + \gamma) (\p{b}(\chz) + \beta \p{S_{\sigma 2}}(\chz) + \gamma)\beta \p{z}(\chz\omega) \cdot \p{S_{\sigma 3}}(X)\right) \\
			& + \alpha^2 \cdot \lag_1(\chz) \cdot \p{z}(X)
		\end{aligned}
	\]
	Output $\p{a}(\chz), \p{b}(\chz), \p{c}(\chz), \p{S_{\sigma 1}}(\chz), \p{S_{\sigma 2}}(\chz), \p{t}(\chz), \p{z}(\chz \omega), \p{r}(\chz).$
	
	\item[Round 5]
	Compute the opening challenge $v \in \FF_p$, $v = \ro(\trans)$.
	Compute the openings for the polynomial commitment scheme 
	\begin{align*}
	& \p{W_\chz}(X) = \frac{1}{X - \chz} \left(
	\begin{aligned}
		& \p{t_{lo}}(X) + \chz^\numberofconstrains \p{t_{mid}}(X) + \chz^{2 \numberofconstrains} \p{t_{hi}}(X) - \p{t}(\chz)\\
		& + v(\p{r}(X) - \p{r}(\chz)) \\
		& + v^2 (\p{a}(X) - \p{a}(\chz))\\
		& + v^3 (\p{b}(X) - \p{b}(\chz))\\
		& + v^4 (\p{c}(X) - \p{c}(\chz))\\
		& + v^5 (\p{S_{\sigma 1}}(X) - \p{S_{\sigma 1}}(\chz))\\
		& + v^6 (\p{S_{\sigma 2}}(X) - \p{S_{\sigma 2}}(\chz))
	\end{aligned}
	\right)\\
	& \p{W_{\chz \omega}}(X) = \frac{\p{z}(X) - \p{z}(\chz \omega)}{X - \chz \omega}
\end{align*}
	Output $\gone{\p{W_{\chz}}(\chi), \p{W_{\chz \omega}}(\chi)}$.
\end{description}

\ncase{$\plonk$ verifier $\verifier(\REL, \crs, \inp, \zkproof)$}\ \newline
The \plonk{} verifier works as follows
\begin{description}
	\item[Step 1] Validate all obtained group elements.
	\item[Step 2] Validate all obtained field elements.
	\item[Step 3] Validate the instance $\inp = \smallset{\wit_i}_{i = 1}^\instsize$.
	\item[Step 4] Compute challenges $\beta, \gamma, \alpha, \alpha', \chz, v, u$ from the transcript.
	\item[Step 5] Compute zero polynomial evaluation $\p{Z_H} (\chz)  =\chz^\numberofconstrains - 1$.
	\item[Step 6] Compute Lagrange polynomial evaluation $\lag_1 (\chz) = \frac{\chz^\numberofconstrains -1}{\numberofconstrains (\chz - 1)}$.
	\item[Step 7] Compute public input polynomial evaluation $\pubinppoly (\chz) = \sum_{i \in \range{1}{\instsize}} \wit_i \lag_i(\chz)$.
	\item[Step 8] Compute quotient polynomials evaluations
	\begin{multline*}
		\p{t} (\chz)  = \frac{1}{\p{Z_H}(\chz)}
		\Big(
			\p{r} (\chz) + \pubinppoly(\chz) - (\p{a}(\chz) + \beta \p{S_\sigma 1}(\chz) + \gamma) (\p{b}(\chz) + \beta \p{S_\sigma 2}(\chz) + \gamma) \\
			(\p{c}(\chz) +
			\gamma)\p{z}(\chz \omega) \alpha - \lag_1 (\chz) \alpha^2
		\Big) \,.
	\end{multline*}
	\item[Step 9] Compute batched polynomial commitment
	$\gone{D} = v \gone{r} + u \gone {z}$ that is
	\begin{align*}
		\gone{D} & = v
		\left(
		\begin{aligned}
			& \p{a}(\chz)\p{b}(\chz) \cdot \gone{\selmulti} + \p{a}(\chz)  \gone{\selleft} + \p{b}  \gone{\selright} + \p{c}  \gone{\seloutput} + \\
			& + (	(\p{a}(\chz) + \beta \chz + \gamma) (\p{b}(\chz) + \beta k_1 \chz + \gamma) (\p{c} + \beta k_2 \chz + \gamma) \alpha  + \lag_1(\chz) \alpha^2)  + \\
			% &   \\
			& - (\p{a}(\chz) + \beta \p{S_{\sigma 1}}(\chz) + \gamma) (\p{b}(\chz) + \beta \p{S_{\sigma 2}}(\chz) + \gamma) \alpha  \beta \p{z}(\chz \omega) \gone{\p{S_{\sigma 3}}(\chi)})
		\end{aligned}
		\right) + \\
		& + u \gone{\p{z}(\chi)}\,.
	\end{align*}
	\item[Step 10] Computes full batched polynomial commitment $\gone{F}$:
	\begin{align*}
		\gone{F} & = \left(\gone{\p{t_{lo}}(\chi)} + \chz^\numberofconstrains \gone{\p{t_{mid}}(\chi)} + \chz^{2 \numberofconstrains} \gone{\p{t_{hi}}(\chi)}\right) + u \gone{\p{z}(\chi)} + \\
		& + v
		\left(
		\begin{aligned}
			& \p{a}(\chz)\p{b}(\chz) \cdot \gone{\selmulti} + \p{a}(\chz)  \gone{\selleft} + \p{b}(\chz)   \gone{\selright} + \p{c}(\chz)  \gone{\seloutput} + \\
			& + (	(\p{a}(\chz) + \beta \chz + \gamma) (\p{b}(\chz) + \beta k_1 \chz + \gamma) (\p{c}(\chz)  + \beta k_2 \chz + \gamma) \alpha  + \lag_1(\chz) \alpha^2)  + \\
			% &   \\
			& - (\p{a}(\chz) + \beta \p{S_{\sigma 1}}(\chz) + \gamma) (\p{b}(\chz) + \beta \p{S_{\sigma 2}}(\chz) + \gamma) \alpha  \beta \p{z}(\chz \omega) \gone{\p{S_{\sigma 3}}(\chi)})
		\end{aligned}
		\right) \\
		& + v^2 \gone{\p{a}(\chi)} + v^3 \gone{\p{b}(\chi)} + v^4 \gone{\p{c}(\chi)} + v^5 \gone{\p{S_{\sigma 1}(\chi)}} + v^6 \gone{\p{S_{\sigma 2}}(\chi)}\,.
	\end{align*}
	\item[Step 11] Compute group-encoded batch evaluation $\gone{E}$
	\begin{align*}
		\gone{E}  = \frac{1}{\p{Z_H}(\chz)} & \gone{
		\begin{aligned}
			& \p{r}(\chz) + \pubinppoly(\chz) +  \alpha^2  \lag_1 (\chz) + \\
			& - \alpha \left( (\p{a}(\chz) + \beta \p{S_{\sigma 1}} (\chz) + \gamma) (\p{b}(\chz) + \beta \p{S_{\sigma 2}} (\chz) + \gamma) (\p{c}(\chz) + \gamma) \p{z}(\chz \omega) \right)
		\end{aligned}
		}\\
		 + & \gone{v \p{r}(\chz) + v^2 \p{a}(\chz) + v^3 \p{b}(\chz) + v^4 \p{c}(\chz) + v^5 \p{S_{\sigma 1}}(\chz) + v^6 \p{S_{\sigma 2}}(\chz) + u \p{z}(\chz \omega) }\,.
	\end{align*}
	\item[Step 12] Check whether the verification $\vereq(\chi)$ equation holds
	\begin{multline}
		\label{eq:ver_eq}
		\left(
		\gone{\p{W_{\chz}}(\chi)} + u \cdot \gone{\p{W_{\chz \omega}}(\chi)}
		\right) \bullet
		\gtwo{\chi} - \\
		\left(
			\chz \cdot \gone{\p{W_{\chz}}(\chi)} + u \chz \omega \cdot \gone{\p{W_{\chz \omega}}(\chi)} + \gone{F} - \gone{E}
		\right) \bullet
		\gtwo{1} = 0\,.
	\end{multline}
The verification equation is a batched version of the verification equation from \cite{AC:KatZavGol10} which allows the verifier to check openings of multiple polynomials in two points (instead of checking an opening of a single polynomial at one point).
\end{description}

Since the original paper \cite{EPRINT:GabWilCio19} lacks of explanation how the simulator of \plonk{} works, it is presented here.

\COMMENT{
\ncase{$\plonk$ simulator $\simulator_\chi(\REL, \crs, \td, \inp)$}
% \paragraph{Simulation in \plonk.}
% The simulator $\simulator$ in $\plonk$ proceeds according to the following steps:
\begin{description}
	\item[Round 1]
	Since the simulator does not know a witness $\wit$ for the proven statement
	$\inp$, $\simulator_\chi$ cannot compute the output of this round accordingly to
	the protocol. Instead, it picks randomly both the randomisers $b_1, \ldots, b_6$ and evaluations of polynomials $\p{a}, \p{b}, \p{c}$ by picking their coefficients randomly and outputting $\gone{\p{a}(\chi), \p{b}(\chi), \p{c}(\chi)}$.
	\item[Round 2]
	The simulator takes permutation argument challenges $\beta, \gamma$ as a random oracle output in the ongoing proof.
	Similarly as in the previous round, the simulator cannot evaluate the requested polynomial $\p{z}$ honestly as it does not know the witness, picks its coefficients randomly and outputs $\gone{\p{z}(\chi)}$.
	\item[Round 3]
	In this round the simulator starts by picking at random a challenge $\chz$ that will be later used to program a random oracle.
	Then it computes evaluations $\p{a}(\chz), \p{b}(\chz), \p{c}(\chz), \p{S_{\sigma 1}}(\chz), \p{S_{\sigma 2}}(\chz), \pubinppoly(\chz), \lag_1(\chz), \p{Z_H}(\chz),\allowbreak \p{z}(\chz\omega)$
	
	Given the evaluations $\simulator_\chi$ computes polynomial $\p{r}(X)$ honestly, i.e.
	\[
		\p{r}(X) = 
		\begin{aligned}
			& \p{a}(\chz) \p{b}(\chz) \selmulti(X) + \p{a}(\chz) \selleft(X) + \p{b}(\chz) \selright(X) + \p{c}(\chz) \seloutput(X) + \selconst(X) \\
			& + \alpha \cdot \left( (\p{a}(\chz) + \beta \chz + \gamma) (\p{b}(\chz) + \beta k_1 \chz + \gamma)(\p{c}(\chz) + \beta k_2 \chz + \gamma) \cdot \p{z}(X)\right) \\
			& - \alpha \cdot \left( (\p{a}(\chz) + \beta \p{S_{\sigma 1}}(\chz) + \gamma) (\p{b}(\chz) + \beta \p{S_{\sigma 2}}(\chz) + \gamma)\beta \p{z}(\chz\omega) \cdot \p{S_{\sigma 3}}(X)\right) \\
			& + \alpha^2 \cdot \lag_1(\chz) \cdot \p{z}(X)
		\end{aligned}
	\]
	and evaluates $\p{r}(X)$ at $\chz$.
	
	In the next step the simulator computes $\ev{t}$ as the verifier would compute in Step 8.
	Next, $\simulator_\chi$ picks randomly a polynomial $\p{t}$ such that $\p{t} (\chz) = \ev{t}$.
	The simulator concludes this round as an honest prover would by dividing $\p{t}$ into $\p{t_{hi}}, \p{t_{mid}}, \p{t_{lo}}$ and outputting $\gone{\p{t_{hi}}(\chi), \p{t_{mid}}(\chi), \p{t_{lo}}(\chi)}$. 
	\item[Round 4]
	The simulator program random oracle to return $\chz$ when queried on the current state of the transcript. 
	Since the necessary evaluations at $\chz$ are already computed,
	$\simulator_\chi$ simply outputs 
	\[
		\p{a}(\chz), \p{b}(\chz), \p{c}(\chz), \p{S_{\sigma 1}}(\chz), \p{S_{\sigma 2}}(\chz), \p{t}(\chz), \p{z}(\chz \omega)\,.
	\]
	\item[Round 5]
	In this round the simulator proceeds as an honest prover would.
	\end{description}
}

	\section{$\sonic$ protocol rolled out}
	\label{sec:sonic_explained}
	\subsection{The constraint system}
	\label{sec:sonic_constraint_system}
	\sonic's system of constraints composes of three $\multconstr$-long vectors
	$\va, \vb, \vc$ which corresponds to left and right inputs to multiplication
	gates and their outputs. It hence holds $\va \cdot \vb = \vc$. 

	There is also $\linconstr$ linear constrains of the form 
	\[
		\va \vec{u_q} + \vb \vec{v_q} + \vc \vec{w_q} = k_q,
	\]
	where $\vec{u_q}, \vec{v_q}, \vec{w_q}$ are vectors for the $q$-th linear
	constraint with instance value $k_q \in \FF_p$. Furthermore define polynomials
\begin{equation}
	\begin{split}
		\p{u_i}(Y) & = \sum_{q = 1}^\linconstr Y^{q + \multconstr} u_{q, i}\,,\\
		\p{v_i}(Y) & = \sum_{q = 1}^\linconstr Y^{q + \multconstr} v_{q, i}\,,\\
	\end{split}
	\qquad
	\begin{split}
		\p{w_i}(Y) & = -Y^i - Y^{-i}  + \sum_{q = 1}^\linconstr Y^{q +
		\multconstr} w_{q, i}\,,\\
		\p{k}(Y) & = \sum_{q = 1}^\linconstr Y^{q + \multconstr} k_{q}.
	\end{split}
\end{equation}
In \sonic{} we will use commitments to the following polynomials.
\begin{align*}
	\pr(X, Y) & = \sum_{i = 1}^{\multconstr} \left(a_i X^i Y^i + b_i X^{-i} Y^{-i}
	+ c_i X^{-i - \multconstr} Y^{-i - \multconstr}\right) \\
	\p{s}(X, Y) & = \sum_{i = 1}^{\multconstr} \left( u_i (Y) X^{-i} +
	v_i(Y) X^i + w_i(Y) X^{i + \multconstr}\right)\\
		\pr'(X, Y) & = \pr(X, Y) + \p{s}(X, Y) \\
		\pt(X, Y) & = \pr(X, 1) \pr'(X, Y) - \p{k}(Y)\,.
\end{align*}
	
	\subsection{Algorithms rolled out}
	\ncase{$\sonic$ CRS generation $\kgen(\secparam)$}
	The CRS generating algorithm picks randomly $\alpha, \chi \sample \FF_p$ and
	outputs 
	\[
		\crs = \left( \gone{\smallset{\chi^i}_{i = -\dconst}^{\dconst},
			\smallset{\alpha \chi^i}_{i = -\dconst, i \neq 0}^{\dconst}},
			\gtwo{\smallset{\chi^i, \alpha \chi^i}_{i = - \dconst}^{\dconst}},
		\gtar{\alpha} \right)
	\]
	\ncase{$\sonic$ prover $\prover(\REL, \crs, \inp, \wit=\va, \vb,
	\vc)$}
\begin{description}
	\item[Round 1]
		The prover picks randomly randomisers $c_{\multconstr + 1}, c_{\multconstr + 2},
		c_{\multconstr + 3}, c_{\multconstr + 4} \sample \FF_p$. Set $\pr(X, Y)
		\gets \pr(X, Y) + \sum_{i = 1}^4 c_{\multconstr + i} X^{- 2 \multconstr -
		i}$. Commit to $\pr(X, 1)$ by setting and outputting $R \gets \com(\crs, \multconstr,
		\pr(X, 1))$.
		Then it gets challenge $y$ from the verifier.
	\item[Round 2] $\prover$ commits to $\pt(X, y)$ by setting and outputting $T
		\gets \com(\crs, \dconst, \pt(X, y))$. Then it gets a challenge $z$ from
		the verifier.
	\item[Round 3] The prover computes commitment openings. That is, it outputs
		\begin{align*}
			W_a & = \open(\crs, z, \pr(z, 1), \pr(X, 1)) \\
			W_b & = \open(\crs, yz, \pr(yz, 1), \pr(X, 1)) \\
			W_t & = \open(\crs, z, \pt(z, y), \pt(X, y)) 
		\end{align*}
		along with evaluations $a = \pr(z, 1), b = \pr(y, z), t = \pt(z, y)$.
		Then it engages in the signature of correct computation playing the role of
		the helper, i.e.~it commits to $\p{s}(X, y)$ and sends the commitment $S$.
		Then it obtains a challenge $u$ from the verifier.
	\item[Round 4] In the next round the prover computes $C \gets \com(\crs,
		\dconst, \p{s}(u, x)) \cdot \gone{\p{s}(u, x)}$ and compute commitments'
		openings 
		\begin{align*}
			W & = \open(\crs, u, \p{s}(X, y)), \\
			Q & = \open(\crs, y, \p{s}(u, Y)),
		\end{align*}
		and returns $W, Q, \p{s}(u, y)$. Eventually the prover gets the last
		challenge from the verifier---$z$.
	\item[Round 5] In the final round, $\prover$ computes opening $Q_z =
		\open(\crs, z, \p{s}(u, X))$ and outputs $Q_z$ and $\p{s}(u, z)$. 
\end{description}

	\ncase{$\sonic$ verifier $\verifier(\REL, \crs, \inp, \zkproof)$}
	The verifier in \sonic{} runs as subroutines the verifier for the polynomial
	commitment. That is it sets $t = a(b + s) - \p{k}(y)$ and checks the following:
	\begin{equation*}
		\begin{split}
		&\PCOMs.\verifier(\crs, \multconstr, R, z, a, W_a), \\
		&\PCOMs.\verifier(\crs, \multconstr, R, yz, b, W_b),\\
		&\PCOMs.\verifier(\crs, \dconst, T, z, t, W_t),\\	
		\end{split}
		\qquad
		\begin{split}
		&\PCOMs.\verifier(\crs, \dconst, S, u, s, W),\\
		&\PCOMs.\verifier(\crs, \dconst, C, y, s, Q),\\
		&\PCOMs.\verifier(\crs, \dconst, C, z, s_z, Q_z),
		\end{split}
	\end{equation*}
	and accepts the proof iff all the checks holds.
	
	\section{Omitted lemmas and proofs}
	\begin{lemma}
			\label{lem:root_prob}
			Let $\p{f}(X)$ be a random degree-$d$ polynomial over $\FF_p[X]$. Then
			the probability that $\p{f}(X)$ has roots in $\FF_p$ is at least
			$\infrac{1}{d!}$.
	\end{lemma}
	\begin{proof}
			First observe that there is $p^{d}$ canonical polynomials in $\FF_p[X]$.
			Each of the polynomials may have up to $d$ roots. Consider polynomials
			which are reducible to polynomials of degree $1$, i.e.~polynomials that
			have all $d$ roots. The roots can be picked in $\bar{C}^{p}_{d}$
			ways, where $\bar{C}^{n}_{k}$ is the number of
			$k$-elements combinations with 
			repetitions from $n$-element set. That is,
			\[
					\bar{C}^n_k = \binom{n + k - 1}{k}\,.
			\]
			Thus, the probability that a randomly picked polynomial has all $d$
			roots is 
			\begin{multline*}
			p^{-d} \cdot \bar{C}^p_d = p^{-d} \cdot \binom{p + d - 1}{d} =
					 p^{-d} \cdot \frac{(p + d - 1)!}{(p + d - 1 - d)! \cdot d!} = \\
					 p^{-d} \cdot \frac{(p + d - 1) \cdot \ldots \cdot p \cdot (p -
						 1)!}{(p - 1)! \cdot d!} =  p^{-d} \cdot \frac{(p + d - 1)\cdot
							 \ldots \cdot p}{d!} \\
					 \geq  p^{-d} \cdot {\frac{p^d}{d!}} = \frac{1}{d!}
			\end{multline*}
			\qed
	\end{proof}

	\section{(Tight) simulation soundness of $\plonk$}
	\task{28.07}{All proofs in this section should be verified}
	% \michals{15.09}{As MK noted, this proof holds only for simulation soundness. For extractability we use special soundness (cf the next section)}

	\begin{theorem}[Simulation soundness]
		% \michals{16.09}{To be change to sim snd proof}
		Assume that $(\numberofconstrains + 2, 1)$-$\dlog$ is $\eps_\dlog(\secpar)$-hard, $\plonkprot$ is sound and $\ur{3}$ with security $\epss(\secpar)$ and $\epsur(\secpar)$ respectively. 
		Then the probability that an algebraic, $\ppt$ adversary $\advss$ breaks simulation soundness of $\plonkprotfs$ is upper-bounded by 
		\[
			\epsur(\secpar) + q_\ro^6 (\eps_{\dlog}(\secpar) + \epss(\secpar))\,,
		\]
		where $q_\ro$ is the total number of queries required by the adversary $\advss$.
	\end{theorem}
	\begin{proof}
		We proceed by contradiction. Suppose there exists a $\ppt$ adversary $\advss$ that breaks simulation soundness with non-negligible probability
		\[
		\eps := \Pr
			\left[
			\begin{aligned}
				& \plonkprotfs.\verifier(\REL, \crs, \inp, \zkproof_{\advss}),\\
				& (\inp_{\advss}, \zkproof_{\advss}) \not\in Q,\\
				& \inp_\advss \not\in \LANG_\REL 
			\end{aligned}
			\,\left|\, 
			\vphantom{\begin{aligned}
				& \plonkprotfs.\verifier(\REL, \crs, \inp, \zkproof_{\advss}),\\
				& (\inp_{\advss}, \zkproof_{\advss}) \not\in Q,\\
				& \inp_\advss \not\in \LANG_\REL 
			\end{aligned}}
			\begin{aligned}
				& \crs \gets \plonkprotfs.\kcrs(\REL, \secparam)\\
				& (\inp_{\advss}, \zkproof_{\advss}) \gets \advss^{\simulator, \ro} (\REL, \crs),		
			\end{aligned}
			\right.\right].
		\]

	In such case, we are able to build reductions $\rdvs$, $\rdvur$, $\rdvdlog$ which using $\advss$ as a black-box, violate either the soundness, unique response properties of the underlying interactive protocol $\plonkprot$, or the $(\numberofconstrains + 2, 1)$-$\dlog$ assumption.

	In the following we denote by $\zkproof_{\advss}, \zkproof_{\simulator}$ proofs
	returned by the adversary and the simulator respectively. We use $\zkproof[i]$
	to denote prover's message in the $i$-th round of the proof, $\zkproof[i].\ch$
	to denote the challenge that is given to the prover after $\zkproof[i]$, and
	$\zkproof\range{i}{j}$ to denote all messages of the proof including challenges between rounds $i$ and $j$.

	% Without loss of generality, for every acceptable proof $\zkproof_{\advss}$, we
	% assume that whenever $\advss$ outputs in Round $i$, $i \geq 3$, a message $m$, then
	% $\zkproof_{\advss}\range{1}{i}$ is a fresh query to the random oracle.
	% \markulf{30.08}{What does fresh mean here? What about messages coming from the simulator?} 
	% \michals{30.08}{I thought that with this assumption we may just consider only queries from $\advss$}
	Without loss of generality, we assume that whenever the accepting proof contains a response to a challenge from a random oracle, we assume that the adversary queried the oracle to get it. 
	It is straightforward to transform any adversary that violates this condition into an adversary that makes these additional queries to the random oracle and wins with the same probability.

	A crucial observation is that the adversary $\advss$ may have learned $\zkproof_{\advss}\range{1}{3}$ by querying the simulator on $\inp_{\advss}$ or might have computed it itself. We denote the first event by $\event{E}$ and the second by $\nevent{E}$. 
	%
	Additionally, we divide event $\nevent{E}$ into two disjunctive subevents: $\nevent{E}_0$ and $\nevent{E}_1$. 
	Event $\nevent{E}_0$ considers a case when the final proof provided by the adversary $\advss$ is accepted by the idealised verification equation, i.e.~for that proof $\vereq(X) = 0$. 
	Alternatively, event $\nevent{E}_1$ covers a case when for $\zkproof_\advss$ it
	holds that $\vereq(\chi) = 0$, but $\vereq(X) \neq 0$, where $\chi$ is $\plonkprotfs$'s trapdoor.
	%
	As all these events are mutually exclusive and exhaustive, we have
	\[
		\eps = \prob{\advss \text{ wins}} = \prob{\advss \text{ wins}, \event{E}} + \prob{\advss \text{ wins}, \nevent{E}_0} + \prob{\advss \text{ wins}, \nevent{E}_1}\,.
	\]


	Before analysing the events, we make the following observation.
	First of all, we allow reductions $\rdv_\dlog, \rdvur, \rdvs$ to simulate the random oracle and simulator for the adversary $\advss$. We argue that since the reductions in their simulation behaves as real random oracle or simulator would, the chances the adversary breaks simulation soundness does not change. 

	Furthermore, we note that since $\advss$ is algebraic, it outputs a proof $\zkproof_\advss$ that can be written as 
	\[
		\zkproof_\advss = \vec{M} \cdot (\underbrace{1 \| \chi \| \ldots \| \chi^{\numberofconstrains + 2}}_{\crs} \| \vec{\tilde{\zkproof}_\simulator}_1^{\top} \| \ldots \| \vec{\tilde{\zkproof}_\simulator}_{q_\simulator}^\top)^\top\,,
	\]
	where $\gone{1, \chi, \ldots, \chi^{\numberofconstrains + 2}}$ are all $\GRP_1$-elements from the CRS of $\plonkprotfs$, $\vec{M}$ is a matrix of coefficients output by $\advss$ aside the proof, $\vec{\tilde{\zkproof}_\simulator}_i$ denote all $\GRP_1$-elements from the simulated proof ${\zkproof_\simulator}_i$, and the adversary makes $q_\simulator$ queries to the simulator. 
	Since the reduction itself provides the simulated proofs, it knows a matrix $\vec{M'}$ such that
	\begin{equation}
		\label{eq:M_prim}
		\zkproof_\advss = \vec{M'} \cdot (1 \| \chi \| \ldots \| \chi^{\numberofconstrains + 2})^\top\,.
	\end{equation}
	We use this property when analysing the success probability of reductions $\rdvs$ and $\rdvdlog$.

	% Before analyzing the events $\nevent{E}_0$ and $\nevent{E}_1$, 
	Also note that a proof $\zkproof$ could be accepted only if the verification equation $\vereq(\chi)$ holds. That is, the verifier plugs-in elements of $\zkproof$ into $\vereq(\chi)$ and checks whether it equals $0$. That is what is called a \emph{real check} in \cite{EPRINT:GabWilCio19}. 
	On the other hand there is an \emph{idealised check}, which verifies whether $\vereq(X) = 0$ \emph{as a polynomial}---with proof elements being polynomials as well.

	\ncase{When $\event{E}$ happens}
	We assume that $\inp_{\advss}$ is submitted to the simulator $\simulator$. 
	We show how $\rdvur$ utilizes $\advss$, that makes use of $\inp_\advss, \zkproof_{\advss}\range{1}{3}$, to break the $\ur{3}$ property of $\plonkprot$. 
	This way we bound the probability $\prob{\adv \text{ wins}, \event{E}}$ by the probability of $\rdvur$ being able to win in the $\ur{3}$ game.

	Consider an algorithm $\rdvur$ that runs $\advss$ internally as a black-box:
	\begin{itemize}
		\item The reduction answers both queries to the simulator $\plonkprotfs.\simulator$ and to the random oracle. 
		It also keeps lists $Q$, for the simulated proofs, and $Q_\ro$ for the random oracle queries. 
		\item When $\advss$ outputs a fake proof $\zkproof_{\advss}$ for  $\inp_\advss$, $\rdvur$ looks through lists $Q$ and $Q_\ro$ until it finds 
		$\zkproof_{\simulator}\range{1}{3}$ such that $\zkproof_{\advss}\range{1}{2} = \zkproof_{\simulator}\range{1}{3}$ and a random oracle query $\zkproof_{\simulator}[3].\ch$ on $\zkproof_{\simulator}\range{1}{3}$.
		\item $\rdvur$ returns two proofs for $\inp_\advss$:
		\begin{align*}
			\zkproof_1 = (\zkproof_{\simulator}\range{1}{3}, \zkproof_{\simulator}[3].\ch, \zkproof_{\simulator}\range{4}{5})\\
			\zkproof_2 = (\zkproof_{\simulator}\range{1}{3}, \zkproof_{\simulator}[3].\ch, \zkproof_{\advss}\range{4}{5})
		\end{align*}
		\end{itemize}  
		If $\zkproof_1 = \zkproof_2$, then $\advss$ fails to break simulation extractability, as $\zkproof_2 \in Q$.
		On the other hand, if the proofs are not equal, then $\rdvur$ breaks $\ur{3}$-ness of $\plonkprot$. Thus 
		\[
			\prob{\advss \text{ wins}, \event{E}} \leq \epsur(\secpar).
		\]

	\ncase{When $\nevent{E}_0$ happens}
	In this case the reduction $\rdvs$ uses $\advss$ to break soundness of $\plonkprot$ with probability $\epss / q_{\ro}^6$, where $q_\ro$ is the number of total random oracle queries performed by the adversary or by $\rdvs$ on behalf of the simulator.
	As previously, $\rdvs$ runs $\advss$ internally and simulates its environment by answering to its queries to $\plonkprotfs.\simulator$ and $\ro$. The reduction works as follows:
	\begin{itemize}
	\item It guesses indices $i_1, \ldots, i_6$ such that random oracle queries $h_{i_1}, \ldots, h_{i_6}$ are the queries used in $\zkproof_\advss$. This is done with probability at least $1/q_{\ro}^6$ (since there are $6$ challenges from the verifier in $\plonkprot$).
		\item On input $h$ for the $i$-th, $i \not\in \smallset{{i_1}, \ldots, {i_6}}$, random
	    oracle query, $\rdvs$ returns randomly picked $
	    y$, sets $\ro(h) = y $ and stores $(h, y)$ in $Q_\ro$ if $h$ is sent to $\ro$ the first time. If that is not the case, $\rdv$ finds $h$ in $Q_\ro$ and returns the corresponding $y$.
		\item On input $h_{i_j}$ for the $i_j$-th, $i_j \in \smallset{{i_1}, \ldots, {i_6}}$,
	    random oracle query, $\rdvs$ parses $h_{i_j}$ as a partial proof transcript
	    $\zkproof[1..j]$ and runs $\plonkprot$ using $\zkproof[j]$ as a $\plonkprot.\prover$'s $j$-th message to $\plonkprot.\verifier$. The verifier responds with a challenge $\zkproof[j].\ch$. The reduction sets $\ro(h_{i_j}) = \zkproof[j].\ch$.
		\item On query $\inp_\simulator$ to $\simulator$ it runs a simulator
	    $\plonkprotfs.\simulator$ internally and returns $\zkproof_\simulator$. If
	    the random oracle query with input $\zkproof_\simulator[j]$, $1 \leq j \leq 2$, of the simulator is the $i_j$-th query,
	    generate $\zkproof_\simulator[j].\ch$ by invoking $\plonkprot.\verifier$ on
	    $\zkproof_\simulator[j]$ and programming $\ro(h_{i_j}) = \zkproof_\simulator[j].\ch$.
		\item Answers $\plonkprot.\verifier$'s challenge $\zkproof[j].\ch$ using the answer given by $\advss$, i.e.~$\zkproof_\advss[j + 1]$.
	\end{itemize}

	Assume that the $\plonkprot.\verifier$ accepts $\zkproof_\advss$. We consider a case when the idealised verification equation accepts. (Thus, the real verification accepts as well.) 
	In that case $\rdvs$ extracts from $\vec{M'}$ coefficients of $1, \chi, \ldots, \chi^{\numberofconstrains + 2}$ for polynomials $\p{a}(X), \p{b}(X)$, and $\p{c}(X)$ and reveals the witness $\wit_\advss$ (as it is encoded in theses polynomials' coefficients).
	If $\REL(\inp_\advss, \wit_\advss)$ holds then $\advss$ failed to break simulation-soundness of $\plonkprotfs$. On the other hand, if that is not the case, then $\rdvs$ breaks soundness of $\plonkprot$.
	%
	Since the reduction guesses queries $h_{i_1}, \ldots, h_{i_6}$ with probability $1/q_\ro^6$, then 
	\[
		\prob{\rdvs \text{ wins}} = \prob{\advss \text{ wins}, h_{i_1}, \ldots, h_{i_6} \text{ are guessed correctly}, \nevent{E}_0}\,.
	\] 
	Hence,
	\[
		\prob{\advss \text{ wins}, \nevent{E}_0} \leq q_\ro^6 \cdot \epsss(\secpar).
	\]

	\ncase{When $\nevent{E}_1$ happens}
	The reduction $\rdvdlog$ runs internally a protocol $\plonkprotfs$, which CRS is computed from the challenge $\gone{1, \chi, \ldots, \chi^{\numberofconstrains + 2}}, \gtwo{\chi}$ from the $(\numberofconstrains + 2, 1)$-$\dlog$ assumption challenger. 
	Then it proceeds as $\rdvs$ does, except in the last part, when the adversary provided its proof $\zkproof_\advss$, $\rdvdlog$ uses the fact that the real verification equation holds, but the ideal verification equation does not to break the $\dlog$ assumption. 

	Since $\vereq(X) \neq 0$, but $\vereq(\chi) = 0$ and $\rdvdlog$ knows
	$\vec{M'}$, as defined in \cref{eq:M_prim}, it can recreate all the polynomials
	submitted by $\advss$ as part of the proof and included in $\vereq(X)$. This
	way, it knows all coefficients of $\vereq(X)$. Thus it can factorise it and find
	its roots, one of them is the required $\chi$. Hence it holds, by the analogous analysis as in the previous case, that
	\[
		\prob{\advss \text{ wins}, \nevent{E}_1} \leq q_\ro^6 \cdot \eps_{\dlog}(\secpar).
	\]

	The proof is concluded by observing that the analysis of events $\event{E}, \nevent{E}_0, \nevent{E}_1$ gives
	\[
		\eps \leq \epsur(\secpar) + q_\ro^6 (\eps_{\dlog}(\secpar) + \epsss(\secpar))\,,
	\]
	hence $\eps$ is negligible if $\dlog$ is hard and $\plonkprot$ is sound and $\ur{3}$.
	\qed
	\end{proof}
\end{document}

