% !TeX spellcheck = en_UK
% \let\accentvec\vec              
\documentclass[runningheads,11pt]{llncs}
\let\spvec\vec
\let\vec\accentvec

\newcommand\hmmax{0}
\newcommand\bmmax{0}

\DeclareFontFamily{U}{mathx}{\hyphenchar\font45}
\DeclareFontShape{U}{mathx}{m}{n}{<-> mathx10}{}
\DeclareSymbolFont{mathx}{U}{mathx}{m}{n}
\DeclareMathAccent{\widebar}{0}{mathx}{"73}

\let\spvec\vec
\usepackage{amssymb,amsmath}
\let\vec\spvec
\usepackage{newtxmath,newtxtext}
\usepackage[T1]{fontenc}
\usepackage[most]{tcolorbox}
  \def\vec#1{\mathchoice{\mbox{\boldmath$\displaystyle#1$}}
  {\mbox{\boldmath$\textstyle#1$}} {\mbox{\boldmath$\scriptstyle#1$}}
  {\mbox{\boldmath$\scriptscriptstyle#1$}}}

% lncs size (as printed in books, with small margins):
 % \usepackage[paperheight=23.5cm,paperwidth=15.5cm,text={13.2cm,20.3cm},centering]{geometry}
 %\usepackage{fullpage}
\usepackage{soulutf8} \soulregister\cite7 \soulregister\ref7
\soulregister\pageref7
\usepackage{hyperref}
\usepackage[color=yellow]{todonotes} \hypersetup{final}
\usepackage{mathrsfs}
\usepackage[advantage,asymptotics,adversary,sets,keys,ff,lambda,primitives,events,operators,probability,logic,mm,complexity]{cryptocode}

\usepackage[capitalise]{cleveref}
%\crefname{appendix}{Supp.~Mat.}{Supp.~Mat.}
%\Crefname{appendix}{Supp.~Mat.}{Supp.~Mat.}
\usepackage{cite} 
\usepackage{booktabs}
\usepackage{paralist}
\usepackage[innerleftmargin=5pt,innerrightmargin=5pt]{mdframed}
\usepackage{caption}
\captionsetup{belowskip=0pt}
\usepackage{bm}
\usepackage{url}
%\usepackage{dirtytalk}
\usepackage[margin=1in,a4paper]{geometry}
\usepackage[normalem]{ulem}
\usepackage{dashbox}
\newcommand\dboxed[1]{\dbox{\ensuremath{#1}}}
\usepackage{setspace}
\include{macros}

%% Save the class definition of \subparagraph
\let\llncssubparagraph\subparagraph
%% Provide a definition to \subparagraph to keep titlesec happy
\let\subparagraph\paragraph
%% Load titlesec
\usepackage[compact]{titlesec}
%% Revert \subparagraph to the llncs definition
\let\subparagraph\llncssubparagraph

\newcommand{\oursubsub}[1] {\smallskip\noindent\textbf{#1}}
\newcommand{\ourpar}[1] {\smallskip\noindent\emph{#1}}

%\title{On Simulation-Extractability of Universal zkSNARKs}
\title{Non-Malleability of the Fiat--Shamir Transform Revisited for Multi-round SRS-Based Protocols}
%\author{Anonymous submission to Asiacrypt}
\author{Markulf Kohlweiss\inst{1,2} \and Michał Zając\inst{3}} 
\institute{University of Edinburgh, Edinburgh, UK \and IOHK \\
\email{mkohlwei@inf.ed.ac.uk} \and Clearmatics, London, UK \\
\email{m.p.zajac@gmail.com}}

\allowdisplaybreaks

\begin{document} \sloppy
\titlerunning{Non-Malleability of the FS transform Revisited [\ldots]}
\maketitle

\begin{abstract}
  Faust, Kohlweiss, Marson, and Venturi (INDOCRYPT 2012) showed that
  non-interactive zero knowledge (NIZK) proof systems obtained by applying the
  Fiat--Shamir transformation to a public-coin sigma protocol are simulation
  sound and simulation extractable under lenient conditions. In this paper, we
  extend this work and formally define simulation extractability for protocols
  in the random oracle model (ROM) which also use a structured reference string
  (SRS). Furthermore, we show that NIZK proof systems obtained by applying the
  Fiat--Shamir transformation to a public-coin multi-round interactive protocol
  with SRS are simulation-extractable under lenient conditions. A consequence of
  our result is that, in the ROM, we obtain non-malleable NIZKs essentially for
  free from a much wider class of protocols than Faust et al. Importantly, we
  show that two popular zero knowledge SNARKs ---
  \plonk{}~\cite{EPRINT:GabWilCio19} and \sonic{}~\cite{CCS:MBKM19} --- are
  simulation extractable out-of-the-box.
\end{abstract}

\input{intro}



\section{Preliminaries}
\label{sec:preliminaries}
Let $\ppt$ denote probabilistic polynomial-time and $\secpar \in \NN$ be the
security parameter. All adversaries are stateful. For an algorithm $\adv$, let
$\image (\adv)$ be the image of $\adv$ (the set of valid outputs of $\adv$), let
$\RND{\adv}$ denote the set of random tapes of correct length for $\adv$
(assuming the given value of $\secpar$), and let $r \sample \RND{\adv}$ denote
the random choice of the randomiser $r$ from $\RND{\adv}$. We denote by $\negl$
($\poly$) an arbitrary negligible (resp.~polynomial) function.

Probability ensembles $X = \smallset{X_\secpar}_\secpar$ and $Y =
\smallset{Y_\secpar}_\secpar$, for distributions $X_\secpar, Y_\secpar$, have
\emph{statistical distance} $\SD$ equal $\epsilon(\secpar)$ if $\sum_{a \in
  \supp{X_\secpar \cup Y_\secpar}} \abs{\prob{X_\secpar = a} - \prob{Y_\secpar =
    a}} = \epsilon(\secpar)$. We write $X \approx_\secpar Y$ if $\SD(X_\secpar,
Y_\secpar) \leq \negl$. For values $a(\secpar)$ and $b(\secpar)$ we write
$a(\secpar) \approx_\secpar b(\secpar)$ if $\abs{a(\secpar) - b(\secpar)} \leq
\negl$.

\newcommand{\samplespace}{\Omega}
\newcommand{\eventspace}{\mathcal{F}}
\newcommand{\probfunction}{\mu}

For a probability space $(\samplespace, \eventspace, \probfunction)$ and event
$\event{E} \in \eventspace$ we denote by $\nevent{E}$ an event that is
complementary to $\event{E}$,
i.e.~$\nevent{E} = \samplespace \setminus \event{E}$.

Denote by $\RELGEN = \smallset{\REL}$ a family of relations. We assume that if
$\REL$ comes with any auxiliary input, it is benign. Directly from the
description of $\REL$ one learns security parameter $\secpar$ and other
necessary information like public parameters $\pp$ containing description of a
group $\GRP$, if the relation is a relation of group elements (as it usually is
in case of zkSNARKs).

\ourpar{Bilinear groups.}
A bilinear group generator $\pgen (\secparam)$ returns public parameters $ \pp =
(p, \GRP_1, \GRP_2, \GRP_T, \pair, \gone{1}, \gtwo{1})$, where $\GRP_1$,
$\GRP_2$, and $\GRP_T$ are additive cyclic groups of prime order $p = 2^{\Omega
  (\secpar)}$, $\gone{1}, \gtwo{1}$ are generators of $\GRP_1$, $\GRP_2$, resp.,
and $\pair: \GRP_1 \times \GRP_2 \to \GRP_T$ is a non-degenerate
$\ppt$-computable bilinear pairing. We assume the bilinear pairing to be Type-3,
i.e., that there is no efficient isomorphism from $\GRP_1$ to $\GRP_2$ or from
$\GRP_2$ to $\GRP_1$. We use the by now standard bracket notation, i.e., we
write $\bmap{a}{\gi}$ to denote $a g_{\gi}$ where $g_{\gi}$ is a fixed generator
of $\GRP_{\gi}$. We denote $\pair (\gone{a}, \gtwo{b})$ as $\gone{a} \bullet
\gtwo{b}$. Thus, $\gone{a} \bullet \gtwo{b} = \gtar{a b}$. We freely use the
bracket notation with matrices, e.g., if $\vec{A} \vec{B} = \vec{C}$ then
$\vec{A} \grpgi{\vec{B}} = \grpgi{\vec{C}}$ and $\gone{\vec{A}}\bullet
\gtwo{\vec{B}} = \gtar{\vec{C}}$. Since every algorithm $\adv$ takes as input
the public parameters we skip them when describing $\adv$'s input. Similarly, we
do not explicitly state that each protocol starts with generating these
parameters by $\pgen$.

\subsection{Computational assumptions.}

\ourpar{Discrete-log assumptions.}  Security of $\plonk$ and $\sonic$ relies on
two discrete-log based security assumptions---$(q_1, q_2)$-$\dlog$ assumption
and its variant that allows for negative exponents $(q_1, q_2)$-$\ldlog$
assumption\footnote{Note that \cite{CCS:MBKM19} dubs their assumption \emph{a
    dlog assumption}. We changed that name to distinguish it from the more
  standard dlog assumption used in \cite{EPRINT:GabWilCio19}. ``l'' in
  \emph{ldlog} relates to use of Laurent polynomials in the assumption.}. We
omit here description of the assumptions and refer to
\cref{sec:dlog_assumptions}.



\ourpar{Proofs by Game-Hopping.}
Proofs by \emph{game hopping} is a method of writing proofs popularised by
e.g.~Shoup \cite{EPRINT:Shoup04} and Dent \cite{EPRINT:Dent06c}. The method
relies on the following lemma.

\begin{lemma}[Difference lemma,~{\cite[Lemma 1]{EPRINT:Shoup04}}]
	\label{lem:difference_lemma}
	Let $\event{A}, \event{B}, \event{F}$ be events defined in some probability
	space, and suppose that $\event{A} \land \nevent{F} \iff \event{B}
		\land \nevent{F}$.  Then 
	$
		\abs{\prob{\event{A}} - \prob{\event{B}}} \leq \prob{\event{F}}\,.
	$
\end{lemma}
\subsection{Algebraic Group Model}
The algebraic group model (AGM) introduced in \cite{C:FucKilLos18} lies between
the standard model and generic bilinear group model\hamid{not clear why generic bilinear group model and not just generic group model!}. In the AGM it is assumed
that an adversary $\adv$ can output a group element $\gnone{y} \in \GRP$ if
$\gnone{y}$ has been computed by applying group operations to group elements
given to $\adv$ as input. It is further assumed, that $\adv$ knows how to
``build'' $\gnone{y}$ from that elements. More precisely, the AGM requires that
whenever $\adv(\gnone{\vec{x}})$ outputs a group element $\gnone{y}$ then it
also outputs $\vec{c}$ such that $\gnone{y} = \vec{c}^\top \cdot
\gnone{\vec{x}}$. Both $\plonk$ and $\sonic$ have been shown secure using the
AGM. An adversary that works in the AGM is called \emph{algebraic}.

\subsection{Polynomial commitment}
\label{sec:poly_com}
In the polynomial commitment scheme $\PCOM = (\kgen, \com, \open, \verify)$ the
committer $\committer$ can convince the receiver $\receiver$ that some
polynomial $\p{f}$ which $\committer$ committed to evaluates to $s$ at some
point $z$ chosen by $\receiver$. $\PCOM$'s subroutines are defined as follows 
\begin{description}
\item[$\kgen(1^\secpar, \maxdeg)$:] The key generation algorithm
  $\kgen(1^\secpar, \maxdeg)$ takes in a security parameter $1^\secpar$ and a
  parameter $\maxdeg$ which determines the maximal degree of the committed
  polynomial. It outputs a structured reference string $\srs$ (including a
  commitment key).
\item[$\com(\srs, \p{f})$:] The commitment algorithm $\com(\srs, \p{f})$ takes
  in $\srs$ and a polynomial $\p{f}$ with maximum degree $\maxdeg$, and outputs
  a commitment $c$.
\item[$\open(\srs, z, s, \p{f})$:] The opening algorithm
  $\open(\srs, z, s \p{f})$ takes as input $\srs$, an evaluation point $z$, a
  value $s$ and the polynomial $\p{f}$. It outputs an opening $o$.
\item[$\verify(\srs, c, z, s, o)$:] The verification algorithm takes in $\srs$,
  a commitment $c$, an evaluation point $z$, a value $s$ and an opening $o$. It
  outputs 1 if $o$ is a valid opening for $(c, z, s)$ and 0 otherwise.
\end{description}

$\plonk$ and $\sonic$ use variants of the KZG polynomial commitment scheme
\cite{AC:KatZavGol10}. We denote the first by $\PCOMp$ and the latter by
$\PCOMs$. Due to page limit, we omit their presentation here and refer to
\cref{fig:pcomp} and \cref{fig:pcoms} in the \cref{sec:pcom}.  In this paper we
use evaluation binding, commitment of knowledge, and, newly introduced, unique
opening and hiding properties. Formal definitions of these could be find in
\cref{sec:pcom}, here we briefly introduce them.
\begin{compactdesc}
  \item[Evaluation binding] intuitively, this property assures that no adversary
    could provide two valid openings for two different evaluations of the same
    commitment in the same point. 
  \item[Commitment of knowledge] when a commitment scheme is ``of
    knowledge'' then if an adversary produces a (valid) commitment $c$, which it
    can open, then it also knows the underlying polynomial $\p{f}$ which commits
    to that value.  \cite{CCS:MBKM19} shows, using AGM, that $\PCOMs$ is a
    commitment of knowledge.  The same reasoning could be used to show that
    property for $\PCOMp$.
  \item[Unique opening] this property assures that there is
    only one valid opening for the committed polynomial and given evaluation
    point. This property is crucial in showing forking simulation-extractability
    of $\plonk$ and $\sonic$. We show that the $\plonk$'s and $\sonic$'s
    polynomial commitment schemes satisfy this requirement in
    \cref{lem:pcomp_op} and \cref{lem:pcoms_unique_op} respectively.
  \item[Hiding] assures that no adversary is able to tell anything about the
    polynomial given only its commitment and bounded number of evaluations.
\end{compactdesc}


\subsection{Zero knowledge}
In a zero-knowledge proof system, a prover convinces the verifier of veracity of
a statement without leaking any other information. The zero-knowledge property
is proven by constructing a simulator that can simulate the view of a cheating
verifier without knowing the secret information---witness---of the prover. A
proof system has to be sound as well, i.e.~for a malicious prover it should be
infeasible to convince a verifier on a false statement. Here, we focus on proof
systems that guarantee soundness against $\ppt$ malicious provers.

More precisely, let $\RELGEN(\secparam) = \smallset{\REL}$ be a family of
$\npol$ relations. Denote by $\LANG_\REL$ the language determined by $\REL$. Let
$\prover$ and $\verifier$ be $\ppt$ algorithms, the former called \emph{prover}
and the latter \emph{verifier}. We allow our proof system to have a setup,
i.e.~there is a $\kgen$ algorithm that takes as input the relation description
$\REL$ and outputs a common reference string $\srs$. We assume that the $\srs$
defines the relation and for universal prove systems, such as Plonk and Sonic,
we treat both the reference string and the relation as universal.

We denote by
$\ip{\prover(\srs, \inp, \wit)}{\verifier(\srs,\inp)}$ a
\emph{transcript} (also called \emph{proof}) $\zkproof$ of a conversation
between $\prover$ with input $(\srs, \inp, \wit)$ and $\verifier$ with
input $(\srs, \inp)$. We write
$\ip{\prover (\srs, \inp, \wit)}{\verifier(\srs, \inp)} = 1$ if in
the end of the transcript the verifier $\verifier$ returns $1$ and say that
$\verifier$ accepts it. We sometimes abuse notation and write
$\verifier(\srs, \inp, \zkproof) = 1$ to denote a fact that $\zkproof$ is
accepted by the verifier. (This is especially handy when the proof system is
non-interactive, i.e.~the whole conversation between the prover and verifier
consists of a single message $\zkproof$ sent by $\prover$).

A proof system $\proofsystem = (\kgen, \prover, \verifier, \simulator)$ for
$\RELGEN$ is required to have three properties: completeness, soundness and zero
knowledge, which are defined as follows:
% \begin{description}

\ourpar{Completeness.}
%\item[Completeness]
  An interactive proof system $\proofsystem$ is
  \emph{complete} if an honest prover always convinces an honest verifier, that
  is for all $\REL \in \RELGEN(\secparam)$ and $(\inp, \wit) \in \REL$
	\[
		\condprob{\ip{\prover (\srs, \inp, \wit)}{\verifier (\srs,
        \inp)} = 1}{\srs \gets \kgen(\REL)} = 1\,.
	\]
    % \item[Soundness]
\ourpar{Soundness.}
    We say that $\proofsystem$ for $\RELGEN$ is \emph{sound} if no
  $\ppt$ prover $\adv$ can convince an honest verifier $\verifier$ to accept a
  proof for a false statement $\inp \not\in\LANG$. More precisely, for
  all $\REL \in \RELGEN(\secparam)$
	\[
    \condprob{\ip{\adv(\srs, \inp)}{\verifier(\srs, \inp)} = 1 \land \inp
      \not\in \LANG_\REL}{\srs \gets \kgen(\REL), \inp \gets \adv(\srs)} \leq
    \negl\,;
	\]
%\end{description}
Sometimes a stronger notion of soundness is required---except requiring that the
verifier rejects proofs of statements outside the language, we request from the
prover to know a witness corresponding to the proven statement. This property is
called \emph{knowledge soundness}.%\markulf{Commented out the formal definition as we don't use it.}

% We call an interactive proof system $\proofsystem$
% \emph{knowledge-sound} if for any $\REL \in \RELGEN(\secparam)$ and a $\ppt$
% adversary $\adv$
% 	\[
% 	\Pr\left[
% 		\begin{aligned}
% 			& \verifier(\srs, \inp, \zkproof) = 1, \\
% 			& \REL(\inp, \wit) = 0
% 	 \end{aligned}
% 	  \,\left|\,
% 	 \begin{aligned}
% 		 & \srs \gets \kgen(\REL), \inp \gets \adv(\srs), \\
% 		 & (\wit, \zkproof) \gets \ext^{\ip{\adv(\srs, \inp)}{\verifier(\srs, \inp)}}(\REL, \inp)
% 	 \end{aligned}
% 	 \vphantom{\begin{aligned}
% 		 \adv (\zkproof) = 1, \\
% 		 \text{if $\zkproof{}$ is accepting} \\
% 		 \pcind \text{then $\REL(\inp, \wit)$}
% 	 \end{aligned}}\right.
% 	 \right] \leq \negl\,,
%  \]
 
 % \end{description}

 % Usually the verifier verifies messages send by the prover by checking a number
 % of equations depend on the instance, SRS and the proof sent. These equations
 % are often called \emph{verification equations} and denoted $\vereq_i$, for $i$
 % being an index of the equation. It is usually required that an acceptable proof
 % yields $\vereq_i = 0$. In the proof systems we consider---$\plonk$ and
 % $\sonic$---verification equations can be seen as polynomials evaluated at the
 % trapdoor $\chi$. Thus, the verifier checks that $\vereq_i(\chi) =
 % 0$. Sometimes, we consider an \emph{idealised verifier},
 % cf.~\cite{EPRINT:GabWilCio19}, who instead of checking that polynomial
 % $\vereq_i(X)$ evaluates to $0$ at $\chi$ just checks that $\vereq_i(X)$ is a
 % zero polynomial.

\ourpar{Zero knowledge.}  We call a proof system $\proofsystem$
\emph{zero-knowledge} if for any $\REL \in \RELGEN(\secparam)$, and adversary
$\adv$ there exists a $\ppt$ simulator $\simulator$ such that for any
$(\inp, \wit) \in \REL$
\begin{multline*}
	  \left\{\ip{\prover(\srs, \inp, \wit)}{\adv(\srs, \inp, \wit)}
      \,\left|\, \srs \gets \kgen(\REL)\COMMENT{, (\inp, \wit) \gets \adv(\REL,
          \srs)}\vphantom{\simulator^\adv}\right.\right\} \approx_\secpar
		%\\
		\left\{\simulator^{\adv}(\srs, \inp)\,\left|\, \srs \gets
        \kgen(\REL)\COMMENT{, (\inp, \wit) \gets \adv(\REL,
          \srs)}\vphantom{\simulator^\adv}\right.\right\}\,.
\end{multline*}
	%
We call zero knowledge \emph{perfect} if the distributions are equal and
\emph{computational} if they are indistinguishable for any $\ppt$ distinguisher.

% \end{description}
Alternatively, zero-knowledge can be defined by allowing the simulator to use
the trapdoor $\td$ that is generated along the $\srs$. In this paper we distinguish
simulators that requires a trapdoor to simulate and those that do not. We call
the former \emph{SRS-simulators}. We say that a protocol is zero knowledge in
the standard model if its simulator does not require the trapdoor.

Occasionally, a weaker version of zero knowledge is sufficient. So called
\emph{honest verifier zero knowledge} (HVZK) assumes that the verifier's
challenges are picked at random from some predefined set. Although weaker, this
definition suffices in many applications. Especially, an interactive
zero-knowledge proof that is HVZK and \emph{public-coin} (i.e.~the verifier
outputs as challenges its random coins) can be made non-interactive and
zero-knowledge in the random oracle model by using the Fiat--Shamir
transformation.

% In our simulation soundness proof (but not simulation extractability \hamid{you mean forking simulation extractability?}) we need
% an additional property of the zero-knowledge  proof system which we call $k$-programmable ZK.
\begin{definition}[$k$-programmable ZK]
  \label{def:kzk}
  Let $\ps$ be a $(2\mu + 1)$-message ZK proof system and let $\ps_\fs$ be its
  Fiat--Shamir variant. We say that $\ps_\fs$ is $k$-programmable ZK if there
  exists a simulator $\simulator_\fs$ that
  \begin{compactenum}
  \item produces proofs indistinguishable from proofs output by an honest
    prover;
  \item $\simulator_\fs$ programs the random oracle \emph{only} for
    challenges from round $k$ to $\mu + 1$.
  \end{compactenum}
\end{definition}
We note that $\plonkprot$ is $2$-programmable ZK and $\sonicprot$ is
$1$-programmable ZK. This follows directly from the proofs of their standard model zero-knowledge property in
\cref{lem:plonk_hvzk,lem:sonic_hvzk}.

\oursubsub{Idealised verifier and verification equations} Let
$(\kgen, \prover, \verifier)$ be a protocol---in this paper, either a proof
system or a polynomial commitment scheme\hamid{might be unclear as we are defining polynomial commitments as $(\kgen, \com, \open, \verify)$.}. Observe that the $\kgen$ algorithm
provides an SRS which can be interpreted as a set of group representation of
polynomials evaluated at trapdoor elements. E.g.~for a trapdoor $\chi$ the SRS
contains $\gone{\p{p_1}(\chi), \ldots, \p{p_k}(\chi)}$, for some polynomials
$\p{p_1}(X), \ldots, \p{p_k}(X) \in \FF_p[X]$. On the other hand, the verifier
$\verifier$ accepts if a (possibly set of) verification equation
$\vereq_{\inp, \zkproof}$ (note that the verification equation changes relate to
the instance $\inp$ and proof $\zkproof$), which can also be interpreted as a
polynomial in $\FF_p[X]$ whose coefficients depend on messages sent by the
prover, zeroes at $\chi$. Following \cite{EPRINT:GabWilCio19} we call verifiers
who checks that $\vereq_{\inp, \zkproof}(\chi) = 0$ \emph{real verifiers} as opposed to
\emph{ideal verifiers} who accepts only when $\vereq_{\inp, \zkproof}(X) = 0$. That is, while a
real verifier accepts when a polynomial \emph{evaluates} to zero, an ideal
verifier accepts only when the polynomial \emph{is} zero.

Although ideal verifiers are impractical, they are very useful in our
proofs. More precisely, we show that
\begin{compactenum}
\item the idealised verifier accepts an incorrect proof (what ``incorrect''
  means depends on the situation) with at most negligible probability (and many
  cases---never);
\item when the real verifier accepts, but not the idealised one, then we show
  how to use a malicious $\prover$ to break the underlying security assumption
  (in our case---a variant of $\dlog$.)
\end{compactenum}

\oursubsub{Sigma protocols}
A sigma protocol $\sigmaprot = (\prover, \verifier, \simulator)$ for a relation
$\REL \in \RELGEN(\secparam)$ is a special case of an interactive proof where a
transcript consists of three messages $(a, b, z)$, where $b$ is a challenge
provided by the verifier. Sigma protocols are honest verifier zero-knowledge in
the standard model and specially-sound. That is, there exists an extractor
$\ext$ which given two accepting transcripts $(a, b, z)$, $(a, b', z')$ for a
statement $\inp$ can recreate the corresponding witness if $b \neq b'$.
More formally:
% \begin{description}

\ourpar{Special soundness.}
% \hamid{The last (short) sentence looks a little unclear.}
A sigma protocol $\sigmaprot$ is \emph{specially-sound}
  if for any adversary $\adv$ the probability
\[
\Pr\left[
\begin{aligned}
& \verifier(\REL, \inp, (a, b, z)) = %\\
\verifier(\REL, \inp, (a, b', z')) = 1 \\
& \land b \neq b' \land \REL(\inp, \wit) = 0 \\
\end{aligned}
\,\left|\,
\begin{aligned}
& (\inp, (a, b, z), (a, b', z')) \gets \adv(\REL), \\ %\\
& \wit \gets \ext(\REL, \inp, (a, b, z), (a, b', z'))\\
\end{aligned}
\right.\right]
\]
is upper-bounded by some negligible function $\negl$.
%\end{description}

Another property that sigma protocols may have is a unique response
property \cite{C:Fischlin05} which states that no $\ppt$ adversary can
produce two accepting transcripts that differ only on the last
element. More precisely, 
%\begin{description} 
\ourpar{Unique response property.} Let
$\sigmaprot = (\prover, \verifier, \simulator)$ be a sigma-protocol for
$\REL \in \RELGEN(\secparam)$ with proofs of the form
$(a, b, z)$. We say that $\sigmaprot$ has the unique response property if for
all $\ppt$ algorithms $\adv$, it holds that,:
\[ \condprob{\verifier (\REL, \inp, (a, b, z)) = \verifier (\REL, \inp, (a, b,
	z')) = 1 \land z \neq z'}{(\inp, a, b, z, z') \gets \adv(\REL)} \leq \negl\,.  \]
%\end{description} 
If this property holds even against unbounded adversaries, it is called
\emph{strict}, cf.~\cite{INDOCRYPT:FKMV12}. Later on we call protocols that
follows this notion \emph{ur-protocols}. For the sake of completeness we note
that many sigma protocols, like e.g.~Schnorr's protocol \cite{C:Schnorr89},
fulfil this property.

\subsection{From interactive to non-interactive---the Fiat--Shamir transform}
Consider a $(2\mu + 1)$-message, public-coin, honest verifier zero-knowledge
interactive proof system
$\proofsystem = (\kgen, \prover, \verifier, \simulator)$ for
$\REL \in \RELGEN(\secparam)$.  Let $\zkproof$ be a proof performed by the
prover $\prover$ and verifier $\verifier$ compound of messages
$(a_1, b_1, \ldots, a_{\mu}, b_{\mu}, a_{\mu + 1})$, where $a_i$ comes from
$\prover$ and $b_i$ comes from $\verifier$.  Denote by $\ro$ a random oracle.
Let $\proofsystem_\fs = (\kgen_\fs, \prover_\fs, \verifier_\fs, \simulator_\fs)$
be a proof system such that
\begin{compactitem}
  \item $\kgen_\fs$ behaves as $\kgen$.
  \item $\prover_\fs$ behaves as $\prover$ except after sending message
    $a_i$, $i \in \range{1}{\mu}$, the prover does not wait for
    the message from the verifier but computes it locally setting $b_i
    = \ro(\zkproof[0..i])$, where $\zkproof[0..j] = (\inp, a_1, b_1, \ldots,
    a_{j - 1}, b_{j - 1}, a_j)$. (Importantly, $\zkproof[0..\mu + 1] =
    (\inp, \zkproof)$).
  \item $\verifier_\fs$ behaves as $\verifier$ but does not provide
    challenges to the prover's proof. Instead it computes the
    challenges locally as $\prover_\fs$ does. Then it verifies the
    resulting transcript $\zkproof$ as the verifier $\verifier$ would. 
  \item $\simulator_\fs$ behaves as $\simulator$, except when
    $\simulator$ picks challenge $b_i$ before computing message $\zkproof[0, i]$, $\simulator_\fs$ programs the
    random oracle to output $b_i$ on $\zkproof[0, i]$.
  \end{compactitem}

\noindent
The Fiat--Shamir heuristic states that $\proofsystem_\fs$ is a zero-knowledge
non-interactive proof system for $\REL \in \RELGEN(\secparam)$.

\subsection{Non-malleability definitions for NIZKs}
\label{sec:simext_def}
Real life applications often require a NIZK proof system to be
non-malleable. That is, no adversary seeing a proof $\zkproof$ for a statement
$\inp$ should be able to provide a new proof $\zkproof'$ related to $\zkproof$.
\emph{Simulation extractability} formalizes a strong version of non-malleability
by requiring that no adversary can produce a valid proof without knowing the
corresponding witness. This must hold even if the adversary is allowed to see
polynomially many simulated proofs for any statements it wishes.

\chaya{remove reference to forking soundness. quantify for $\ext_\se$}
\begin{definition}[Forking simulation-extractable NIZK, \cite{INDOCRYPT:FKMV12}]
	\label{def:simext}
  Let $\ps_\fs = (\kgen_\fs, \prover_\fs, \verifier_\fs, \simulator_\fs)$ be a
  HVZK proof system. We say that $\ps_\fs$ is \emph{forking
    simulation-extractable} with \emph{extraction error} $\nu$ if for any $\ppt$
  adversary $\adv$ that is given oracle access to a random oracle $\ro$ and
  simulator $\simulator_\fs$, and produces an accepting transcript of $\ps$ with
  probability $\accProb$, where
	\[
		\accProb = \Pr \left[
		\begin{aligned}
			& \verifier_\fs(\srs, \inp_{\advse}, \zkproof_{\advse}) = 1,\\
			& (\inp_{\advse}, \zkproof_{\advse}) \not\in Q
		\end{aligned}
		\, \left| \,
		\begin{aligned}
			& \srs \gets \kgen_\fs(\REL), r \sample \RND{\advse}, \\
			& (\inp_{\advse}, \zkproof_{\advse}) \gets \advse^{\simulator_\fs,
			\ro} (\srs; r)
		\end{aligned}
		\right.\right]\,,
	\]
	there exists an extractor $\extse$ such that
	\[
		\extProb = \Pr \left[
		\begin{aligned}
			& \verifier_\fs(\srs, \inp_{\advse}, \zkproof_{\advse}) = 1,\\
			& (\inp_{\advse}, \zkproof_{\advse}) \not\in Q,\\
			& \REL(\inp_{\advse}, \wit_{\advse}) = 1
		\end{aligned}
		\, \left| \,
		\begin{aligned}
			& \srs \gets \kgen_\fs(\REL), r \sample \RND{\advse},\\
			& (\inp_{\advse}, \zkproof_{\advse}) \gets \advse^{\simulator_\fs,
			\ro} (\srs; r) \\
			& \wit_{\advse} \gets \ext_\se (\srs, \advse, r, \inp_{\advse}, \zkproof_{\advse},
			Q, Q_\ro) 
		\end{aligned}
		\right.\right]
	\]
	is at at least 
	\[
		\extProb \geq \frac{1}{\poly} (\accProb - \nu)^d - \eps(\secpar)\,,
	\]
	for some polynomial $\poly$, constant $d$ and negligible $\eps(\secpar)$ whenever
  $\accProb \geq \nu$. List $Q$ contains all $(\inp, \zkproof)$ pairs where
  $\inp$ is an instance provided to the simulator by the adversary and
  $\zkproof$ is the simulator's answer. List $Q_\ro$ contains all $\advse$'s
  queries to $\ro$ and $\ro$'s answers.
\end{definition}

% Consider a sigma protocol $\sigmaprot = (\prover, \verifier, \simulator)$ that
% is special-sound and has a unique response property. Let $\sigmaprot_\fs =
% (\prover_\fs, \verifier_\fs, \simulator_\fs)$ be a NIZK obtained by applying the
% Fiat--Shamir transform to $\sigmaprot$. Faust et al.~\cite{INDOCRYPT:FKMV12}
% show that every such $\sigmaprot_\fs$ is forking simulation-extractable. This result is
% presented in \cref{sec:forking_lemma} along with the instrumental forking lemma,
% cf.~\cite{CCS:BelNev06}.

\iffalse
\noindent \textbf{Simulation sound NIZKs.}
Another notion for non-malleable NIZKs is \emph{simulation soundness}. It allows the adversary to see simulated proof, however, in contrast to simulation
extractability it does not require an extractor to provide a witness for the
proven statement. Instead, it is only necessary, that an adversary who sees
simulated proofs cannot make the verifier accept a proof of an incorrect
statement. More precisely,
\chaya{this definition will go}
\begin{definition}[Simulation soundness]
  	\label{def:simsnd}
    Let $\ps = (\kgen, \prover, \verifier, \simulator)$ be a NIZK proof and
    $\ps_\fs = (\kgen_\fs, \prover_\fs, \verifier_\fs, \simulator_\fs)$ be $\ps$
    transformed by the Fiat--Shamir transform. We say that $\ps_\fs$ is
    \emph{simulation-sound}
    for any $\ppt$ adversary $\adv$ that is given oracle access to a random
    oracle $\ro$ and simulator $\simulator_\fs$, probability
    \[
      \ssndProb =
      \Pr\left[
        \begin{aligned}
          & \verifier_\fs(\srs, \inp_{\adv}, \zkproof_{\adv}) = 1,\\
          & (\inp_{\advse}, \zkproof_{\advse}) \not\in Q,\\
          & \neg \exists \wit_{\adv}: \REL(\inp_{\adv}, \wit_{\adv}) = 1
        \end{aligned}
        \, \left| \,
          \vphantom{\begin{aligned}
          & \verifier_\fs(\srs, \inp_{\adv}, \zkproof_{\adv}) = 1,\\
          & (\inp_{\advse}, \zkproof_{\advse}) \not\in Q,\\
          & \neg \exists \wit_{\adv}: \REL(\inp_{\adv}, \wit_{\adv}) = 1
        \end{aligned}}
      \begin{aligned}
        & \srs \gets \kgen(\REL), r \sample \RND{\advse},\\
        & (\inp_{\advse}, \zkproof_{\advse}) \gets \advse^{\simulator_\fs,
          \ro} (\srs; r)
      \end{aligned}
		\right.  \right]
    \]
    is at most negligible.  List $Q$ contains all $(\inp, \zkproof)$ pairs where
  $\inp$ is an instance provided to the simulator by the adversary and
  $\zkproof$ is the simulator's answer. 
\end{definition}

  \label{rem:simext_to_simsnd}
  We note that the probability $\ssndProb$ \cref{def:simsnd} can be expressed in
  terms of simulation-extractability. More precisely, the
  condition $\neg \exists \wit: \REL(\inp_\adv, \wit_\adv) = 1$ can be substituted with
  $\REL(\inp_\adv, \wit_\adv) = 0$, where $\wit_\adv$, returned by a possibly unbounded
  extractor, is either a witness to $\inp_\adv$ (if there exists any) or $\bot$ (if
  there is none). More precisely,
\[
      \ssndProb =
      \Pr\left[
        \begin{aligned}
          & \verifier_\fs(\srs, \inp_{\adv}, \zkproof_{\adv}) = 1,\\
          & (\inp_{\advse}, \zkproof_{\advse}) \not\in Q,\\
          & \REL(\inp_{\adv}, \wit_{\adv}) = 0
        \end{aligned}
        \, \left| \,
      \begin{aligned}
        & \srs \gets \kgen(\REL), r \sample \RND{\advse},\\
        & (\inp_{\advse}, \zkproof_{\advse}) \gets \advse^{\simulator_\fs,
          \ro} (\srs; r)\\
        & \wit_{\adv} \gets \ext(\srs, \advse, r, \inp_{\advse}, \zkproof_{\advse},
			Q, Q_\ro,) 
      \end{aligned}
		\right.  \right].
\]
The only necessary input to the unbounded extractor $\ext$ is the instance
$\inp_\adv$ (the rest is given for the consistency with the simulation extractability
definition). 
%
With the probabilities in \cref{def:simext} holding regardless of whether the extractor
is unbounded or not, we obtain the following equality
$ \ssndProb = \accProb - \extProb$.

% In \cref{cor:simext_to_ssnd} we show that (under some mild conditions) this is enough
% to conjecture that probability $\ssndProb$ is not only at most negligible, but
% also, in some parameters, exponentially smaller than $(1 - \extProb)$
% (probability of extraction failure in \cref{def:simext}).

\fi
\section{Definitions and lemmas for multi-round SRS-based protocols}
\label{sec:se_definitions}
The result of Faust et al.\cite{INDOCRYPT:FKMV12} do not apply to our setting
since the protocols we consider have an SRS, more than three messages, require more than
just two transcripts for standard model extraction and are not special
sound. We thus adapt special
soundness to forking soundness, and generalize the forking lemma and the unique response property to make them compatible with
multi-round SRS-based protocols.

\subsection{Generalised forking lemma.}
%\label{sec:forking_lemma}
First of all, although dubbed ``general'', \cref{lem:forking_lemma} is not
general enough for our purpose as it is useful only for protocols where witness
can be extracted from just two transcripts. To be able to extract a witness
from, say, an execution of $\plonkprot$ we need to obtain at least
$(3 \numberofconstrains + 1)$ valid proofs, and $(\multconstr + \linconstr + 1)$ for $\sonicprot$. Here we
propose a generalisation of the general forking lemma that given probability of
producing an accepting transcript, $\accProb$, lower-bounds the probability of
generating a \emph{tree of accepting transcripts} $\tree$, which allows to
extract a witness.

\begin{definition}[Tree of accepting transcripts, cf.~{\cite{EC:BCCGP16}}]
	\label{def:tree_of_accepting_transcripts}
	Consider a $(2\mu + 1)$-message interactive proof system $\ps$. A $(n_1,
  \ldots, n_\mu)$-tree of accepting transcripts is a tree where each node on
  depth $i$, for $i \in \range{1}{\mu + 1}$, is an $i$-th prover's message in an
  accepting transcript; edges between the nodes are labeled with verifier's
  challenges, such that no two edges on the same depth have the same
  label; and each node on depth $i$ has $n_{i} - 1$ siblings and $n_{i +
    1}$ children. The tree consists of $N = \prod_{i = 1}^\mu n_i$
  branches, where $N$ is the number of accepting transcripts. We require $N = \poly$.
\end{definition}


\begin{lemma}[General forking lemma II]
	\label{lem:generalised_forking_lemma}
	Fix $q \in \ZZ$ and set $H$ of size $h \geq m$. Let $\zdv$ be a $\ppt$
  algorithm that on input $y, h_1, \ldots, h_q$ returns $(i, s)$ where $i \in
  \range{0}{q}$ and $s$ is called a side output. Denote by $\ig$ a randomised
  instance generator. We denote by $\accProb$ the probability
	\[
		\condprob{i \neq 0}{ y \gets \ig;\ h_1, \ldots, h_q \sample H;\ (i, s)
		\gets \zdv(y, h_1, \ldots, h_q)}\,.
	\]
	Let $\genforking_{\zdv}^{m}$ denote the algorithm described in
  \cref{fig:genforking_lemma} then the probability $\frkProb := \condprob{b =
    1}{y \gets \ig;\ h_1, \ldots, h_{q} \sample H;\ (b, \vec{s}) \gets
    \genforking_{\zdv}^{m}(y, h_1, \ldots, h_q)}$ is at least
	\[
		\frac{\accProb^m}{q^{m - 1}} - \accProb \cdot \left(1 -
    \frac{h!}{(h - m)! \cdot h^{m}}\right).
	\]
		
	\begin{figure}[t]
		\centering
		\fbox{
		\procedure{$\genforking_{\zdv}^{m} (y,h_1^{1}, \ldots, h_{q}^{1})$}		
		{
		\rho \sample \RND{\zdv}\\
		(i, s_1) \gets \zdv(y, h_1^{1}, \ldots, h_{q}^{1}; \rho)\\
    i_1 \gets i\\
		\pcif i = 0\ \pcreturn (0, \bot)\\
		\pcfor j \in \range{2}{m}\\
		\pcind h_{1}^{j}, \ldots, h_{i - 1}^{j} \gets h_{1}^{j - 1}, \ldots,
		h_{i - 1}^{j - 1}\\
		\pcind h_{i}^{j}, \ldots, h_{q}^{j} \sample H\\
		\pcind (i_j, s_j) \gets \zdv(y, h_1^{j}, \ldots, h_{i - 1}^{j}, h_{i}^{j},
		\ldots, h_{q}^{j}; \rho)\\
		\pcind \pcif i_j = 0 \lor i_j \neq i\ \pcreturn (0, \bot)\\
    \pcif \exists (j, j') \in \range{1}{m}^2, j \neq j' : (h_{i}^{j} = h_{i}^{j'})\
	\pcreturn (0, \bot)\\
		\pcelse \pcreturn (1, \vec{s})
	}}
	\caption{Generalised forking algorithm $\genforking_{\zdv}^{m}$}
	\label{fig:genforking_lemma}
\end{figure}
\end{lemma}
The proof goes similarly to \cite[Lemma 1]{CCS:BelNev06} with some modifications required
by the fact that the protocol has more than 3 rounds and the number of
transcripts required is larger. Due to page limit, the proof is presented in
\cref{sec:forking_proof}.

To highlight importance of the generalised forking lemma we describe how we use
it in our forking simulation-extractability proof.  Let $\proofsystem$ be a
forking sound proof system where for an instance $\inp$ the
corresponding witness can be extracted from an
$(1, \ldots, 1, n_k, 1, \ldots, 1)$-tree of accepting transcripts.  Let $\advse$
be the simulation-extractability adversary that outputs an accepting proof with
probability at least $\accProb$. (Although we use the same $\accProb$ to denote
probability of $\zdv$ outputing a non-zero $i$ and the probability of $\advse$
outputing an accepting proof, we claim that these probabilities are exactly the
same by the way we define $\zdv$.)  Let $\advse$ produce an accepting
proof $\zkproof_{\advse}$ for instance $\inp_{\advse}$; $r$ be $\advse$'s
randomness; $Q$ the list of queries submitted by $\advse$ along with simulator
$\simulator$'s answers; and $Q_\ro$ be the list of all random oracle
queries made by $\advse$.  All of these are given to the extractor $\ext$ that
internally runs the forking algorithm $\genforking_\zdv^{n_k}$.  Algorithm $\zdv$
takes $(\srs, \advse,
%\inp_\advse,
%\zkproof_\advse, 
Q, r)$ as input $y$ and $Q_\ro$ as input $h_1^1, \ldots,
h_q^1$. 
(For the sake of completeness, we allow $\genforking_\zdv^{n_k}$ to
pick $h^1_{l + 1}, \ldots, h^1_q$ responses if $Q_\ro$ has only $l < q$
elements.)  

Next, $\zdv$ internally runs $\advse(\srs; r)$ and responds to its random
oracle and simulator queries by using $Q_\ro$ and $Q$. Note that $\advse$ makes
the same queries as it did before it output $(\inp_{\advse}, \zkproof_{\advse})$
as it is run on the same random tape and with the same answers from the
simulator and random oracle. Once $\advse$ outputs 
$\zkproof_{\advse}$, algorithm $\zdv$ outputs $(i, \zkproof_{\advse})$, where
$i$ is the index of a random oracle query submitted by $\advse$ to receive the challenge after the
$k$-th message from the prover---a message where the tree of transcripts
branches.
Then, after the first run of $\advse$ is done, the extractor runs $\zdv$ again,
but this time it provides fresh random oracle responses $h^2_i, \ldots,
h^2_q$. Note that this is equivalent to rewinding $\advse$ to a point just
before $\advse$ is about to ask its $i$-th random oracle
query. The probability that the adversary produces an accepting transcript with the
fresh random oracle responses is at least $\accProb$. This continues until the
required number of transcripts is obtained. 

We note that in the original forking lemma, the forking algorithm $\forking$,
cf.~\cref{fig:forking_lemma}, gets only as input $y$ and elements $h^1_1, \ldots,
h^1_q$ are randomly picked from $H$ internally by $\forking$. However, assuming
that $h^1_1, \ldots, h^1_q$ are random oracle responses, and thus random, makes
the change only notational.

We also note that the general forking lemma proposed in
\cref{lem:generalised_forking_lemma} works for protocols with an extractor that can obtain the
witness from a $(1, \ldots, 1, n_k, 1, \ldots, 1)$-tree of accepting
transcripts. This limitation however does not affect the main result of this
paper, i.e.~showing that both $\plonk$ and $\sonic$ are forking simulation extractable.

\subsection{Unique-response protocols}
Another technical hurdle is the assumption of unique response property 
of the transformed sigma protocol required by Faust et al. The original
Fischlin's formulation, although suitable for applications presented in
\cite{C:Fischlin05,INDOCRYPT:FKMV12}, does not suffice in our case. First,
the property assumes that the protocol has three messages, with the second being
the challenge from the verifier. That is not the case we consider here. Second,
it is not entirely clear how to generalize the property. Should one require that
after the first challenge from the verifier, the prover's responses are fixed?
That does not work since the prover needs to answer differently on different
verifier's challenges, as otherwise the protocol could have fewer
rounds. Another problem is that the protocol could consist of
a round other than the first one where the prover message is randomized.
Unique response cannot hold in this case. Finally, the protocols we
consider here are not in the standard model, but use an SRS
what also complicates things considerably.

We walk around these obstacles by providing a generalised notion of the unique
response property. More precisely, we say that a $(2\mu + 1)$-message protocol
has \emph{unique responses from $i$}, and call it an $\ur{i}$-protocol, if it
follows the definition below:

\begin{definition}[$\ur{i}$-protocol]
\label{def:wiur}
Let $\proofsystem$ be a $(2\mu + 1)$-message public coin proof system
$\ps = (\kgen, \prover, \verifier, \simulator)$. Let $\proofsystem_\fs$ be
$\proofsystem$ after the Fiat--Shamir transform and $\ro$ the random
oracle. Denote by $a_1, \ldots, a_{\mu}, a_{\mu + 1}$ protocol messages output
by the prover, We say that $\proofsystem$ has \emph{unique responses from $i$
  on} if for any $\ppt$ adversary $\adv$:
\[
	\prob{
		\begin{aligned}
		&	\inp, \vec{a} = (a_1, \ldots, a_{\mu + 1}), \vec{a'} = (a'_1, \ldots,
    a'_{\mu + 1})
		\gets \adv^\ro(\srs), \\
    & \vec{a} \neq \vec{a'}, a_1, \ldots, a_{i} = a'_1,
    \ldots, a'_{i}, \\
		& \verifier^\ro_\fs (\srs, \inp, \vec{a}) =
		\verifier^\ro_\fs(\srs, \inp, \vec{a'}) = 1
		\end{aligned}
		\ \left|\  
	\vphantom{\begin{aligned}
	&	\vec{a} = (a_0, b_0, \ldots, a_j, b_j, a_\mu), \vec{a'} = (a'_0, b'_0, \ldots, a'_j,
	b'_j a'_\mu) \gets \adv(\srs), \vec{a} \neq \vec{a'}, \\
	& b_k = b'_k, k \in \range{1, \mu - 1},\\ a_l = a'_l, l \in
\range{1}{j}, j > i 
	\end{aligned}}
\srs \gets \kgen_\fs(\REL) \right.
}
\]
is upper-bounded by some negligible function $\negl$.
% Let $\proofsystem$ be a $(2\mu + 1)$-message public coin proof system
% $\ps = (\kgen, \prover, \verifier, \simulator)$ and let $r$ be verifier's
% randomness which determines its challenges $r_1, \ldots, r_\mu$. Denote by
% $\vec{a} = a_1, \ldots, a_{\mu}, a_{\mu + 1}$ protocol messages output by the prover and by $r_1, \ldots, r_\mu$ the challenges of the verifier, We
% say that $\proofsystem$ has \emph{unique responses from $i$ on} if for any
% $\ppt$ adversary $\adv$:
% \[
% 	\prob{
% 		\begin{aligned}
%     & \vec{a} \neq \vec{a'}, a_1, \ldots, a_{i} = a'_1,
%     \ldots, a'_{i}, \\
% 		& \verifier (\srs, \inp, \vec{a}; r) =
% 		\verifier(\srs, \inp, \vec{a'}; r) = 1
% 		\end{aligned}
% 		\ \left|\  
% 	\vphantom{\begin{aligned}
% 	&	\vec{a} = (a_0, b_0, \ldots, a_j, b_j, a_\mu), \vec{a'} = (a'_0, b'_0, \ldots, a'_j,
% 	b'_j a'_\mu) \gets \adv(\srs), \vec{a} \neq \vec{a'}, \\
% 	& b_k = b'_k, k \in \range{1, \mu - 1},\\ a_l = a'_l, l \in
% \range{1}{j}, j > i 
% 	\end{aligned}}
%   \begin{aligned}
% &\srs \gets \kgen(\REL) \\
% &\inp, \vec{a}, \vec{a'} \gets \adv(\srs) \\
% & r_1, \ldots, r_i \gets H^i\\
% &    r_{i+1}, \ldots r_\mu \gets H^{\mu-i}, r'_{i+1}, \ldots r'_\mu \gets H^{\mu-i} 
% \end{aligned}
%      \right.
% } \leq \negl.
% \]
\end{definition}
Intuitively, a protocol is $\ur{i}$ if it is infeasible for a $\ppt$ adversary
to produce a pair of acceptable and different proofs $\zkproof$, $\zkproof'$
that are the same on  first $i$ messages. 
% after $i$-th prover's message, all
% $\prover$'s further messages are determined by the witness it knows, the
% messages it already send and received and the future challenges from the
% verifier.
We note that the definition above is also meaningful for protocols without an SRS. Intuitively in that case $\srs$ is the empty string.

\iffalse
\subsection{Forking soundness}
Note that the special soundness property (as usually defined) holds for
all---even computationally unbounded---adversaries. Unfortunately, since a
simulation trapdoors for $\plonkprot$ and $\sonicprot$ exist, the protocols
cannot be special sound in that regard. This is because an unbounded adversary
could recover the trapdoor and build a number of simulated proofs for a fake
statement. Hence, we provide a weaker, yet sufficient, definition of
\emph{forking soundness}. More precisely, we state that an
adversary that is able to answer correctly multiple challenges either knows the
witness or can be used to break some computational assumption.


\begin{definition}[Forking soundness]
  Let $\proofsystem = (\kgen, \prover, \verifier, \simulator)$ be an
  $(2 \mu + 1)$-message proof system for a relation $\REL$. We say that
  $\proofsystem$ is $(\epsss(\secpar), (n_1, \ldots, n_\mu))$-\emph{forking sound}
  if there exists an extractor $\extt$ that given an $(n_1, \ldots, n_\mu)$-tree
  of acceptable transcripts $\tree$ and instance $\inp$ output by some $\ppt$ adversary $\adv(\REL,
  \srs)$, for $\srs \sample \kgen(\REL)$, outputs $\wit$ such that $\REL(\inp,
  \wit) = 1$ with probability at least $1 - \epsss$.
\end{definition}

Since we do not utilise the classical special soundness (that holds for all,
even unbounded, adversaries) all references to that property should be
understood as references to its computational version.
\fi

\subsection{Forking soundness}
Note that the special soundness property (as usually defined) holds for
all---even computationally unbounded---adversaries. Unfortunately, since a
simulation trapdoors for $\plonkprot$ and $\sonicprot$ exist, the protocols
cannot be special sound in that regard. This is because an unbounded adversary
can recover the trapdoor and build a number of simulated proofs for a fake
statement. Hence, we provide a weaker, yet sufficient, definition of
\emph{forking soundness}. More precisely, we state that an
adversary that is able to answer correctly multiple challenges either knows the
witness or can be used to break some computational assumption.
\chaya{a notion of computational special soundness has been used to mean exactly the above in prior works, like BBF19. we should clarify if forking soundness different from computational special soundness? seems to me like the diference is just that here it is tailored for the NI version.}
\chaya{I now see that the diff is the access to the simulator that the adversary gets. might be helpful to clarify this, either here or in a tech overview section. I am not sure why this needs to be defined this way by giving access to the simulator.}
However, differently from the standard definition of special soundness, we do
not require from the extractor to be able to extract the witness from \emph{any}
tree of acceptable transcripts. We require that the tree be produced honestly,
that is, all challenges are picked randomly---exactly as an honest verifier would pick.
Intuitively, the tree is as it would be generated by a $\genforking$
algorithm from the generalized forking lemma.

% \begin{definition}[Forking soundness]
%   % Let $\proofsystem = (\kgen, \prover, \verifier, \simulator)$ be an
%   % $(2 \mu + 1)$-message proof system for a relation $\REL$. Let $\tree$ be an
%   % $(n_1, \ldots, n_\mu)$-tree of acceptable transcripts output by a $\ppt$ tree
%   % building algorithm $\tdv$ which plays the role of the verifier $\ps.\verifier$
%   % against $\ppt$ adversary $\adv$ which it interacts with.
% %
%   % We say that $\proofsystem$ is
%   % $(\epsss, (n_1, \ldots, n_\mu))$-\emph{computationally special sound} if there
%   % exists an extractor $\extt$ that given an $ (n_1, \ldots, n_\mu)$-tree of
%   % acceptable transcripts $\tree$ for an instance $\inp \in \LANG_\REL$ output by
%   % a $\ppt$ tree building adversary $\tdv$ some $\ppt$ adversary
%   % $\adv(\srs)$, for $\srs \sample \kgen(\REL)$, outputs $\wit$ such that
%   % $\REL(\inp, \wit) = 1$ with probability at least $1 - \epsss$.
%   Let $\proofsystem = (\kgen, \prover, \verifier, \simulator)$ be an
%   $(2 \mu + 1)$-message proof system for a relation $\REL$. Let $\tree$ be an
%   $(n_1, \ldots, n_\mu)$-tree of transcripts for an in valid instance
%   $\inp$. We say that $\ps$ is $(\epsss, (n_1, \ldots, n_\mu))$-forking sound if there is an extractor $\extt$ that given $\tree$ extracts $\wit$
%   such that $\REL(\inp, \wit) = 1$ with probability at least $1 - \epsss.$
% \end{definition}

% \begin{definition}[$k$-round forking soundness]
%   % Let $\proofsystem = (\kgen, \prover, \verifier, \simulator)$ be an
%   % $(2 \mu + 1)$-message proof system for a relation $\REL$. Let $\tree$ be an
%   % $(n_1, \ldots, n_\mu)$-tree of acceptable transcripts output by a $\ppt$ tree
%   % building algorithm $\tdv$ which plays the role of the verifier $\ps.\verifier$
%   % against $\ppt$ adversary $\adv$ which it interacts with.
% %
%   % We say that $\proofsystem$ is
%   % $(\epsss, (n_1, \ldots, n_\mu))$-\emph{computationally special sound} if there
%   % exists an extractor $\extt$ that given an $ (n_1, \ldots, n_\mu)$-tree of
%   % acceptable transcripts $\tree$ for an instance $\inp \in \LANG_\REL$ output by
%   % a $\ppt$ tree building adversary $\tdv$ some $\ppt$ adversary
%   % $\adv(\srs)$, for $\srs \sample \kgen(\REL)$, outputs $\wit$ such that
%   % $\REL(\inp, \wit) = 1$ with probability at least $1 - \epsss$.
%   Let $\proofsystem = (\kgen, \prover, \verifier, \simulator)$ be an
%   $(2 \mu + 1)$-message proof system for a relation $\REL$. Let $\tree$ be the
%   algorithm below that rewinds $\advse^{\simulator_\fs,
%           \ro} (\srs; r)$ to produce a $(1, n_k, 1)$-tree of
%   transcripts such that none of the challenges in round $k$ were used in
%   simulated proofs. We say that $\ps$ is $(\epsss, ((1, n_k, 1)))$-forking
%   special sound if there is an extractor $\extt$ that given a tree produced by
%   $\tree$ extracts $\wit$ such that $\REL(\inp, \wit) = 1$ with probability at
%   least $1 - \epsss.$
% \end{definition}

% Since we do not utilise the classical special soundness (that holds for all,
% even unbounded, adversaries) all references to that property should be
% understood as references to its computational version.



\begin{definition}[$(\eps(\secpar), k,n)$-forking soundness]
  Let $\proofsystem = (\kgen, \prover, \verifier, \simulator)$ be an
  $(2 \mu + 1)$-message proof system for a relation $\REL$.

For any $\ppt$ adversary $\advse^{\simulator_\fs,
  \ro} (\srs; r)$ we consider the procedure $\zdv$ that provided the transcript $(\srs, \adv, r, Q, Q_{H})$ and $h_1, \ldots, h_q$ runs $\adv$ by providing it with random oracle queries
and simulated proofs. While $Q_{H}$ is consistent with $h_1, \ldots, h_q$, it replays the proofs of $Q$.
%
$\zdv$ returns the index $i$ of the
  random oracle query made for challenge $k$ and the proof $\adv$ returns

  Consider the algorithm $\genforking_{\zdv}^{n}$
  that rewinds $\zdv$ to produce a $(1,\dots, n,\dots, 1)$-tree of
  transcripts such that none of the $n$ challenges in round $k$ were used in
  simulated proofs.

  We say that $\ps$ is $(\eps(\secpar), k,n)$-forking sound if
  for any PPT adversary the probability that
  \begin{align*}
    Pr\left[
    \REL(\inp, \wit) = 0
    \,\Biggl|\,
    \begin{aligned}
       & \srs \sample \kgen(\REL),
        r \sample \RND{\advse},
         (\inp_{\advse}, \zkproof_{\advse}) \gets \advse^{\simulator_\fs,\ro} (\srs; r), \\
       &    (1, \tree) \gets \genforking_{\zdv}^{m}((\srs,\adv,r,Q, Q_{H}),Q_{H}),
           \wit \gets \extt(\tree)
    \end{aligned}
    \right] \leq \eps(\secpar).
  \end{align*}
   List $Q$ contains all $(\inp, \zkproof)$ pairs where
  $\inp$ is an instance provided to the simulator by the adversary and
  $\zkproof$ is the simulator's answer. List $Q_\ro$ contains all $\advse$'s
  queries to $\ro$ and $\ro$'s answers.
\end{definition}

\begin{definition}[$(\eps(\secpar), k,n)$-forking soundness]
  Let $\proofsystem = (\kgen, \prover, \verifier, \simulator)$ be an
  $(2 \mu + 1)$-message proof system for a relation $\REL$.

  Let $\tdv$, called tree creator, be the
  algorithm below that rewinds the $\ppt$ adversary $\advse^{\simulator_\fs,
          \ro} (\srs; r)$ to produce a $(1,\dots, n,\dots, 1)$-tree of
  transcripts such that none of the $n$ challenges in round $k$ were used in
  simulated proofs.

  $\tdv$ has oracle access to $\adv$ and provides it with (oracle) access to
  random oracle $\ro$ and simulator $\simulator_\fs$ -- more precisely $\tdv$
  has an internal procedure $\bdv$ that provided $\srs$ and random oracle
  queries' responses $h_1, \ldots, h_q$ gives $\adv$ access to the random oracle
  and simulates proof for it. In the end, $\bdv$ returns the index $i$ of the
  random oracle query made for challenge $k$, the set $Q$ of simulator random
  oracle indexes, the instance $\inp$, and the proof $\adv$ returns. Eventually,
  $\tdv$ returns a $(1, \ldots, n, \dots, 1)$ tree of acceptable
  transcripts~$\tree$.


  \begin{figure}
    \centering
 		\fbox{ \procedure{$\tdv(\adv, \srs \sample \kgen(\REL))$}
      {h_1^{1}, \ldots,
      h_{q}^1 \sample H \\
      (i, Q, \inp, \zkproof_1) \gets \bdv(\adv, \srs, h_1^{1}, \ldots, h_{q}^{1})\\
        % i_1 \gets i\\
        \pcif i\in Q \lor \verifier(\srs, \inp, \zkproof_1) = 0\ \pcreturn (0, \bot)\\
        \pcfor j \in \range{2}{m}\\
        \pcind h_{1}^{j}, \ldots, h_{i - 1}^{j} \gets h_{1}^{j - 1}, \ldots,
        h_{i - 1}^{j - 1}\\
        \pcind h_{i}^{j}, \ldots, h_{q}^{j} \sample H\\
        \pcind (i_j, Q_j, \inp_j, \zkproof_j) \gets \bdv(\adv, \srs, h_1^{j}, \ldots, h_{i -
          1}^{j}, h_{i}^{j},
        \ldots, h_{q}^{j})\\
        \pcind \pcif i \neq i_j \lor i_j \in Q_j \lor \inp \neq \inp_j \lor \verifier(\srs, \inp_j, \zkproof_i) = 0\ \pcreturn (0, \bot)\\
        %\pcind \pcif i_j = 0 \lor i_j \neq i\ \pcreturn (0, \bot)\\
        % \pcif \exists (j, j') \in \range{1}{m}^2, j \neq j' : (h_{i}^{j} =
        % h_{i}^{j'})\
        % 
        % pcreturn (0, \bot)\\
        \pcelse \pcreturn (1, \tree = (\inp, \pmb{\pi}))}
    }
  \end{figure}
%
  We say that $\ps$ is $(\eps(\secpar), k,n)$-forking sound if
  for any PPT adversary the probability that
  \[
    \Pr\left[
    %  \begin{aligned}
       % & \forall_{\zkproof \in \tree} \verifier(\srs, \inp, \zkproof) = 1, \\
         \wit \gets \extt(\tree), 
         \REL(\inp, \wit) = 0
    %  \end{aligned}
      \,\left|\,
        %\begin{aligned}
           \srs \sample \kgen(\REL), 
           (1, \tree) \gets \tdv(\adv, \srs)
     %   \end{aligned}
      \right.\right] \leq \eps(\secpar).
      \]
\end{definition}

\section{Simulation soundness and forking simulation-extractability---the general result}
\label{sec:general}
Equipped with definitional framework of \cref{sec:se_definitions} we are ready
to present the main result of this paper---a proof of simulation soundness and
forking simulation extractability of Fiat-Shamir NIZK based on multi-round protocols.

The proofs go by game hopping. The games are controlled by an environment $\env$
that internally runs a simulation extractability adversary $\advse$, provides it
with access to a random oracle and simulator, and when necessary, rewinds it. The
games differ by various breaking points, i.e.~points where the environment
decides to abort the game.

Denote by $\zkproof_{\advse}, \zkproof_{\simulator}$ proofs returned by the
adversary and the simulator respectively. We use $\zkproof[i]$ to denote
prover's message in the $i$-th round of the proof (counting from 1),
i.e.~$(2i - 1)$-th message exchanged in the protocol. $\zkproof[i].\ch$ denotes
the challenge that is given to the prover after $\zkproof[i]$, and
$\zkproof[i..j]$ to denote all messages of the proof including challenges
between rounds $i$ and $j$, but not challenge $\zkproof[j].\ch$. When it is not
explicitly stated, we denote the proven instance $\inp$ by $\zkproof[0]$
(however, there is no following challenge $\zkproof[0].\ch$).

Without loss of generality, we assume that whenever the accepting proof contains
a response to a challenge from a random oracle, then the adversary queried the
oracle to get it. It is straightforward to transform any adversary that violates
this condition into an adversary that makes these additional queries to the
random oracle and wins with the same probability.

\begin{theorem}[Simulation soundness]
  \label{thm:simsnd}
  Assume that $\ps$ is $k$-programmable HVZK in the standard model, that is
  $\epss(\secpar)$-sound and $\ur{k}$ with security $\epsur(\secpar)$. Then, the
  probability that a $\ppt$ adversary $\adv$ breaks simulation soundness of
  $\ps_{\fs}$ is upper-bounded by
  \(
    \epsur(\secpar) + q_\ro^\mu  \epss(\secpar)\,,
  \)
  where $q$ is the total number of queries made by the adversary $\adv$ to a
  random oracle $\ro\colon \bin^{*} \to \bin^{\secpar}$.
\end{theorem}

\begin{proof}
  \ngame{0} This is a simulation soundness game played between an adversary
  $\adv$ who is given access to a random oracle $\ro$ and simulator
  $\psfs.\simulator$. $\adv$ wins if it manages to produce an accepting proof
  for a false statement. In the following game hops, we upper-bound the
  probability that this happens.

  \ngame{1} This is identical to $\game{0}$ except that the game is aborted if
  there is a simulated proof $\zkproof_\simulator$ for $\inp_{\adv}$ such that
  $(\inp_{\adv}, \zkproof_\simulator[1..k]) = (\inp_{\adv},
  \zkproof_{\adv}[1..k])$. That is, the adversary in its final proof reuses at
  least $k$ messages from a simulated proof it saw before and the proof is
  accepting.  Denote this event by $\event{\errur}$.

  \ncase{Game 0 to Game 1} We have, \( \prob{\game{0} \land
    \nevent{\errur}} = \prob{\game{1} \land \nevent{\errur}} \) and, from the
  difference lemma, cf.~\cref{lem:difference_lemma},
  $ \abs{\prob{\game{0}} - \prob{\game{1}}} \leq \prob{\event{\errur}}\,$.
  Thus, to show that the transition from one game to another introduces only
  minor change in probability of $\adv$ winning it should be shown that
  $\prob{\event{\errur}}$ is small.

  We can assume that $\adv$ queried the simulator on the instance it wishes to
  output, i.e.~$\inp_{\adv}$. We show a reduction $\rdvur$ that utilises $\adv$
  to break the $\ur{k}$ property of $\ps$. Let $\rdvur$ run $\advse$ internally
  as a black-box:
  \begin{compactitem}
	\item The reduction answers both queries to the simulator $\psfs.\simulator$
    and to the random oracle.  It also keeps lists $Q$, for the simulated
    proofs, and $Q_\ro$ for the random oracle queries.
  \item When $\adv$ makes a fake proof $\zkproof_{\adv}$ for $\inp_{\adv}$,
    $\rdvur$ looks through lists $Q$ and $Q_\ro$ until it finds
    $\zkproof_{\simulator}[0..k]$ such that
    $\zkproof_{\adv}[0..k] = \zkproof_{\simulator}[0..k]$ and a random oracle
    query $\zkproof_{\simulator}[k].\ch$ on $\zkproof_{\simulator}[0..k]$.
	\item $\rdvur$ returns two proofs for $\inp_{\adv}$:
	\begin{align*}
		\zkproof_1 = (\zkproof_{\simulator}[1..k],
		\zkproof_{\simulator}[k].\ch, \zkproof_{\simulator}[k + 1..\mu + 1])\\
		\zkproof_2 = (\zkproof_{\simulator}[1..k],
		\zkproof_{\simulator}[k].\ch, \zkproof_{\adv}[k + 1..\mu + 1])
	\end{align*}
	\end{compactitem}  
	If $\zkproof_1 = \zkproof_2$, then $\adv$ fails to break simulation soundness,
  as $\zkproof_2 \in Q$. On the other hand, if the proofs are not equal, then
  $\rdvur$ breaks $\ur{k}$-ness of $\ps$. This happens only with negligible
  probability $\epsur(\secpar)$, hence
  \( \prob{\event{\errur}} \leq \epsur(\secpar)\,. \)
	
  \ngame{2} This is identical to $\game{1}$ except that now the environment
  aborts if the instance the adversary proves is not in the language.

  \ncase{Game 1 to Game 2} 
  % REDUCTION TO INTERACTIVE SOUNDNESS:
  We show that
  $\abs{\prob{\game{1}} - \prob{\game{2}}} \leq q^{\mu} \cdot \epss(\secpar)$,
  where $\epss(\secpar)$ is the probability of breaking soundness of the underlying
  \emph{interactive} protocol $\ps$. Note that
  $\abs{\prob{\game{1}} - \prob{\game{2}}}$ is the probability that $\adv$
  outputs an acceptable proof for a false statement which does not break the
  unique response property (such proofs have been excluded by
  $\game{1}$). Consider a soundness adversary $\adv'$ who initiates a proof with
  $\ps$'s verifier $\ps.\verifier$, internally runs $\adv$ and proceeds as
  follows:
  \begin{compactitem}
  \item It guesses indices $i_1, \ldots, i_\mu$ such that random oracle queries
    $h_{i_1}, \ldots, h_{i_\mu}$ are the queries used in the $\zkproof_\adv$
    proof eventually output by $\adv$. This is done with probability at least
    $1/q^\mu$ (since there are $\mu$ challenges from the verifier in
    $\ps$).
  \item On input $h$ for the $i$-th,
    $i \not\in \smallset{{i_1}, \ldots, {i_\mu}}$, random oracle query, $\adv'$
    returns randomly picked $y$, sets $\ro(h) = y $ and stores $(h, y)$ in
    $Q_\ro$ if $h$ is sent to $\ro$ the first time. If that is not the case,
    $\adv$ finds $h$ in $Q_\ro$ and returns the corresponding $y$.
  \item On input $h_{i_j}$ for the $i_j$-th,
    $i_j \in \smallset{{i_1}, \ldots, {i_\mu}}$, random oracle query, $\adv'$
    parses $h_{i_j}$ as a partial proof transcript $\zkproof_\adv[1..j]$ and
    runs $\ps$ using $\zkproof_\adv[j]$ as a $\ps.\prover$'s $j$-th message to
    $\ps.\verifier$. The verifier responds with a challenge
    $\zkproof_\adv[j].\ch$. $\adv'$ sets $\ro(h_{i_j}) =
    \zkproof_\adv[j].\ch$. If we guessed the indices correctly we have that
    $h_{i_{j'}}$, for $j' \leq j$, parsed as $\zkproof_\adv[1..j']$ is a prefix
    of $\zkproof_\adv[1..j]$.
  \item On query $\inp_\simulator$ to $\simulator$, $\adv'$ runs the simulator
    $\ps.\simulator$ internally. Note that we require a simulator that only
    programs the random oracle for $j \geq k$.  If the simulator makes a
    previously unanswered random oracle query with input
    $\zkproof_\simulator[1..j]$, $1 \leq j < k$, and this is the $i_j$-th query,
    it generates $\zkproof_\simulator[j].\ch$ by invoking $\ps.\verifier$ on
    $\zkproof_\simulator[j]$ and programs
    $\ro(h_{i_j}) = \zkproof_\simulator[j].\ch$.  It returns
    $\zkproof_\simulator$.
  \item Answers $\ps.\verifier$'s final challenge $\zkproof_\adv[\mu].\ch$ using the
    answer given by $\adv$, i.e.~$\zkproof_\adv[\mu]$.
  \end{compactitem}
  That is, $\adv'$ manages to break soundness of $\ps$ if $\adv$ manages to
  break simulation soundness without breaking the unique response property and
  $\adv'$ correctly guesses the indices of $\adv$ random oracle queries. This
  happens with probability upper-bounded by $\abs{\prob{\game{1}} -
    \prob{\game{2}}} \cdot \infrac{1}{q^{\mu}}$. Hence $\abs{\prob{\game{1}} -
    \prob{\game{2}}} \leq q^{\mu} \cdot \epss(\secpar)$.

  Note that in $\game{2}$ the adversary cannot win. Thus the probability
  that $\advss$ is successful is upper-bounded by
  $\epsur(\secpar) + q^{\mu} \cdot \epss(\secpar)$.  \qed
\end{proof}


We conjecture that based on the recent results on state restoration soundness~\cite{cryptoeprint:2020:1351}, which effectively allows to query the verifier multiple times on different overlapping transcripts, the $q^{\mu}$ loss could be avoided. However, this would reduce the class of protocols covered by our results. 


\begin{theorem}[Forking simulation-extractable multi-message protocols]
  \label{thm:se}
  Let $\ps = (\kgen, \prover, \verifier, \simulator)$ be an interactive
  $(2 \mu + 1)$-message proof system for $\RELGEN(\secparam)$ that is honest
  verifier zero-knowledge in the standard model\footnote{Crucially, we require
    that one can provide an indistinguishable simulated proof without any
    additional knowledge, as e.g~knowledge of a SRS trapdoor.}, has $\ur{k}$
  property with security $\epsur(\secpar)$, and is $(\epss(\secpar), k, n)$-forking sound.
  % for
  % $n_i = 1, i \in \range{1}{\mu} \setminus \smallset{k}$ and $n_k = n$.
%
Let $\ro\colon \bin^{*} \to \bin^{\secpar}$ be a random oracle. 
Then $\psfs$ is forking simulation-extractable with extraction error $\epsur(\secpar)$
against $\ppt$ algebraic adversaries that makes up to $q$ random oracle queries and
returns an acceptable proof with probability at least $\accProb$. 
The extraction probability $\extProb$ is at least
\(
	\extProb \geq \frac{1}{q^{n - 1}} (\accProb - \epsur(\secpar))^{n} -\eps(\secpar)\,,
\)
for some negligible $\eps(\secpar)$.	
\end{theorem}
\begin{proof}		

  \ngame{0} This is a simulation extraction game played between an adversary
  $\advse$ who has given access to a random oracle $\ro$ and simulator
  $\psfs.\simulator$. There is also an extractor $\ext$ that, from a proof
  $\zkproof_{\advse}$ for instance $\inp_{\advse}$ output by the adversary and from
   transcripts of $\advse$'s operations is tasked to extract a witness
  $\wit_{\advse}$ such that $\REL(\inp_{\advse}, \wit_{\advse})$ holds. $\advse$ wins
  if it manages to produce an acceptable proof and the extractor fails to reveal
  the corresponding witness. In the following game hops we upper-bound the
  probability that this happens.

  \ngame{1} This is identical to $\game{0}$ except that now the game is aborted
  if there is a simulated proof $\zkproof_\simulator$ for $\inp_{\advse}$ such
  that $(\inp_{\advse}, \zkproof_\simulator[1..k]) = (\inp_{\advse},
  \zkproof_{\advse}[1..k])$. That is, the adversary in its final proof
  reuses at least $k$ messages from a simulated proof it saw before and the proof is acceptable.
  Denote that event by $\event{\errur}$.

  \ncase{Game 0 to Game 1} $\prob{\event{\errur}} \leq \epsur(\secpar)$. The
  proof goes exactly as in \cref{thm:simsnd}.

  \COMMENT{We have,
    \( \prob{\game{0} \land \nevent{\errur}} = \prob{\game{1} \land
      \nevent{\errur}} \) and, from the difference lemma,
    cf.~\cref{lem:difference_lemma},
  \[ \abs{\prob{\game{0}} - \prob{\game{1}}} \leq \prob{\event{\errur}}\,. \]
  Thus, to show that the transition from one game to another introduces only
  minor change in probability of $\advse$ winning it should be shown that
  $\prob{\event{\errur}}$ is small.

  We can assume that $\advse$ queried the simulator on the instance it wishes to
  output---$\inp_{\advse}$. We show a reduction $\rdvur$ that utilises $\advse$,
  who outputs a valid proof for $\inp_{\advse}$, to break the $\ur{k}$ property of
  $\ps$. Let $\rdvur$ run $\advse$ internally as a black-box:
\begin{itemize}
	\item The reduction answers both queries to the simulator $\psfs.\simulator$ and to the random oracle. 
	It also keeps lists $Q$, for the simulated proofs, and $Q_\ro$ for the random oracle queries. 
\item When $\advse$ makes a fake proof $\zkproof_{\advse}$ for $\inp_{\advse}$,
  $\rdvur$ looks through lists $Q$ and $Q_\ro$ until it finds
  $\zkproof_{\simulator}[0..k]$ such that
  $\zkproof_{\advse}[0..k] = \zkproof_{\simulator}[0..k]$
  and a random oracle query $\zkproof_{\simulator}[k].\ch$ on
  $\zkproof_{\simulator}[0..k]$.
	\item $\rdvur$ returns two proofs for $\inp_{\advse}$:
	\begin{align*}
		\zkproof_1 = (\zkproof_{\simulator}[1..k],
		\zkproof_{\simulator}[k].\ch, \zkproof_{\simulator}[k + 1..\mu + 1])\\
		\zkproof_2 = (\zkproof_{\simulator}[1..k],
		\zkproof_{\simulator}[k].\ch, \zkproof_{\advse}[k + 1..\mu + 1])
	\end{align*}
	\end{itemize}  
	If $\zkproof_1 = \zkproof_2$, then $\advse$ fails to break simulation
  extractability, as $\zkproof_2 \in Q$. On the other hand, if the proofs are
  not equal, then $\rdvur$ breaks $\ur{k}$-ness of $\ps$. This happens only with
  negligible probability $\epsur(\secpar)$, hence \( \prob{\event{\errur}} \leq
  \epsur(\secpar)\,. \)
}

  \ngame{2} This is identical to $\game{1}$ except that now the environment
  aborts also when it fails to build a $(1, \ldots, 1, n, 1, \ldots, 1)$-tree
  of accepting transcripts $\tree$ by rewinding $\advse$. Denote that event by
  $\event{\errfrk}$. 

  \ncase{Game 1 to Game 2} Note that for every acceptable proof
  $\zkproof_{\advse}$, we may assume that whenever $\advse$ outputs in Round $k$
  message $\zkproof_{\advse}[k]$, then the
  $(\inp_{\advse}, \zkproof_{\advse}[1..k])$ random oracle query was made
  by the adversary, not the simulator\footnote{\cite{INDOCRYPT:FKMV12} calls
    these queries \emph{fresh}.}, i.e.~there is no simulated proof
  $\zkproof_\simulator$ on $\inp_\simulator$ such that
  $(\inp_{\advse}, \zkproof_{\advse} [1..k]) = (\inp_\simulator,
  \zkproof_\simulator[1..k])$. Otherwise, the game would be already interrupted
  by the error event in Game $\game{1}$.  As previously,
\(
  \abs{\prob{\game{1}} - \prob{\game{2}}} \leq \prob{\event{\errfrk}}\,.
\)

We describe our extractor $\ext$ here. The extractor takes as input relation
$\REL$, SRS $\srs$, $\advse$'s code, its randomness $r$, the output instance
$\inp_{\advse}$ and proof $\zkproof_{\advse}$, as well as the list $Q$ of
simulated proofs (and their instances) and the list of random oracle queries and
responses $Q_\ro$. Then, $\ext$ starts a forking algorithm
$\genforking^{n}_\zdv(y,h_1, \ldots, h_q)$ for
$y = (\srs, \advse, r, \inp_{\advse}, \zkproof_{\advse}, Q)$ where we set
$h_1, \ldots, h_q$ to be the consecutive queries from list $Q_\ro$. We run
$\advse$ internally in $\zdv$.% which returns the proof $\zkproof$ and index $i$
%of the random oracle query that $\advse$ used to answer $\zkproof$'s $k$-th challenge. 

To assure that in the first execution of $\zdv$ the adversary $\advse$ produce
the same $(\inp_{\advse}, \zkproof_{\advse})$ as in the extraction game, $\zdv$
provides $\advse$ with the same randomness $r$ and answers queries to the random
oracle and simulator with pre-recorded responses in $Q_\ro$ and $Q$.
%
Note, that since the view of the adversary when run inside $\zdv$ is the same as its
view with access to the real random oracle and simulator, it produces exactly the
same output. After the first run, $\zdv$ outputs the index $i$ of a random oracle
query that was used by $\advse$ to compute the challenge $\zkproof[k].\ch =
\ro(\zkproof_{\advse}[0..k])$ it had to answer in the $(k + 1)$-th round and
adversary's transcript, denoted by $s_1$ in $\genforking$'s description. If no
such query took place $\zdv$ outputs $i = 0$.

Then new random oracle responses are picked for queries indexed by
$i, \ldots, q$ and the adversary is rewound to the point just prior to when it gets the
response to RO query $\zkproof_{\advse}[0..k]$. The adversary gets a random
oracle response from a new set of responses $h^2_i, \ldots, h^2_q$. If the
adversary requests a simulated proof after seeing $h^2_i$ then $\zdv$ computes
the simulated proof on its own. Eventually, $\zdv$ outputs index $i'$ of a query
that was used by the adversary to compute $\ro(\zkproof_{\advse}[0..k])$, and a
new transcript $s_2$. $\zdv$ is run $n$ times with different random oracle
responses. If a tree $\tree$ of $n$ transcripts is built then $\ext$
runs internally the tree extractor $\extt(\tree)$ and outputs what it returns.

We emphasize here the importance of the unique response property. If it does not
hold then in some $j$-th execution of $\zdv$ the adversary could reuse a
challenge that it learned from observing proofs in $Q$. In that case, $\zdv$
would output $i = 0$, making the extractor fail. Fortunately, the case that the
adversary breaks the unique response property has already been covered by the
abort condition in $\game{1}$.

Denote by $\waccProb$ the probability that $\advse$ outputs a proof that is
accepted and does not break $\ur{k}$-ness of $\ps$.  Denote by $\waccProb'$ the
probability that algorithm $\zdv$, defined in the lemma, produces an accepting
proof with a fresh challenge after Round $k$. Given the discussion above, we can
state that $\waccProb = \waccProb'$.

Next, from the generalised forking lemma, cf.~\cref{lem:generalised_forking_lemma}, we get that
\begin{equation}
  \begin{split}
    \prob{\event{\errfrk}} \leq 1 - \waccProb \cdot \left(\infrac{\waccProb^{n -
          1}}{q^{n - 1}} + \infrac{(2^\secpar) !}{((2^\secpar - n)! \cdot
        (2^\secpar)^{n})} - 1\right).
    % = \\
    % 1 - \left(\frac{\waccProb^{n}}{q^{n - 1}} + 
    %   \waccProb \cdot \frac{(2^\secpar) !}{(2^\secpar - n)! \cdot
    %     (2^\secpar)^{n}} - \waccProb\right)\,.
\end{split}
\end{equation}

% \ngame{3} This game is identical to $\game{2}$ except that the environment
% aborts it when the adversary manages to break simulation soundness in one the
% transcripts in tree $\tree$.

% \ncase{$\game{2} \mapsto \game{3}$} From \cref{thm:simsnd} we have that
% probability that $\adv$ breaks simulation soundness, while not breaking the
% unique response property, is upper-bounded by $q_\ro^{\mu} \epss$. Since all the
% branches of the tree are either for a valid or invalid statement, then the
% probability that $\adv$ breaks simulation soundness in one of the tree branches
% is upper-bounded by $q_{\ro}^{\mu} \epss$ as well.

\ngame{3} This game is identical to $\game{2}$ except that it aborts if
$\extt(\tree)$ run by $\ext$ fails to extract the witness. 

\ncase{Game 2 to Game 3}	
Since $\ps$ is forking-sound the probability that $\extt(\tree)$
fails is upper-bounded by $\epsss(\secpar)$. 

Since Game $\game{3}$ is aborted when it is impossible to extract the correct witness
from $\tree$, hence the adversary $\advse$ cannot win.  Thus, by the game-hopping
argument,
\[
	\abs{\prob{\game{0}} - \prob{\game{4}}} \leq 1 -
  \left(\frac{\waccProb^{n}}{q^{n - 1}} + \waccProb \cdot \frac{(2^\secpar)
      !}{(2^\secpar - n)! \cdot (2^\secpar)^{n}} - \waccProb\right) + \epsur(\secpar) +
  %q_{\ro}^{\mu} \epss +
  \epsss(\secpar)\,.
\]
Thus the probability that extractor $\extss$ succeeds is at least
\[
	\frac{\waccProb^{n}}{q^{n - 1}} + \waccProb \cdot \frac{(2^\secpar)
    !}{(2^\secpar - n)! \cdot (2^\secpar)^{n}} - \waccProb - \epsur(\secpar) 
  %- q_{\ro}^{\mu} \epss
  - \epsss(\secpar)\,.
\]
Since $\waccProb$ is probability of $\advse$ outputting acceptable transcript
that does not break $\ur{k}$-ness of $\ps$, then $\waccProb \geq \accProb -
\epsur(\secpar)$, where $\accProb$ is the probability of $\advse$ outputing an acceptable
proof as defined in \cref{def:simext}. It thus holds
\begin{equation}
	\label{eq:frk}
	\extProb \geq \frac{(\accProb - \epsur(\secpar))^{n}}{q^{n - 1}} -
  \underbrace{(\accProb - \epsur(\secpar)) \cdot \left( 1 - \frac{(2^\secpar)
        !}{(2^\secpar - n)! \cdot (2^\secpar)^{n}}\right) - \epsur(\secpar) -
    % q_{\ro}^{\mu} \epss -
    \epsss(\secpar)}_{\eps(\secpar)}\,.
\end{equation}
Note that the part of \cref{eq:frk} denoted by $\eps(\secpar)$ is negligible as
$\epsur(\secpar), \epsss(\secpar)$ are negligible, and
$\frac{(2^\secpar) !}{(2^\secpar - n)! \cdot (2^\secpar)^{n}} \geq
\left(\infrac{(2^\secpar - n)}{2^\secpar}\right)^{n}$ is overwhelming.  Thus,
\[
	\extProb \geq q^{-(n - 1)} (\accProb - \epsur(\secpar))^{n} -\eps(\secpar)\,.
\] 
and $\psfs$ is forking simulation extractable with extraction error $\epsur(\secpar)$.
\qed
\end{proof}

% \ourpar{Inefficient simulation extractability gives efficient simulation soundness.}
% As noted in \cref{sec:simext_def}, simulation soundness can be expressed in
% terms of simulation extractability with an  unbounded extractor. Holds then,

% \begin{corollary}[Simulation extractability to simulation soundness.]
%   \label{cor:simext_to_ssnd}
%   Let $\ps$, as defined in \cref{thm:se} be simulation-extractable with
% 	\[
%     \extProb \geq \frac{(\accProb - \epsur)^{n}}{q^{n - 1}} -
% 	(\accProb - \epsur) \cdot \left( 1 -
%       \frac{(2^\secpar) !}{(2^\secpar - n)! \cdot
%         (2^\secpar)^{n}}\right)
%     - \epsur - \epsss,
%   \]
%   then it is simulation sound with
%   $\ssndProb \leq 2\epsur + \epsss 2^{-\secpar}(\accProb - \epsur)$
% \end{corollary}
% \begin{proof}
%   Since according to the discussion in \cref{rem:simext_to_simsnd} $\ssndProb =
%   \accProb - \extProb$ it holds:
%   \begin{align*}
%     \ssndProb \leq \accProb - \left(\frac{(\accProb - \epsur)^{n}}{q^{n - 1}} -
% 	(\accProb - \epsur) \cdot \left( 1 -
%       \frac{(2^\secpar) !}{(2^\secpar - n)! \cdot
%         (2^\secpar)^{n}}\right)
%     - \epsur - \epsss,\right).
%   \end{align*}
%   Furthermore, as the extractor $\ext$ from \cref{rem:simext_to_simsnd} is unbounded, we can
%   assume that it is able to extract the witness from a single run of a
%   simulation-extractability adversary $\adv$. hence $n = 1$. Thus,
%   \begin{equation*}
%     \begin{split}
%       \ssndProb & \leq \accProb - \left(\accProb - \epsur -
%                   (\accProb - \epsur) \cdot \left( 1 -
%                   1)\right)
%                   - \epsur - \epsss\right) \\
%                 & =  2 \epsur 
%                   + \epsss \\
%                 \end{split}
%               \end{equation*}
%               \qed
% \end{proof}


\section{Non-Malleability of $\plonkprotfs$} 
\label{sec:plonk}
In this section, we show that $\plonkprotfs$ is simulation-sound and forking simulation-extractable. To that
end, we proceed as follows. First, we show that the version of the KZG polynomial
commitment scheme that is proposed in the \plonk{} paper has the unique opening
property, cf.~\cref{sec:poly_com} and \cref{lem:pcomp_op}. This is then
used to show that $\plonkprot$ has the $\ur{2}$ property,
cf.~\cref{lem:plonkprot_ur}.

Next, we show that $\plonkprot$ is forking-sound. That is, given a
number of accepting transcripts which match on the first 3 rounds of the
protocol we can either recover a correct witness for the proven statement or use
one of the transcripts to break the $\dlog$ assumption. This result is shown in
the AGM, cf.~\cref{lem:plonkprot_ss}.

Given forking-soundness of $\plonkprot$, we use the fact that it is also
$\ur{2}$ and show, in a similar fashion to \cite{INDOCRYPT:FKMV12}, that it is
simulation-extractable. That is, we build reductions that given a simulation
extractability adversary $\advse$ either break the protocol's unique response
property or based on forking soundness break the $\dlog$ assumption, if extracting a valid witness from a
tree of transcripts is impossible. See \cref{thm:plonkprotfs_se}.

% Due to page limit, we omit description of \plonk{} here and refer to
% \cref{sec:plonk_explained}. Unfortunately, we also have to move some of the
% proofs to the Supplementary Materials as well, cf.~\cref{sec:plonk_supp_mat}

  
\subsection{Unique opening property of $\PCOMp$}
\begin{lemma}
\label{lem:pcomp_op}
Let $\PCOMp$ be a batched version of a KZG polynomial commitment,
cf.~\cref{fig:pcomp}, then $\PCOMp$ has the unique opening property in the AGM
with security
$\epsop(\secpar) \leq 2 \epsdlog(\secpar) + \infrac{1}{\abs{\FF_p}}$, where
$\epsdlog(\secpar)$ is security of the $(\noofc + 2, 1)$-dlog assumption and
$\FF_p$ is the field used in $\PCOMp$.\end{lemma}
\begin{proof}
  Let $\vec{z} = (z, z') \in \FF_p^2$ be the two points the polynomials are
  evaluated at, $k \in \NN$ be the number of the committed polynomials to be
  evaluated at $z$, and $k' \in \NN$ be the number of the committed polynomials
  to be evaluated at $z'$, $\vec{c} \in \GRP^k, \vec{c'} \in \GRP^{k'}$ be the
  commitments, $\vec{s} \in \FF_p^k, \vec{s'} \in \FF_p^{k'}$ the evaluations,
  and $\vec{o} = (o, o') \in \FF_p^2$ be the commitment openings.  We need to
  show that the probability a $\ppt$ $\adv$ opens the same commitment in two
  different ways is at most $\epsop(\secpar)$, even when the commitment openings
  are verified in batches.

  The idealised verifier checks whether the following equality, for $\gamma, r'$
  picked at random, holds:
  \begin{multline}
    \label{eq:ver_eq_poly}
    \left(\sum_{i = 1}^{k} \gamma^{i - 1} \cdot \p{f}_i(X) - \sum_{i = 1}^{k}
      \gamma^{i - 1} \cdot s_i\right) + r' \left(\sum_{i = 1}^{k'} \gamma'^{i -
        1} \cdot \p{f'}_i(X) -
      \sum_{i = 1}^{k'} \gamma'^{i - 1} \cdot s'_i \right)\\
    \equiv \p{o}(X)(X - z) + r' \p{o}'(X)(X- z').
  \end{multline}
  Since $r'$ has been picked at random from $\FF$, probability that
  \cref{eq:ver_eq_poly} holds while either
  \[
    \sum_{i = 1}^{k} \gamma^{i - 1} \cdot \p{f}_i(X) - \sum_{i = 1}^{k}
    \gamma^{i - 1} \cdot s_i \not\equiv \p{o}(X)(X - z) \text{, or}
  \]
  \[
    \sum_{i = 1}^{k'} \gamma'^{i - 1} \cdot \p{f'}_i(X) - \sum_{i = 1}^{k'}
    \gamma'^{i - 1} \cdot s'_i \not\equiv \p{o'}(X)(X - z')
  \]
  is $\infrac{1}{\abs{\FF_p}}$~cf.~\cite{EPRINT:GabWilCio19}. 
  When \(
    \sum_{i = 1}^{k} \gamma^{i - 1} \cdot \p{f}_i(X) - \sum_{i = 1}^{k}
    \gamma^{i - 1} \cdot s_i = \p{o}(X)(X - z) 
  \)
  holds, polynomial $\p{o}(X)$ is uniquely determined from the uniqueness of
  polynomial composition. Similarly, $\p{o'}(X)$ is uniquely determined as well.

  Since any discrepancy
  between the idealised verifier rejection and real verifier acceptance allows
  one to break the discrete logarithm problem, the probability that the real
  verifier accepts in one of the cases above is upper-bounded by
  $2 \epsdlog + \infrac{1}{\FF_p}$.
  \qed
\end{proof}


\subsection{Unique response property}
\begin{lemma}
	\label{lem:plonkprot_ur}
  Let $\PCOMp$ be commitment of knowledge with security $\epsk(\secpar)$,
  $\epsbind(\secpar)$-binding and has unique opening property with security
  $\epsop(\secpar)$, then probability that a $\ppt$ adversary $\adv$ breaks
  $\plonkprotfs$'s $\ur{2}$ property is at most
  $\epsop + 9 \cdot (\epsbind + \infrac{2}{\FF_p}) + \epss + \epsro$, where
  $\epsro$ is probability that a $\ppt$ adversary finds collision in a random
  oracle.
\end{lemma}
\begin{proof}
  Let
  $\adv(\REL,\srs = (\gone{1, \chi, \ldots, \chi^{\noofc + 2}}, \gtwo{\chi}))$
  be an algebraic adversary tasked to break the $\ur{2}$-ness of
  $\plonkprotfs$. We show that the first 2 rounds of the protocol determines,
  along with the verifiers challenges, the rest of it.  This is done by game
  hops. In the games, the adversary outputs two proofs $\zkproof$ and
  $\zkproof'$ for the same statement.  To distinguish polynomials and
  commitments which an honest prover sends in the proof from the polynomials and
  commitments computed by the adversary we write the latter using indices $0$
  and $1$ (two indices as we have two transcripts), e.g.~to describe the
  quotient polynomial provided by the adversary we write $\p{t}^0$ and $\p{t}^1$
  instead of $\p{t}$ as in the description of the protocol.

  \ngame{0} In this game, the adversary is given the SRS and wins if provides two
  transcripts that match on all $5$ messages send by the prover or finds a
  collision in the random oracle. Since such two transcripts cannot break the
  unique response property, the adversary wins this game with probability
  $\epsro$ tops.

  \ngame{1} This game is identical to Game $\game{0}$ except that now the
  adversary additionally wins if it provides two transcripts that matches on the first four
  messages of the proof.

  \ncase{Game 0 to Game 1} We show that the probability that $\adv$
  wins in one game but does not in the other is negligible.  Observe that in
  Round 5 of the proof, the adversary is given a challenge $v$ and has to open
  the previously computed commitments. Since the transcripts match up to Round
  4, the challenge is the same in both. Hence, to be able to give two different
  openings in Round 5, $\adv$ has to break the unique opening property of the
  KZG commitment scheme which happens with probability $\epsop$ tops.
  % Since
  % there are two commitments that the adversary opens, by the union bound
  % probability that $\adv$ wins in one game but not the other is upper-bounded
  % by
  % $2 \cdot \epsop$.

  \ngame{2} This game is identical to Game $\game{1}$ except that now the
  adversary additionally wins if it provides two transcripts that matches on the
  first three messages of the proof.

  \ncase{Game 0 to Game 1} In Round 4 of the protocol the adversary
  has to provide evaluations
  $a_\chz = \p{a}(\chz), b_\chz = \p{b}(\chz), c_\chz = \p{c}(\chz), t_\chz =
  \p{t}(\chz), S_{1, \chz} = \p{S_{\sigma 1}}(\chz), s_{2, \chz} = \p{S_{\sigma
      2}}(\chz), z_\chz = \p{z}(\chz \omega)$ of previously committed
  polynomials, and compute and evaluate a linearlization polynomial $\p{r}$.

  As before, the adversary cannot provide two different evaluations for the
  committed polynomials, since that would require breaking the evaluation
  binding property, which happens (by the union bound) with probability at most
  $7 \cdot (\epsbind + \infrac{2}{\abs{\FF_p}})$. The latter terms are since
  the adversary does not provide an opening for each of the commitment
  separately, but only in a batched way. That comes with $\infrac{1}{\FF_p}$ of
  security loss. Another $\infrac{1}{\FF_p}$ security loss comes from the fact
  that the verification of commitment openings are batched as well.

  The adversary cannot also provide two different linearization polynomials
  $\p{r^0}$ and $\p{r^1}$ evaluations $r^0_\chz$ and $r^1_\chz$ as the
  linearization polynomial is determined by values known to the verifier who
  also can compute a commitment to $\p{r}(X)$ equal $\gone{\p{r}(\chi)}$ by its
  own. The evaluation of $\p{r}$ provided by the adversary is later checked, as
  $\adv$ opens the commitment in Round 5. Hence, the probability that the
  adversary manages to build two convincing proofs that differ in evaluations
  $r_\chz$ and $r'_\chz$ is at most $\epsbind + \infrac{2}{\abs{\FF_p}}$.

  Hence, the probability that adversary wins in one game but does not in the
  other is upper-bounded by $8 \cdot (\epsbind + \infrac{2}{\FF_p})$

  \ngame{3} This game is identical to Game $\game{2}$ except that now the
  adversary additionally wins if it provides two transcripts that matches on the
  first two messages of the proof.

  \ncase{Game 2 to Game 3} In Round 3 the adversary computes the
  quotient polynomial $\pt(X)$ and provides its commitment that compounds of
  three separate commitments
  $\gone{\p{t_{lo}}(\chi), \p{t_{mid}}(\chi), \p{t_{hi}}(\chi)}$. Let
  $\gone{\p{t^0_{lo}}(\chi), \p{t^0_{mid}}(\chi), \p{t^0_{hi}}(\chi)}$ be the
  commitments output by the adversary in one transcript, and
  $\gone{\p{t^1_{lo}}(\chi), \p{t^1_{mid}}(\chi), \p{t^1_{hi}}(\chi)}$ the commitments
  provided in the other.
%
  Since the commitment scheme is deterministic, the adversary cannot come up
  with two different valid commitments for the same polynomial.

  If the adversary picks two different polynomials: $\p{t^0}(X)$, that is committed
  as $\gone{\p{t^0_{lo}}(\chi), \p{t^0_{mid}}(\chi), \p{t^0_{hi}}(\chi)}$, and
  $\p{t^1}(X)$ that is committed as
  $\gone{\p{t^1_{lo}}(\chi), \p{t^1_{mid}}(\chi), \p{t^1_{hi}}(\chi)}$, then one of
  them has to be computed incorrectly. 

  Importantly, polynomial $\p{t}(X)$ assures that the constraints of the system
  hold. Hence, the probability that one of $\p{t^0}(X)$, $\p{t^1}(X)$ is computed
  incorrectly, the adversary gives and opens acceptably a commitment to it, and
  the proof is acceptable, is upper bounded by the soundness of the proof system
  $\epss$. Alternatively, $\adv$ may compute a commitment to an invalid
  $\p{t^0}(X)$ (or $\p{t^1}(X)$) and later open the commitment at $\chz$ to
  $\p{t}(\chz)$. That is, give an evaluation from the correct polynomial
  $\p{t}(X)$. Since the commitment scheme is evaluation binding, probability of
  such event is upper bounded by $\epsbinding + \infrac{2}{\abs{\FF_p}}$.

  \ncase{Conclusion} Taking all the games together, probability that $\adv$ wins
  in $\game{3}$ is upper-bounded by
  \[
    2 \cdot \epsop + 9 \cdot (\epsbind + \infrac{2}{\FF_p}) + \epsro + \epss.
  \]
  \qed
\end{proof}

\subsection{Forking soundness}
\begin{lemma}
\label{lem:plonkprot_ss}
Let KZG be hiding with security $\epsh (\secpar)$, $\plonkprot$'s idealized
verifier fail with probability $\epsid (\secpar)$, and $(\noofc + 2, 1)$-$\dlog$
problem be $\epsdlog (\secpar)$ hard. Then $\plonkprot$ is
$(\epsid (\secpar) + \epsdlog (\secpar) + 2 \cdot \epsh(\secpar) +
\eps(\secpar), 3, 3 \noofc + 1)$-forking sound against algebraic adversary
$\adv$ who makes up to $S = \poly$ simulation oracle queries and for some
negligible $\eps(\secpar)$.
\end{lemma}

\begin{proof}
  The main idea of the proof is to show that an adversary who breaks forking
  soundness can be used to break hiding properties of the polynomial commitment
  scheme or a $\dlog$ problem instance. The proof goes by game hops. Let $\tree$
  be the tree produced by $\tdv$ by rewinding $\adv$. Note that since the tree
  branches after Round 3, the instance $\inp$, commitments
  $\gone{\p{a} (\chi), \p{b} (\chi), \p{c} (\chi), \p{z} (\chi), \p{t_{lo}}
    (\chi), \p{t_{mid}} (\chi), \p{t_{hi}} (\chi)}$, and challenges
  $\alpha, \beta, \gamma$ are the same. The tree branches after the third round
  of the protocol where the challenge $\chz$ is presented, thus tree $\tree$ is
  build using different values of $\chz$. 
%
  We consider the following games.

  \ncase{Game 0} In this game the adversary wins if
 % \begin{inparaenum}[(1)]
 % \item
  all the transcripts it produced are acceptable by the ideal verifier,
    i.e.~$\vereq_{\inp, \zkproof}(X) = 0$, cf.~\cref{eq:ver_eq}, and
    % \item
    none of commitments
    $\gone{\p{a} (\chi), \p{b} (\chi), \p{c} (\chi), \p{z} (\chi), \p{t_{lo}}
      (\chi), \p{t_{mid}} (\chi), \p{t_{hi}} (\chi)}$ use elements from a
    simulated proof, and
    % \item
    the extractor fails to extract a valid witness out of the proof.
  %\end{inparaenum}

  \ncase{Probability that $\adv$ wins Game 0 is negligible} Probability of
  $\adv$ winning this game is $\epsid(\secpar)$ as the protocol $\plonkprot$,
  instantiated with the idealised verification equation, is perfectly knowledge
  sound except with negligible probability of the idealised verifier failure
  $\epsid(\secpar)$. Hence for a valid proof $\zkproof$ for a statement $\inp$
  there exists a witness $\wit$, such that $\REL(\inp, \wit)$ holds. Note that
  since the $\tdv$ produces $(3 \noofc + 1)$ acceptable transcripts for
  different challenges $\chz$, it obtains the same number of different
  evaluations of polynomials
  $\p{a} (X), \p{b} (X), \p{c} (X), \p{z} (X), \p{t} (X)$. Since the transcripts
  are acceptable by an idealised verifier, the equality between polynomial
  $\p{t} (X)$ and combination of polynomials
  $\p{a} (X), \p{b} (X), \p{c} (X), \p{z} (X)$ described in Round 3 of the
  protocol holds. Hence, $\p{a} (X), \p{b} (X), \p{c} (X)$ encodes the valid
  witness for the proven statement. Since $\p{a} (X), \p{b} (X), \p{c} (X)$ are
  of degree at most $(\noofc + 2)$ and there is more than $(\noofc + 2)$ their
  evaluations known, $\extt$ can recreate polynomials' coefficients by interpolation
  and reveal the witness with probability $1$. Hence, the probability that
  extraction fails in that case is upper-bounded by probability of an idealised
  verifier failing $\epsid(\secpar)$, which is negligible.

  \ncase{Game 1} In this game the adversary additionally wins if
  %\begin{inparaenum}
  % \item
  it produces a transcript in $\tree$ such that
  $\vereq_{\inp, \zkproof}(\chi) = 0$, but $\vereq_{\inp, \zkproof}(X) \neq 0$,
  and
  % \item
  none of commitments
  $\gone{\p{a} (\chi), \p{b} (\chi), \p{c} (\chi), \p{z} (\chi), \p{t_{lo}}
    (\chi), \p{t_{mid}} (\chi), \p{t_{hi}} (\chi)}$ use elements from a
  simulated proof.
  % \end{inparaenum}
  The first condition means that the ideal verifier does not accept the proof,
  but the real verifier does.

  \ncase{Game 0 to Game 1} Assume the adversary wins in Game 1, but
  does not win in Game 0. We show that such adversary may be used to break the
  $\dlog$ assumption. More precisely, let $\tdv$ be an algorithm that for
  relation $\REL$ and randomly picked $\srs \sample \kgen(\REL)$ produces a tree
  of acceptable transcripts such that the winning condition of the game
  holds. Let $\rdvdlog$ be a reduction that gets as input an
  $(\noofc + 2, 1)$-dlog instance $\gone{1, \ldots, \chi^{\noofc}}, \gtwo{\chi}$
  and is tasked to output $\chi$.

  The reduction $\rdvdlog$ proceeds as follows.
  \begin{enumerate}
  \item Build $\plonkprot$'s SRS $\srs$ using the input $\dlog$ instance and
    start $\tdv(\adv, \srs)$;
  \item Let $(1, \tree)$ be the output returned by $\tdv$. Let $\inp$ be a
    relation proven in $\tree$.  Consider a transcript $\zkproof \in \tree$ such
    that $\vereq_{\inp, \zkproof}(X) \neq 0$, but
    $\vereq_{\inp, \zkproof}(\chi) = 0$. Since $\adv$ is algebraic, all group
    elements included in $\tree$ are extended by their representation as a
    combination of the input $\GRP_1$-elements. Hence, all coefficients of the
    verification equation polynomial $\vereq_{\inp, \zkproof}(X)$ are known.
  \item Find $\vereq_{\inp, \zkproof}(X)$ zero points and find $\chi$ among
    them.
  \item Return $\chi$.
  \end{enumerate}
  Hence, the probability that the adversary wins Game 1 is upper-bounded by
  $\epsdlog(\secpar)$.

  \iffalse
  \ncase{Game 0} In this game the adversary $\adv$ wins if $\tdv$ fails to
  output tree $\tree$ such that $\extt$ is able to extract a witness out of it
  and none of commitments
  $\gone{\p{a} (\chi), \p{b} (\chi), \p{c} (\chi), \p{z} (\chi), \p{t_{lo}}
    (\chi), \p{t_{mid}} (\chi), \p{t_{hi}} (\chi)}$ use elements from a
  simulated proof.

  \ncase{Probability that $\adv$ wins Game 0 is negligible}
  \michals{10.09}{Reduction to knowledge soundness}
  Let $\adv$ be considered forking soundness adversary. 
  Let $\adv$ be an adversary that breaks forking soundness.
  Note that if
  $\p{a}(X), \p{b} (X), \p{c} (X)$ contain witness at their coefficients, then 
  We use $\adv$ to
  build a reduction $\rdv$ that breaks soundness of $\plonkprotfs$. Let $\inp$
  be the instance the adversary output and $\zkproof$ its proof. The
  reduction proceeds as follows:
  \begin{enumerate}
  \item 
  \end{enumerate}
  \fi

  \ncase{Game 2} In this game the adversary additionally wins if at least one of
  the commitments $\p{a} (\chi), \p{b} (\chi), \p{c} (\chi), \p{z} (\chi)$
  utilizes a commitment that comes from a simulated proof; for example, $\adv$
  could compute its commitment to $\p{c} (X)$ as follows: it picks a polynomial
  $\p{p} (X)$, computes $\gone{\p{p} (\chi)}$, and outputs commitment
  $\gone{\p{c} (\chi)} = \gone{\p{p} (\chi)} + c$, where $c$ is a commitment
  output by a simulator. In the following, w.l.o.g, we assume that $\adv$ uses
  some simulated element to compute commitment $\gone{\p{c} (\chi)}$.

  \ncase{Game 1 to Game 2} Given adversary $\adv$ that wins in Game
  2, but not in Game 1, we show a reduction $\rdv$ that uses $\adv$ and $\tdv$
  to break hiding property \michals{9.9}{Define hiding property --
    w.r.t.~masking, adversary can get evaluation at number of points (here --
    1)} of the commitment scheme. $\rdv$ proceeds as follows:
  \begin{enumerate}
  \item Given polynomial commitment SRS $\srs_{\PCOM}$, produce $\plonk$'s SRS
    $\srs$.
  \item Pick random polynomials $\p{p} (X), \p{p'} (X) \in \FF^{< |\HHH|} [X]$,
    hiding parameter $k = 2$ and send them to the polynomial commitment
    challenger $\cdv$.
  \item From the challenger get the challenge commitment $c$.
  \item Let $S$ be the upper bound on the number of simulator oracles queries
    the adversary can make. \michals{9.9}{Note -- new bound!}
  \item Guess which simulator's response is going to be used by $\adv$ in its
    proof. Let $s$ be the index of this response.
  \item Guess which of the simulated polynomials in response $s$ will be
    used. Let $i$ be the index of this polynomial.
  \item Let $\tdv'$ be an algorithm that behaves exactly as $\tdv$, except when
    $\adv$ asks for $s$-th simulated proof, $\tdv'$'s internal procedure $\bdv'$
    provides $\adv$ with a simulated proof such that instead of randomly picked
    commitment $\p{c} (\chi)$ it gives $c$.
    \michals{9.9}{Alternatively, we can parametrize $\tdv$ by $\bdv$.}
  \item Start $\tdv'(\adv, \srs)$ and get the tree $\tree$.
  \item If indices $s$ or $i$ have not been guessed correctly, rewind $\adv$ to
    the beginning and pick new $s$ and $i$. Since $S = \poly$ probability that
    the correct $s$ will be guessed in polynomial time is overwhelming. That is,
    the reduction works in expected polynomial time. Similarly, $i$ takes values
    from $\range{1}{4}$, hence probability that $\rdv$ guesses $i$ in polynomial
    time is overwhelming. 
  \item Since $\tree$ contains $\noofc + 1$ evaluations of $\p{c} (X)$, the
    polynomial can be reconstructed. 
  \item Since $\adv$ is algebraic, $\rdv$ learns composition of $\p{c} (X)$ in
    the $\srs$ and simulated elements. 
  \item Hence $\rdv$ learns whether $c$ is a commitment to $\p{p} (X)$ or
    $\p{p'} (X)$.
  \item $\rdv$ returns its guessing bit to $\cdv$.
  \end{enumerate}

  \ncase{Game 3} In this game the adversary additionally wins if at least one of
  the commitments $\p{t_{lo}} (\chi), \p{t_{mid}} (\chi), \p{t_{hi}} (\chi)$
  comes from a simulated proof.

  \ncase{Game 2 to Game 3} Given adversary $\adv$ that wins in Game
  3, but not in Game 2, we show a reduction $\rdv$ that uses $\adv$ and $\tdv$
  to break hiding property \michals{9.9}{Define hiding property --
    w.r.t.~masking, adversary can get evaluation at number of points (here --
    1)} of the commitment scheme. $\rdv$ proceeds as follows:
  \begin{enumerate}
  \item Guess the simulation query index $s$ the polynomial(s) come from and
    whether the polynomial is $\p{t_{lo}} (X), \p{t_{mid}} (X)$, or
    $\p{t_{hi}} (X)$. Denote by $i \in \range{1}{3}$ the index of the guessed
    polynomial. W.l.o.g.~assume $i = 1$, i.e.~it is $\p{t_{lo}} (X)$.
  \item Produce two random polynomials $\p{p_0} (X)$ and $\p{p_1} (X)$ and
    send them to the challenger $\cdv$. Get commitment $c$.
  \item Let $\tdv'$ be an algorithm that behaves exactly as $\tdv$, except when
    $\adv$ asks for $s$-th simulated proof, $\tdv'$'s internal procedure $\bdv'$
    provides $\adv$ with a simulated proof such that:
    \begin{enumerate}
    \item Start making the simulated proof as a trapdoor-less simulator would.
    \item Before polynomials $\p{t_{lo}} (X), \p{t_{mid}}, \p{t_{hi}}$ are
      computed, pick random $\chz$ and get evaluation $p_\chz = \p{p_b} (\chz)$,
      i.e.~evaluate the polynomial in $c$ at $\chz$. \michals{9.9}{If we give
        hiding adversary possibility to evaluate polynomials, we need to use
        masking}
    \item Let $\ev{t_{lo}}$ be the evaluation of the simulated $\p{t_{lo}} (\chz)$.
    \item Pick $r$ such that $p_\chz + r = \p{t_{lo}} (\chz)$.
    \item For the commitment of $\p{t_{lo}}$ output $c' = c + \gone{r}$.
    \item Compute the rest of the simulated proof as a simulator would.
    \end{enumerate}
  \item Let $\tdv'$ be an algorithm that behaves exactly as $\tdv$, except when
    $\adv$ asks for $s$-th simulated proof, $\tdv'$'s internal procedure $\bdv'$
    provides $\adv$ with a simulated proof such that instead of a simulated
    $\p{t_{lo}} (\chi)$  it gives $c'$.
  \item Start $\tdv'(\adv, \srs)$ and get the tree $\tree$.
  \item If indices $s$ or $i$ have not been guessed correctly, rewind $\adv$ to
    the beginning and pick new $s$ and $i$. Since $S = \poly$ probability that
    the correct $s$ will be guessed in polynomial time is overwhelming. That is,
    the reduction works in expected polynomial time. Similarly, $i$ takes values
    from $\range{1}{3}$, hence probability that $\rdv$ guesses $i$ in polynomial
    time is overwhelming. 
  \item Since $\tree$ contains $\noofc + 1$ evaluations of $\p{t_{lo}} (X)$, the
    polynomial can be reconstructed. 
  \item Since $\adv$ is algebraic, $\rdv$ learns composition of $\p{t_{lo}} (X)$ in
    the $\srs$ and simulated elements. 
  \item Hence $\rdv$ learns whether $c$ is a commitment to $\p{p}$ or $\p{p'}$.
  \item $\rdv$ returns its guessing bit to $\cdv$.
  \end{enumerate}
  % \ncase{Conclusion}
  % Since probability 
\end{proof}

\iffalse
\begin{proof}
  Let $\srs$ be $\plonkprot$'s SRS and denote by $\srs_1$ all SRS's
  $\GRP_1$-elements; that is,
  $\srs_1 = \gone{1, \chi, \ldots, \chi^{\noofc + 2}}$. Let $\tdv$ be an
  algebraic adversary that produces a statement $\inp$ and a
  $(1, 1, 3\noofc + 1, 1)$-tree of acceptable transcripts $\tree$.  Note that in
  all transcripts the instance $\inp$, proof elements
  $\gone{\p{a}(\chi), \p{b}(\chi), \p{c}(\chi), \p{z}(\chi), \p{t}(\chi)}$ and
  challenges $\alpha, \beta, \gamma$ are common as the transcripts share the
  first three rounds. The tree branches after the third round of the protocol
  where the challenge $\chz$ is presented, thus tree $\tree$ is build using
  different values of $\chz$.

  We consider two games.

  \ncase{Game 0} In this game the adversary wins if all the transcripts it
  produced are acceptable by the ideal verifier,
  i.e.~$\vereq_{\inp, \zkproof}(X) = 0$, cf.~\cref{eq:ver_eq}, yet the extractor
  fails to extract a valid witness out of them.

  Probability of $\tdv$ winning this game is $\epsid(\secpar)$ as the protocol
  $\plonkprot$, instantiated with the idealised verification equation, is
  perfectly sound except with negligible probability of the idealised verifier
  failure $\epsid(\secpar)$. Hence for a valid proof $\zkproof$ for a statement
  $\inp$ there exists a witness $\wit$, such that $\REL(\inp, \wit)$ holds. Note
  that since the $\tdv$ produces $(3 \noofc + 1)$ acceptable transcripts for
  different challenges $\chz$, it obtains the same number of different
  evaluations of polynomials $\p{a}, \p{b}, \p{c}, \p{z}, \p{t}$. Since the
  transcripts are acceptable by an idealised verifier, the equality between
  polynomial $\p{t}$ and combination of polynomials $\p{a}, \p{b}, \p{c}, \p{z}$
  described in Round 3 of the protocol holds. Hence, $\p{a}, \p{b}, \p{c}$
  encodes the valid witness for the proven statement. Since
  $\p{a}, \p{b}, \p{c}$ are of degree at most $(\noofc + 2)$ and there is more
  than $(\noofc + 2)$ their evaluations known, $\extt$ can recreate their
  coefficients by interpolation and reveal the witness with probability
  $1$. Hence, the probability that extraction fails in that case is
  upper-bounded by probability of an idealised verifier failing
  $\epsid(\secpar)$, which is negligible.

  \ncase{Game 1} In this game the adversary additionally wins if it produces a
  transcript in $\tree$ such that $\vereq_{\inp, \zkproof}(\chi) = 0$, but
  $\vereq_{\inp, \zkproof}(X) \neq 0$. That is, the ideal verifier does not
  accept the proof, but the real verifier does.

  \ncase{Game 0 to Game 1} Assume the adversary wins in Game 1, but
  does not win in Game 0. We show that such adversary may be used to break the
  $\dlog$ assumption. More precisely, let $\tdv$ be an adversary that for
  relation $\REL$ and randomly picked $\srs \sample \kgen(\REL)$ produces a tree
  of acceptable transcripts such that the winning condition of the game
  holds. Let $\rdvdlog$ be a reduction that gets as input an
  $(\noofc + 2, 1)$-dlog instance $\gone{1, \ldots, \chi^{\noofc}}, \gtwo{\chi}$
  and is tasked to output $\chi$. The reduction proceeds as follows---it gives
  the input instance to the adversary as the SRS. Let $(1, \tree)$ be the output
  returned by $\adv$. Let $\inp$ be a relation proven in $\tree$.  Consider a
  transcript $\zkproof \in \tree$ such that $\vereq_{\inp, \zkproof}(X) \neq 0$,
  but $\vereq_{\inp, \zkproof}(\chi) = 0$. Since the adversary is algebraic, all
  group elements included in $\tree$ are extended by their representation as a
  combination of the input $\GRP_1$-elements. Hence all coefficients of the
  verification equation polynomial $\vereq_{\inp, \zkproof}(X)$ are known and
  $\rdvdlog$ can find its zero points. Since
  $\vereq_{\inp, \zkproof}(\chi) = 0$, the targeted discrete log value $\chi$ is
  among them.  Hence, the probability that this event happens is upper-bounded
  by $\epsdlog(\secpar)$. \qed
\end{proof}
\fi

\subsection{Honest verifier zero-knowledge}
\begin{lemma}
  \label{lem:plonk_hvzk}
  Let $\plonkprot$ be zero knowledge with security $\epszk(\secpar)$. Let
  $(\pR, \pS, \pT, \pf, 1)$-uber assumption for $\pR, \pS, \pT, \pf$ as defined
  in \cref{eq:uber} hold with security $\epsuber(\secpar)$. Then $\plonkprot$ is
  computationally honest verifier zero-knowledge with simulator $\simulator$
  that does not require a SRS trapdoor with security
  $\epszk(\secpar) + \epsuber(\secpar)$.\footnote{The simulator works as a simulator for proofs
    that are zero-knowledge in the standard model. However, we do not say that
    $\plonk$ is HVZK in the standard model as proof of that \emph{requires} the
    SRS simulator.}
\end{lemma}
\begin{proof}
  The proof goes by game-hopping. The environment that controls the games
  provides the adversary with a SRS $\srs$, then the adversary outputs an
  instance--witness pair $(\inp, \wit)$ and, depending on the game, is provided
  with either real or simulated proof for it. In the end of the game the
  adversary outputs either $0$ if it believes that the proof it saw was provided
  by the simulator and $1$ in the other case.

  \ngame{0} In this game $\adv(\srs)$ picks an instance--witness pair
  $(\inp, \wit)$ and gets a real proof $\zkproof$ for it.

  \ngame{1} In this game for $\adv(\srs)$ picks an instance--witness pair
  $(\inp, \wit)$ and gets a proof $\zkproof$ that is simulated by a simulator
  $\simulator_\chi$ which utilises for the simulation the SRS trapdoor and
  proceeds as described in \cref{sec:plonk_explained}.
  \COMMENT{follows. In the first round the simulator $\simulator_\chi$ picks
  randomisers $b_1, \ldots b_9$, sets $\wit_i = 0$, for $i \in \range{1}{3
    \noofc}$, computes polynomials $\pa(X), \pb(X), \pc(X)$ and
  outputs $\gone{\pa(\chi), \pb(\chi), \pc(\chi)}$. Then it picks Round 1
  challenges $\beta, \gamma$ honestly.

  In Round 2 $\simulator_\chi$ computes the polynomial $\pz(X)$ and
  outputs $\gone{\pz(\chi)}$. Then it picks randomly Round 2 challenge $\alpha$.

  In Round 3 the simulator computes polynomial $\pt(X)$ and evaluates it
  at $\chi$, then outputs $\gone{\ptlo(\chi), \ptmid(\chi), \pthi(\chi)}$. Note
  that this evaluation is feasible (in the polynomial time with non-negligible
  probability) only since $\simulator_\chi$ knows the trapdoor.

  In the last two rounds the simulator proceeds as an honest prover would
  proceed and picks corresponding challenges at random as an honest verifier
  would.
}

  \ncase{Game 0 to Game 1} Since $\plonk$ is zero-knowledge,
  probability that $\adv$ outputs a different bit in both games is negligible.
  Hence
  \(
	\abs{\prob{\game{0}} - \prob{\game{1}}} \leq \epszk(\secpar).
\)

\ngame{2} In this game $\adv(\srs)$ picks an instance--witness pair
$(\inp, \wit)$ and gets a proof $\zkproof$ simulated by the simulator
$\simulator$ which proceeds as follows.

In Round 1 the simulator  picks randomly both the randomisers $b_1, \ldots, b_6$ and
sets $\wit_i = 0$ for $i \in \range{1}{3\noofc}$. Then $\simulator$
outputs $\gone{\p{a}(\chi), \p{b}(\chi), \p{c}(\chi)}$. For the first round
challenge, the simulator picks permutation argument challenges $\beta, \gamma$
randomly.

In Round 2, the simulator computes $\p{z}(X)$ from
the newly picked randomisers $b_7, b_8, b_9$ and coefficients of polynomials
$\p{a}(X), \p{b}(X), \p{c}(X)$. Then it evaluates $\p{z}(X)$ honestly and outputs
$\gone{\p{z}(\chi)}$. Challenge $\alpha$ that should be sent by the verifier
after Round 2 is picked by the simulator at random.

In Round 3 the simulator starts by picking at random a challenge $\chz$, which
in the real proof comes as a challenge from the verifier sent \emph{after} Round
3. Then $\simulator$ computes evaluations
\(\p{a}(\chz), \p{b}(\chz), \p{c}(\chz), \p{S_{\sigma 1}}(\chz), \p{S_{\sigma
    2}}(\chz), \pubinppoly(\chz), \lag_1(\chz), \p{Z_H}(\chz),\allowbreak
\p{z}(\chz\omega)\) and computes $\p{t}(X)$ honestly. Since for a random
$\p{a}(X), \p{b}(X), \p{c}(X), \p{z}(X)$ the constraint system is (with
overwhelming probability) not satisfied and the constraints-related polynomials
are not divisible by $\p{Z_H}(X)$, hence $\p{t}(X)$ is a rational function
rather than a polynomial. Then, the simulator evaluates $\p{t}(X)$ at $\chz$ and
picks randomly a degree-$(3 \noofc - 1)$ polynomial $\p{\tilde{t}}(X)$ such that
$\p{t}(\chz) = \p{\tilde{t}}(\chz)$ and publishes a commitment
$\gone{\p{\tilde{t}_{lo}}(\chi), \p{\tilde{t}_{mid}}(\chi),
  \p{\tilde{t}_{hi}}(\chi)}$. After this round the simulator outputs $\chz$ as a
challenge.

In the next round, the simulator computes polynomial $\p{r}(X)$ as an honest
prover would, cf.~\cref{sec:plonk_explained} and evaluates $\p{r}(X)$ at $\chz$.

The rest of the evaluations are already computed, thus $\simulator$ simply
outputs
\(
  \p{a}(\chz), \p{b}(\chz), \p{c}(\chz), \p{S_{\sigma 1}}(\chz), \p{S_{\sigma
      2}}(\chz), \p{t}(\chz), \p{z}(\chz \omega)\,.
\)
After that it picks randomly the challenge $v$, proceeds in the last round as an
honest prover would proceed and outputs the final challenge, $u$, by picking it
at random as well.

\ncase{Game 1 to Game 2} We now describe the reduction $\rdv$ which
relies on the $(\pR, \pS, \pT, \pF, 1)$-uber assumption, cf.~\cref{sec:uber_assumption}
where $\pR, \pS, \pT, \pF$ are polynomials over variables
$\vB = B_1, \ldots, B_9$ and are defined as follows. Let
$E = \smallset{\smallset{2}, \smallset{3, 4}, \smallset{5, 6}, \smallset{7, 8,
    9}}$ and $E' = E \setminus \smallset{2}$. Let
\begin{align}
\label{eq:uber}
\pF(\vB) & = \smallset{B_1} \cup \smallset{B_1B_i \mid i \in A,\ A \in E'} \cup
             \smallset{B_1B_iB_j \mid i \in A, j \in B,\ A, B \in E', B
             \neq A} \cup \notag\\
           & \smallset{B_1B_iB_jB_k \mid i \in A, j \in
             B, k \in C,\ A, B, C \in E', A \neq B \neq C \neq A}\notag\,,\\
  \pR(\vB) & = \smallset{B_i \mid i \in A,\ A \in E} \cup \smallset{B_i B_j \mid i \in
             A, j \in B,\ A \neq B, A, B \in E} \cup \\ 
           & \smallset{B_i B_j B_k \mid i \in A,\ j \in
             B,\ k \in C,\
             A, B, C \text{ all different and in } E} \cup \notag \\
           & \smallset{B_i B_j B_k B_l \mid i \in A,\ j \in B,\ k \in C,\ l \in D,\
             A, B, C, D \text{ all different and in } E} \notag \\
           & \setminus \pF(\vB)\,,\notag \\
  \pS(\vB) & = \emptyset, \qquad \pT(\vB) = \emptyset.
\end{align}
That is, the elements of $\pR$ are all singletons, pairs, triplets and
quadruplets of $B_i$ variables that occur in polynomial $\pt(\vB)$ except the
challenge element $\pf(\vB)$ which are all elements that depends on a variable
$B_1$. Variables $\vB$ are evaluated to randomly picked
$\vb = b_1, \ldots, b_9$.

The reduction $\rdv$ learns $\gone{\pR}$ and challenge
$\gone{\vec{w}} = \gone{w_1, \ldots, w_{12}}$ where $\vec{w}$ is either a vector
of evaluations $\pF(\vb)$ or a sequence of random values $y_1, \ldots, y_{12}$,
for the sake of concreteness we state $w_1 = b_1$ or $w_1 = y_1$ (depending on
the chosen random bit). Then it picks $\chi$, $\chz$ and computes the SRS $\srs$
from $\chi$. Elements $b_i$ are interpreted as polynomials in $X$ that are
evaluated at $\chi$, i.e. $b_i = b_i(\chi)$. Next, $\rdv$ sets for
$\xi_i, \zeta_i \sample \FF_p$
\(
  \gone{\p{\tb}_1(X)} =
(X - \chz)(X - \ochz) \gone{w_1}(X) + \xi_i (X - \chz) \gone{1} +
\zeta_i (X - \ochz) \gone{1}, % \text{ for } i \in % \range{1}{9}, u_1
\),
and
\(
  \gone{\p{\tb}_i(X)} =
(X - \chz)(X - \ochz) \gone{b_i}(X) + \xi_i (X - \chz) \gone{1} +
\zeta_i (X - \ochz) \gone{1}, % \text{ for } i \in % \range{1}{9}, u_1
\) 
for $i \in \range{2}{9}$.

Denote by $\tb_i$ evaluations of $\p{\tb}_i$ at $\chi$.  The reduction computes
all
$\gone{\tb_i \tb_j}, \gone{\tb_i \tb_j \tb_k}, \gone{\tb_i \tb_j \tb_k \tb_l}$
such that $\gone{B_i B_j, B_i B_j B_k, B_i B_j B_k B_l} \in \pR$.  This is
possible since $\rdv$ knows all singletons $\gone{w_1, b_2, \ldots, b_9}$ and pairs
$\gone{b_i b_j} \in \pR$ which can be used to compute all required pairs
$\gone{\tb_i \tb_j}$:
\begin{align*}
\gone{\tb_i \tb_j} 
& = ((\chi - \chz)(\chi - \ochz)\gone{b_i} + \xi_i (\chi - \chz)\gone{1} +
\zeta_i (\chi - \ochz) \gone{1}) 
\cdot \\
 & ((\chi - \chz)(\chi - \ochz)\gone{b_j} + \xi_j (\chi - \chz)\gone{1} +
\zeta_j (\chi - \ochz) \gone{1}) = \\
 & ((\chi - \chz)(\chi - \ochz))^2 \gone{b_i b_j} +  ((\chi - \chz)(\chi -
 \ochz)\gone{b_i} (\xi_j (\chi - \chz) \gone{1} + \zeta_j (\chi - \ochz)
 \gone{1}) + \\
 & ((\chi - \chz)(\chi -
 \ochz)\gone{b_j} (\xi_i (\chi - \chz) \gone{1} + \zeta_i (\chi - \ochz)
 \gone{1}) + \psi,
\end{align*}
where $\psi$ compounds of $\xi_i, \xi_j, \zeta_i, \zeta_j, \chz, \ochz, \chi$ which
are all known by $\rdv$ and no $b_i$ nor $b_j$.
Analogously for the triplets and quadruplets and elements dependent on~$\vec{w}$. 

Next the reduction runs the adversary $\adv(\srs)$ and obtains from $\adv$ an
instance--witness pair $(\inp, \wit)$.  $\rdv$ now prepares a simulated proof as follows:
\begin{compactdesc} 
\item[Round 1] $\rdv$ computes $\gone{\pa(\chi)}$ using as
randomisers $\gone{\tb_1}, \gone{\tb_2}$ and setting $\wit_i = 0$, for $i
\in \range{1}{3 \noofc}$. Similarly it computes
$\gone{\pb(\chi)}, \gone{\pc(\chi)}$.  $\rdv$ publishes the obtained values
and picks a Round 1 challenge $\beta, \gamma$ at random.  Note that regardless
$w_1 = b_1$ or a random element, $\gone{a(\chi)}$ is random. Thus $\rdv$'s
output has the same distribution as output of a real prover.  
\item[Round 2]
$\rdv$ computes $\gone{\pz(\chi)}$ using $\tb_7, \tb_8, \tb_9$ and publishes
it. Then it picks randomly the challenge $\alpha$. This round output is
independent on $b_1$ thus $\rdv$'s output is indistinguishable from the prover's. 
\item[Round 3] The reduction computes
  $\p{t_{lo}}(\chi), \p{t_{mid}}(\chi), \p{t_{hi}}(\chi)$, which all depend on
  $b_1$. To that end $\gone{\tb_1}$ is used. Note that if $\vec{w}$ is a vector
  of $\pF(b_1, \ldots, b_9)$ evaluations then
  $\gone{\p{t_{lo}}(\chi), \p{t_{mid}}(\chi), \p{t_{hi}}(\chi)}$ is the same as
  the real prover's. Alternatively, if $\vec{w}$ is a vector of random values,
  then $\p{t_{lo}}(\chi), \p{t_{mid}}(\chi), \p{t_{hi}}(\chi)$ are all random
  polynomials which evaluates at $\chz$ to the same value as the polynomials
  computed by the real prover. That is, in that case
  $\p{t_{lo}}(\chi), \p{t_{mid}}(\chi), \p{t_{hi}}(\chi)$ are as the simulator
  $\simulator$ would compute. Eventually, $\rdv$ outputs $\chz$.
\item[Round 4] The reduction outputs
  $\pa(\chz), \pb(\chz), \pc(\chz), \p{S_{\sigma 1}}(\chz), \p{S_{\sigma 2}
    (\chz)}, \pt(\chz), \pz(\ochz)$.  For the sake of concreteness, denote by
  $S = \smallset{\pa, \pb, \pc, \pt, \pz}$. Although for a polynomial
  $\p{p} \in S$, reduction $\rdv$ does not know $\p{p}(\chi)$ or even do not
  know all the coefficients of $\p{p}$, the polynomials in $S$ was computed such
  that the reduction always knows their evaluation at $\chz$ and $\ochz$.
\item[Round 5] $\rdv$ computes the openings of the polynomial commitments
assuring that evaluations at $\chz$ it provided were computed honestly.
\end{compactdesc}

If the adversary $\adv$'s output distribution differ in Game $\game{1}$ and
$\game{2}$ then the reduction uses it to distinguish between
$\vec{w} = \pF(b_1, \ldots, b_9)$ and $\vec{w}$ being random, thus
\( \abs{\prob{\game{1}} - \prob{\game{2}}} \leq \epsuber(\secpar).  \) Eventually,
\(
\abs{\prob{\game{0}} - \prob{\game{2}}} \leq \epszk(\secpar) + \epsuber(\secpar).  \) \qed
\end{proof}

\subsection{Simulation soundness and simulation
  extractability of~$\plonkprotfs$}
Since \cref{lem:plonkprot_ur,lem:plonkprot_ss} hold, $\plonkprot$ is $\ur{2}$
and forking sound. We now make use of \cref{thm:simsnd} and \cref{thm:se} and show that
$\plonkprot_\fs$ is simulation sound and forking simulation-extractable as defined in
\cref{sec:simext_def}.

\begin{corollary}[Forking simulation extractability of $\plonkprot_\fs$]
\label{thm:plonkprotfs_se}
Assume an idealised $\plonkprot$ verifier fails at most with probability
$\epsid(\secpar)$, the discrete logarithm advantage is bounded by
$\epsdlog(\secpar)$ and the $\PCOMp$ is a commitment of knowledge with security
$\epsk(\secpar)$, binding security $\epsbind(\secpar)$ and has unique opening
property with security $\epsop(\secpar)$. Let
$\ro\colon \bin^* \to \bin^\secpar$ be a random oracle. Let $\advse$ be an
algebraic adversary that can make up to $q$ random oracle queries, up to $S$
simulation oracle queries, and outputs an acceptable proof for $\plonkprotfs$
with probability at least $\accProb$. Then $\plonkprotfs$ is forking
simulation-extractable with extraction error $\eta = \epsur(\secpar)$. The
extraction probability $\extProb$ is at least
\[
	\extProb \geq \frac{1}{q^{3 (\epsid(\secpar)+\epsdlog(\secpar))}} (\accProb - \epsk(\secpar) - 2\cdot\epsbind(\secpar) -
  \epsop(\secpar))^{3\noofc + 1} -\eps(\secpar)\,,
\]
for some negligible $\eps(\secpar)$ and $\noofc$ being the number of
constrains in the proven circuit.
\end{corollary}

\begin{corollary}[Simulation soundness of $\plonkprot_\fs$]
  \label{thm:simsnd}
  Assume that $\plonkprot$ is $2$-programmable HVZK in the standard model, that
  is $\epss(\secpar)$-sound and the $\PCOMp$ is a commitment of knowledge with
  security $\epsk(\secpar)$, binding security $\epsbind(\secpar)$ and has unique
  opening property with security $\epsop(\secpar)$. Then the probability that a
  $\ppt$ adversary $\adv$ breaks simulation soundness of $\ps_{\fs}$ is
  upper-bounded by
  \( \epsk(\secpar) + 2\cdot\epsbind(\secpar) + \epsop(\secpar) + q_\ro^4
  \epss(\secpar)\,, \) where $q$ is the total number of queries made by the
  adversary $\adv$ to a random oracle $\ro\colon \bin^{*} \to \bin^{\secpar}$.
\end{corollary}

\section{Non-malleability of $\sonicprotfs$}
\label{sec:sonic}
\subsection{\sonic{} protocol rolled out}
In this section we present $\sonic$'s constraint system and algorithms. Reader
familiar with them may jump directly to the next section.

\oursubsub{The constraint system}
\label{sec:sonic_constraint_system}
\sonic's system of constraints composes of three $\multconstr$-long vectors
$\va, \vb, \vc$ which corresponds to left and right inputs to multiplication
gates and their outputs. It hence holds $\va \cdot \vb = \vc$.

There is also $\linconstr$ linear constrains of the form
\[
  \va \vec{u_q} + \vb \vec{v_q} + \vc \vec{w_q} = k_q,
\]
where $\vec{u_q}, \vec{v_q}, \vec{w_q}$ are vectors for the $q$-th linear
constraint with instance value $k_q \in \FF_p$. Furthermore define polynomials
\begin{equation}
  \begin{split}
    \p{u_i}(Y) & = \sum_{q = 1}^\linconstr Y^{q + \multconstr} u_{q, i}\,,\\
    \p{v_i}(Y) & = \sum_{q = 1}^\linconstr Y^{q + \multconstr} v_{q, i}\,,\\
  \end{split}
  \qquad
  \begin{split}
    \p{w_i}(Y) & = -Y^i - Y^{-i} + \sum_{q = 1}^\linconstr Y^{q +
      \multconstr} w_{q, i}\,,\\
    \p{k}(Y) & = \sum_{q = 1}^\linconstr Y^{q + \multconstr} k_{q}.
  \end{split}
\end{equation}

$\sonic$ constraint system requires that
\begin{align}
  \label{eq:sonic_constraint}
  \vec{a}^\top \cdot \vec{\p{u}} (Y) + \vec{b}^\top \cdot \vec{\p{v}} (Y) +
  \vec{c}^\top \cdot \vec{\p{w}} (Y) + \sum_{i = 1}^{\multconstr} a_i b_i (Y^i +
  Y^{-i}) - \p{k} (Y) = 0.
\end{align}

In \sonic{} we will use commitments to the following polynomials.
\begin{align*}
  \pr(X, Y) & = \sum_{i = 1}^{\multconstr} \left(a_i X^i Y^i + b_i X^{-i} Y^{-i}
              + c_i X^{-i - \multconstr} Y^{-i - \multconstr}\right) \\
  \p{s}(X, Y) & = \sum_{i = 1}^{\multconstr} \left( u_i (Y) X^{-i} +
                v_i(Y) X^i + w_i(Y) X^{i + \multconstr}\right)\\
  \pt(X, Y) & = \pr(X, 1) (\pr(X, Y) + \p{s}(X, Y)) - \p{k}(Y)\,.
\end{align*}

Polynomials $\p{r} (X, Y), \p{s} (X, Y), \p{t} (X, Y)$ are designed such that
$\p{t} (0, Y) = \vec{a}^\top \cdot \vec{\p{u}} (Y) + \vec{b}^\top \cdot
\vec{\p{v}} (Y) + \vec{c}^\top \cdot \vec{\p{w}} (Y) + \sum_{i =
  1}^{\multconstr} a_i b_i (Y^i + Y^{-i}) - \p{k} (Y) $. That is, the prover is
asked to show that $\p{t} (0, Y) = 0$, cf.~\cref{eq:sonic_constraint}.

Furthermore, the commitment system in $\sonic$ is designed such that it is
infeasible for a $\ppt$ algorithm to commit to a polynomial with non-zero
constant term.

\oursubsub{Algorithms rolled out}
\ourpar{$\sonic$ SRS generation $\kgen(\REL)$.} The SRS generating algorithm picks
randomly $\alpha, \chi \sample \FF_p$ and outputs
	\[
      \srs = \left( \gone{\smallset{\chi^i}_{i = -\dconst}^{\dconst},
          \smallset{\alpha \chi^i}_{i = -\dconst, i \neq 0}^{\dconst}},
        \gtwo{\smallset{\chi^i, \alpha \chi^i}_{i = - \dconst}^{\dconst}},
        \gtar{\alpha} \right)
	\]
\ourpar{$\sonic$ prover $\prover(\srs, \inp, \wit=\va, \vb, \vc)$.}
\begin{description}
\item[Round 1] The prover picks randomly randomisers
  $c_{\multconstr + 1}, c_{\multconstr + 2}, c_{\multconstr + 3}, c_{\multconstr
    + 4} \sample \FF_p$. Sets
  $\pr(X, Y) \gets \pr(X, Y) + \sum_{i = 1}^4 c_{\multconstr + i} X^{- 2
    \multconstr - i}$. Commits to $\pr(X, 1)$ and outputs
  $\gone{r} \gets \com(\srs, \multconstr, \pr(X, 1))$.  Then it gets challenge $y$ from
  the verifier.
\item[Round 2] $\prover$ commits to $\pt(X, y)$ and outputs
  $\gone{t} \gets \com(\srs, \dconst, \pt(X, y))$. Then it gets a challenge $z$ from
  the verifier.
\item[Round 3] The prover computes commitment openings. That is, it outputs
  \begin{align*}
    \gone{o_a} & = \open(\srs, z, \pr(z, 1), \pr(X, 1)) \\
    \gone{o_b} & = \open(\srs, yz, \pr(yz, 1), \pr(X, 1)) \\
    \gone{o_t} & = \open(\srs, z, \pt(z, y), \pt(X, y)) 
  \end{align*}
  along with evaluations $a' = \pr(z, 1), b' = \pr(y, z), t' = \pt(z, y)$.  Then it
  engages in the signature of correct computation playing the role of the
  helper, i.e.~it commits to $\p{s}(X, y)$ and sends the commitment $\gone{s}$, commitment opening
  \begin{align*}
    \gone{o_s} & = \open(\srs, z, \p{s}(z, y), \p{s}(X, y)), \\
  \end{align*} and $s'=\p{s}(z, y)$. 
%
  Then
  it obtains a challenge $u$ from the verifier.
\item[Round 4] In the next round the prover computes
  $\gone{c} \gets \com(\srs, \dconst, \p{s}(u, Y))$ and
  computes commitments' openings
  \begin{align*}
    \gone{w} & = \open(\srs, u, \p{s}(u, y), \p{s}(X, y)), \\
    \gone{q_y} & = \open(\srs, y,\p{s}(u, y), \p{s}(u, Y)),
  \end{align*}
  and returns $\gone{w}, \gone{q_y}, s = \p{s}(u, y)$. Eventually the prover gets the last challenge
  from the verifier---$z'$.
\item[Round 5] In the final round, $\prover$ computes opening
  $\gone{q_{z'}} = \open(\srs, z', \p{s}(u, z'), \p{s}(u, X))$ and outputs $\gone{q_{z'}}$.
\end{description}

\ourpar{$\sonic$ verifier $\verifier(\srs, \inp, \zkproof)$.} The verifier
in \sonic{} runs as subroutines the verifier for the polynomial commitment. That
is it sets $t' = a'(b' + s') - \p{k}(y)$ and checks the following:
\begin{equation*}
  \begin{split}
    &\PCOMs.\verifier(\srs, \multconstr, \gone{r}, z, a', \gone{o_a}), \\
    &\PCOMs.\verifier(\srs, \multconstr, \gone{r}, yz, b', \gone{o_b}),\\
    &\PCOMs.\verifier(\srs, \dconst, \gone{t}, z, t', \gone{o_t}),\\
    &\PCOMs.\verifier(\srs, \dconst, \gone{s}, z, s', \gone{o_s}),\\
  \end{split}
  \qquad
  \begin{split}
    &\PCOMs.\verifier(\srs, \dconst, \gone{s}, u, s, \gone{w}),\\
    &\PCOMs.\verifier(\srs, \dconst, \gone{c}, y, s, \gone{q_y}),\\
    &\PCOMs.\verifier(\srs, \dconst, \gone{c}, z', \p{s}(u, z'), \gone{q_{z'}}),
  \end{split}
\end{equation*}
and accepts the proof iff all the checks holds. Note that the value
$\p{s}(u, z')$ that is recomputed by the verifier uses separate challenges $u$
and $z'$. This enables the batching of many proof and outsourcing of this
part of the proof to an untrusted helper.

\subsection{Unique opening property of $\PCOMs$}
\begin{lemma}
\label{lem:pcoms_unique_op}
$\PCOMs$ has the unique opening property in the AGM. 
\end{lemma}
\begin{proof}
Let 
$z \in \FF_p$ be the attribute the polynomial is evaluated at,
$\gone{c} \in \GRP$ be the commitment,  
$s \in \FF_p$ the evaluation value, and 
$o \in \GRP$ be the commitment opening. 
We need to show that for every $\ppt$ adversary $\adv$ probability
\[
  \Pr \left[
    \begin{aligned}
      & \verify(\srs, \gone{c}, z, s, \gone{o}) = 1, \\
      & \verify(\srs, \gone{c}, z, \tilde{s}, \gone{\tilde{o}}) = 1
    \end{aligned}
    \,\left|\, \vphantom{\begin{aligned}
          & \verify(\srs, \gone{c}, z, s, \gone{o}),\\
          & \verify(\srs, \gone{c}, z, \tilde{s}, \gone{\tilde{o}}) \\
          &o \neq \tilde{o})
		\end{aligned}}
      \begin{aligned}
        & \srs \gets \kgen(\secparam, \maxdeg), \\
        & (\gone{c}, z, s, \tilde{s}, \gone{o}, \gone{\tilde{o}}) \gets \adv(\srs)
      \end{aligned}
    \right.\right]
  % \leq \negl.
\]
is at most negligible.

As noted in \cite[Lemma 2.2]{EPRINT:GabWilCio19} it is enough to upper bound the
probability of the adversary succeeding using the idealised verification
equation---which considers equality between polynomials---instead of the real
verification equation---which considers equality of the polynomials' evaluations.

For a polynomial $f$, its degree upper bound $\maxconst$, evaluation point $z$,
evaluation result $s$, and opening $\gone{o(X)}$ the idealised check verifies that
\begin{equation}
  \alpha (X^{\dconst - \maxconst}f(X) \cdot X^{-\dconst + \maxconst} -  s) \equiv \alpha \cdot o(X) (X - z)\,,
\end{equation}
what is equivalent to 
\begin{equation}
	f(X) -  s \equiv o(X) (X - z)\,.
	\label{eq:pcoms_idealised_check}
\end{equation}
Since $o(X)(X - z) \in \FF_p[X]$ then from the uniqueness of polynomial
composition, there is only one $o(X)$ that fulfils the equation above.
\qed
\end{proof}


\subsection{Unique response property}
The unique response property of $\sonicprot$ follows from the unique opening
property of the used polynomial commitment scheme $\PCOMs$.
\begin{lemma}
\label{lem:sonicprot_ur}
If a polynomial commitment scheme $\PCOMs$ is evaluation binding with
parameter $\epsbind(\secpar)$ and has unique openings property with parameter
$\epsop(\secpar)$, then $\sonicprot$ is $\ur{1}$ with parameter $\epsur(\secpar) \leq
\epsbind(\secpar) + \epsop(\secpar)$.  
\end{lemma}
\begin{proof}
  Let $\adv$ be an adversary that breaks $\ur{1}$-ness of $\sonicprot$.  We
  consider two cases, depending on which round $\adv$ is able to provide at
  least two different outputs such that the resulting transcripts are
  acceptable.  For the first case we show that $\adv$ can be used to break the
  evaluation binding property of $\PCOMs$, while for the second case we show
  that it can be used to break the unique opening property of $\PCOMs$.

  The proof goes similarly to the proof of \cref{lem:plonkprot_ur} thus we
  provide only draft of it here.  In each Round $i$, for $i > 1$, the prover
  either commits to some well-defined polynomials (deterministically), evaluates
  these on randomly picked points, or shows that the evaluations were performed
  correctly.  Obviously, for a committed polynomial $\p{p}$ evaluated at point
  $x$ only one value $y = \p{p}(x)$ is correct. If the adversary was able to
  provide two different values $y$ and $\tilde{y}$ that would be accepted as an
  evaluation of $\p{p}$ at $x$ then the $\PCOMs$'s evaluation binding would be
  broken.  Alternatively, if $\adv$ was able to provide two openings $\p{W}$ and
  $\p{\tilde{W}}$ for $y = \p{p}(x)$ then the unique opening property would be
  broken.
%
Hence the probability that $\adv$ breaks $\ur{1}$-property of $\PCOMs$ is
upper-bounded by $\epsbind(\secpar) + \epsop(\secpar)$. 
\qed

\end{proof}

\subsection{Forking soundness}
\begin{lemma}
	\label{lem:sonicprot_ss}
  $\sonicprot$ is $(\epsss(\secpar), 2, \multconstr + \linconstr + 1)$-forking sound against
  algebraic adversaries who can make up to $S$ simulation oracle queries, with
  \[
			\epss(\secpar) \leq \epsid(\secpar) + \epsldlog(\secpar) + 2\cdot \epsh(\secpar)\,,
	\]
	where $\epsid(\secpar)$ is a soundness error of the idealized verifier, $\epsldlog(\secpar)$ is
  security of $(\dconst, \dconst)$-$\ldlog$ assumption, and $\epsh (\secpar)$ is
  a security of hiding of the polynomial commitment scheme.
\end{lemma}
\begin{proof}
  Similarly as in the case of $\plonk$, the main idea of the proof is to show
  that an adversary who breaks forking soundness can be used to break hiding
  properties of the polynomial commitment scheme or a $\dlog$ problem
  instance. The proof goes by game hops. Let $\tree$ be the tree produced by
  $\tdv$ by rewinding $\adv$. Note that since the tree branches after Round 2,
  the instance $\inp$, commitments
  $\gone{\p{r} (\chi, 1), \p{r} (\chi, y), \p{s} (\chi, y), \p{t} (\chi, y)}$, and challenge
  $y$ are the same. The tree branches after the second round
  of the protocol where the challenge $z$ is presented, thus tree $\tree$ is
  build using different values of $z$.
%
  We consider the following games.

  \ncase{Game 0} In this game the adversary wins if all the transcripts it
  produced are acceptable by the ideal verifier,
  i.e.~$\vereq_{\inp, \zkproof}(X) = 0$, cf.~\cref{eq:ver_eq}, and none of
  commitments
  $\gone{\p{r} (\chi, 1), \p{r} (\chi, y), \p{s} (\chi, y), \p{t} (\chi, y)}$ use
    elements from a simulated proof, and the extractor fails to extract a valid
    witness out of the proof.

    \ncase{Probability that $\adv$ wins Game 0 is negligible} Probability of
    $\adv$ winning this game is $\epsid(\secpar)$ as the protocol $\sonicprot$,
    instantiated with the idealised verification equation, is perfectly
    knowledge sound except with negligible probability of the idealised verifier
    failure $\epsid(\secpar)$. Hence for a valid proof $\zkproof$ for a
    statement $\inp$ there exists a witness $\wit$, such that $\REL(\inp, \wit)$
    holds. Note that since the $\tdv$ produces $(\multconstr + \linconstr + 1)$
    acceptable transcripts for different challenges $z$. As noted in
    \cite{CCS:MBKM19} this assures that the correct witness is encoded in
    $\p{r} (X, Y)$. Hence $\extt$ can recreate polynomials' coefficients by
    interpolation and reveal the witness with probability $1$. Moreover, the
    probability that extraction fails in that case is upper-bounded by
    probability of an idealised verifier failing $\epsid(\secpar)$, which is
    negligible.

    \ncase{Game 1} In this game the adversary additionally wins if it produces a
    transcript in $\tree$ such that $\vereq_{\inp, \zkproof}(\chi) = 0$, but
    $\vereq_{\inp, \zkproof}(X) \neq 0$, and none of commitments
    $\gone{\p{r} (\chi, 1), \p{r} (\chi, y), \p{s} (\chi, y), \p{t} (\chi, y)}$
      use elements from a simulated proof.  The first condition means that the
      ideal verifier does not accept the proof, but the real verifier does.

      \ncase{Game 0 to Game 1} Assume the adversary wins in Game 1, but does not
      win in Game 0. We show that such adversary may be used to break an
      instance of a $\ldlog$ assumption. More precisely, let $\tdv$ be an
      algorithm that for relation $\REL$ and randomly picked
      $\srs \sample \kgen(\REL)$ produces a tree of acceptable transcripts such
      that the winning condition of the game holds. Let $\rdvdlog$ be a
      reduction that gets as input an
      $(\dconst, \dconst)$-ldlog instance
      $\gone{\chi^{-\dconst}, \ldots, \chi^{\dconst}}, \gtwo{\chi^{-\dconst},
        \ldots, \chi^{\dconst}}$ and is tasked to output $\chi$.

      The reduction $\rdvdlog$ proceeds as follows.
  \begin{enumerate}
  \item Build $\sonicprot$'s SRS $\srs$: pick a random $\alpha$ and compute
    $\gone{\alpha \chi^{- \dconst}, \ldots, \alpha \chi^{-1}, \alpha \chi,
      \ldots, \alpha \chi^{\dconst}}$,
    $\gtwo{\alpha \chi^{- \dconst}, \ldots, \alpha \chi^{-1}, \alpha \chi,
      \ldots, \alpha \chi^{\dconst}}$. Compose the SRS.
  \item Let $(1, \tree)$ be the output returned by $\tdv$. Let $\inp$ be a
    relation proven in $\tree$.  Consider a transcript $\zkproof \in \tree$ such
    that $\vereq_{\inp, \zkproof}(X) \neq 0$, but
    $\vereq_{\inp, \zkproof}(\chi) = 0$. Since $\adv$ is algebraic, all group
    elements included in $\tree$ are extended by their representation as a
    combination of the input $\GRP_1$-elements. Hence, all coefficients of the
    verification equation polynomial $\vereq_{\inp, \zkproof}(X)$ are known.
  \item Find $\vereq_{\inp, \zkproof}(X)$ zero points and find $\chi$ among
    them.
  \item Return $\chi$.
  \end{enumerate}
  Hence, the probability that the adversary wins Game 1 is upper-bounded by
  $\epsldlog(\secpar)$.

  \ncase{Game 2} In this game the adversary additionally wins if at least one of
  the commitments $\gone{\p{r} (\chi, 1), \p{r} (\chi, y), \p{s} (\chi, y)}$
  %$\gone{\chi^{-\alpha \dconst}, \ldots, \chi^{\alpha \dconst}} \setminus
 % \smallset{\gone{\chi^{\alpha}}}$
  utilizes a commitment that comes from a simulated proof; for example, $\adv$
  could compute its commitment to $\p{r} (X, 1)$ as follows: it picks a
  polynomial $\p{p} (X, Y)$, computes $\gone{\p{p} (\chi, 1)}$, and outputs
  commitment $\gone{\p{c} (\chi, 1)} = \gone{\p{p} (\chi, 1)} + c$, where $c$ is
  a commitment output by a simulator. In the following, w.l.o.g, we assume that
  $\adv$ uses some simulated element to compute commitment
  $\gone{\p{r} (\chi, 1)}$.

  \ncase{Game 1 to Game 2} Given adversary $\adv$ that wins in Game 2, but not
  in Game 1, we show a reduction $\rdv$ that uses $\adv$ and $\tdv$ to break
  hiding property of the commitment scheme. $\rdv$ proceeds as follows:
  \begin{enumerate}
  \item Given polynomial commitment SRS $\srs_{\PCOMs}$, produce $\sonic$'s SRS
    $\srs$ similarly to Game 1. 
  \item Pick random polynomials
    $\p{p} (X, Y), \p{p'} (X, Y) \in \FF^{< |\dconst|} (X)$, hiding parameter
    $k = 2$ and send them to the polynomial commitment challenger $\cdv$.
  \item From the challenger get the challenge commitment $c$.
  \item Let $S$ be the upper bound on the number of simulator oracles queries
    the adversary can make. 
  \item Guess which simulator's response is going to be used by $\adv$ in its
    proof. Let $s$ be the index of this response.
  \item Guess which of the simulated polynomials in response $s$ will be
    used. Let $i$ be the index of this polynomial.
  \item Let $\tdv'$ be an algorithm that behaves exactly as $\tdv$, except when
    $\adv$ asks for $s$-th simulated proof, $\tdv'$'s internal procedure $\bdv'$
    provides $\adv$ with a simulated proof such that instead of commitment
    $\p{r} (\chi, 1)$ it gives $c$. 
  \item Start $\tdv'(\adv, \srs)$ and get the tree $\tree$.
  \item If indices $s$ or $i$ have not been guessed correctly, rewind $\adv$ to
    the beginning and pick new $s$ and $i$. Since $S = \poly$ probability that
    the correct $s$ will be guessed in polynomial time is overwhelming. That is,
    the reduction works in expected polynomial time. Similarly, $i$ takes values
    from $\range{1}{3}$, hence probability that $\rdv$ guesses $i$ in polynomial
    time is overwhelming. 
  \item Since $\tree$ contains $(\multconstr + \linconstr + 1)$ evaluations of
    $\p{t} (X, y)$, the polynomial can be reconstructed.
  \item Since $\adv$ is algebraic, $\rdv$ learns composition of $\p{t} (X, y)$
    in the $\srs$ and simulated elements.
  \item Hence $\rdv$ learns whether $c$ is a commitment to $\p{p} (X, 1)$ or
    $\p{p'} (X, 1)$.
  \item $\rdv$ returns its guessing bit to $\cdv$.
  \end{enumerate}

  \ncase{Game 3} In this game the adversary additionally wins if the commitment
  $\p{t} (\chi, y)$ comes from a simulated proof.

  \ncase{Game 2 to Game 3} Given adversary $\adv$ that wins in Game 3, but not
  in Game 2, we show a reduction $\rdv$ that uses $\adv$ and $\tdv$ to break
  hiding property of the commitment scheme. $\rdv$ proceeds as follows:
  \begin{enumerate}
  \item Guess the simulation query index $s$ the polynomial(s) come from.
  \item Pick random $y, z$ and compute polynomial $\p{t} (X, Y)$ as a simulator
    would compute.
  \item Produce two random polynomials $\p{p_0} (X, y)$ and $\p{p_1} (X, y)$ and
    send them to the challenger $\cdv$. Get commitment $c$.
  \item Let $\tdv'$ be an algorithm that behaves exactly as $\tdv$, except when
    $\adv$ asks for $s$-th simulated proof, $\tdv'$'s internal procedure $\bdv'$
    provides $\adv$ with a simulated proof such that:
    \begin{enumerate}
    \item Start making the simulated proof as a trapdoor-less simulator would.
    \item Before $\gone{\p{t} (z , y)}$ is computed
      get evaluation $p_z = \p{p_b} (z, y)$, i.e.~evaluate the polynomial in
      $c$ at $z$. 
    \item Let $\tilde{t}$ be the evaluation of the simulated $\p{t} (z, y)$.
    \item Pick $r$ such that $p_z + r = \p{t} (z, y)$.
    \item For the commitment of $\p{t} (X, y)$ output $c' = c + \gone{r}$.
    \item Compute the rest of the simulated proof as a simulator would.
    \end{enumerate}
  \item Let $\tdv'$ be an algorithm that behaves exactly as $\tdv$, except when
    $\adv$ asks for $s$-th simulated proof, $\tdv'$'s internal procedure $\bdv'$
    provides $\adv$ with a simulated proof such that instead of a simulated
    $\gone{\p{t} (\chi, y)}$ it gives $c'$.
  \item Start $\tdv'(\adv, \srs)$ and get the tree $\tree$.
  \item If index $s$ has not been guessed correctly, rewind $\adv$ to
    the beginning and pick new $s$. Since $S = \poly$ probability that
    the correct $s$ will be guessed in polynomial time is overwhelming. That is,
    the reduction works in expected polynomial time. 
  \item Since $\tree$ contains $\multconstr + \linconstr + 1$ evaluations of
    $\p{t} (X, y)$, the polynomial can be reconstructed.
  \item Since $\adv$ is algebraic, $\rdv$ learns composition of $\p{t} (X, y)$ in
    the $\srs$ and simulated elements. 
  \item Hence $\rdv$ learns whether $c$ is a commitment to $\p{p} (X, y)$ or
    $\p{p'} (X, y)$.
  \item $\rdv$ returns its guessing bit to $\cdv$.
  \end{enumerate}
 \end{proof}

\subsection{Honest verifier zero-knowledge}
\begin{lemma}
\label{lem:sonic_hvzk}
$\sonic$ is honest verifier zero-knowledge.
\end{lemma}
\begin{proof}
  The simulator proceeds as follows.
  \begin{enumerate}
  \item Pick randomly vectors $\vec{a}$, $\vec{b}$ and set
    \begin{equation}
      \label{eq:ab_eq_c}
      \vec{c} = \vec{a} \cdot \vec{b}. 
    \end{equation}
  \item Pick randomisers $c_{\multconstr + 1}, \ldots, c_{\multconstr + 4}$,
    honestly compute polynomials $\p{r}(X, Y), \p{r'}(X, Y), \p{s}(X, Y)$ and
    pick randomly challenges $y$, $z$.
  \item Output commitment $\gone{r} \gets \com(\srs, \multconstr, \p{r} (X,
    1))$ and challenge $y$. 
  \item Compute
    \begin{align*}
      & a' = \p{r}(z, 1),\\
      & b' = \p{r}(z, y),\\
      & s' = \p{s}(z, y).
    \end{align*} 
  \item Pick polynomial $\p{t}(X, Y)$ such that
    \begin{align*}
      & \p{t} (X, y) = \p{r} (X, 1) (\p{r}(X, y) + \p{s} (X, y)) - \p{k} (Y)\\
      & \p{t} (0, y) = 0
    \end{align*}
  \item Output commitment $\gone{t} = \com (\srs, \dconst, \p{t} (X, y))$ and
    challenge $z$.
  \item Continue following the protocol.
  \end{enumerate}

  We note that the simulation is perfect. This comes since, except polynomial
  $\p{t} (X, Y)$ all polynomials are computed following the protocol. For
  polynomial $\p{t} (X, Y)$ we observe that in a case of both real and simulated
  proof the verifier only learns commitment $\gone{t} = \p{t} (\chi, y)$ and
  evaluation $t' = \p{t} (z, y)$. Since the simulator picks $\p{t} (X, Y)$ such
  that 
  \begin{align*}
      \p{t} (X, y) = \p{r} (X, 1) (\p{r}(X, y) + \p{s} (X, y)) - \p{k} (Y)
  \end{align*}
  Values of $\gone{t}$ are equal in both proofs.
  Furthermore, the simulator picks its polynomial such that $\p{t}(0, y) = 0$,
  hence it does not need the trapdoor to commit to it. (Note that the proof
  system's SRS does not allow to commit to polynomials which have non-zero
  constant term). \qed
\end{proof}
\begin{remark} 
  As noted in \cite{CCS:MBKM19}, $\sonic$ is statistically subversion-zero
  knowledge (Sub-ZK). As noted in \cite{AC:ABLZ17}, one way to achieve
  subversion zero knowledge is to utilise an extractor that extracts a SRS
  trapdoor from a SRS-generator. Unfortunately, a NIZK made subversion
  zero-knowledge by this approach cannot achieve perfect Sub-ZK as one has to
  count in the probability of extraction failure. However, with the simulation
  presented in \cref{lem:sonic_hvzk}, the trapdoor is not required for the
  simulator as it is able to simulate the execution of the protocol just by
  picking appropriate (honest) verifier's challenges. This result transfers to
  $\sonicprotfs$, where the simulator can program the random oracle to provide
  challenges that fits it.
\end{remark}

\subsection{From forking soundness and unique response property to forking
  simulation extractability of $\sonicprotfs$}
Since \cref{lem:sonicprot_ur,lem:sonicprot_ss} hold, $\sonicprot$ is $\ur{1}$
and forking sound. We now make use
of \cref{thm:se} and show that $\sonicprotfs$ is forking simulation-extractable as defined in \cref{def:simext}.

\begin{corollary}[Forking simulation extractability of $\sonicprotfs$]
  \label{thm:sonicprotfs_se}
  Assume that $\sonicprot$ is $\ur{1}$ with security $\epsur(\secpar)$, and
  forking-sound with security $\epsss(\secpar)$. Let
  $\ro\colon \bin^* \to \bin^\secpar$ be a random oracle. Let $\advse$ be an
  algebraic adversary that can make up to $q$ random oracle queries, up to $S$
  simulation oracle queries, and outputs an acceptable proof for $\sonicprotfs$
  with probability at least $\accProb$. Then $\sonicprotfs$ is forking
  simulation-extractable with extraction error $\eta = \epsur(\secpar)$. The
  extraction probability $\extProb$ is at least
\[
		\extProb  \geq \frac{1}{q^{\multconstr + \linconstr}} (\accProb - \epsur(\secpar))^{\multconstr +
		\linconstr + 1} - \eps(\secpar).
	\]
	for some negligible $\eps(\secpar)$, $\multconstr$ and $\linconstr$ being,
  respectively, the number of multiplicative and linear constrains of the system.
\end{corollary}

\section{Non-malleability of $\marlinprotfs$}
\subsection{$\marlin$ protocol rolled-out}
\subsection{Unique response property}
\begin{lemma}
  Let $\PCOM$ be a commitment of knowledge with security $\epsk(\secpar)$,
  $\epsbind(\secpar)$ and has unique response property with security
  $\epsop(\secpar)$. Then probability that a $\ppt$ adversary $\adv$ breaks
  $\marlinprotfs$ $\ur{1}$ property is at most
  $6 \cdot (\epsbind + \epsop + \epsk)$ \michals{8.9}{Do we need to add
    probability that the idealized verifier fails $\epsid$?}
\end{lemma}
\begin{proof}
  As in previous proofs, we show the property by game hops. Let
  $M = \p{g_1}, \p{h_1}, \p{g_2}, \p{h_2}, \p{g_3}, \p{h_3}$. That is, $M$ is a
  set of all polynomials which commitments are send during the protocol after
  Round 1.

  \ncase{Game 0} In this game the adversary wins if it breaks evaluation
  binding, unique opening property, or knowledge soundness of one of commitments
  for polynomials in $M$.

  Probability that a $\ppt$ adversary wins in Game 0, is upper bounded by $6
  \cdot (\epsbind + \epsop + \epsk)$.

  \ncase{Game 1} In this game the adversary additionally wins if it breaks the
  $\ur{2}$ property of the protocol

  \ncase{Game 0 to Game 1} Probability that the adversary wins in
  Game 1 but not in Game 0 is $0$. This is since the polynomials in $M$ are
  uniquely determined. W.l.o.g.~we analyse probability that adversary is able to
  produce two (different) pairs of polynomials $(\p{h_2}, \p{g_2})$ and $(\p{h'_2},
  \p{g'_2})$ such that
  \begin{align*}
    \p{h_2} (X) \ZERO_{\HHH} (X) + X \p{g_2} (X) & = \p{h_2} (X) \ZERO_{\HHH} (X) +
                                                   X \p{g_2} (X)\\
    (\p{h_2} (X) - \p{h'_2} (X)) \ZERO_{\HHH} (X) & = X (\p{g'_2} (X) - \p{g'_2}
    (X)).
  \end{align*}
  Since $\p{h_2}, \p{g_2} \in \FF^{< |\HHH| - 1} [X]$ and
  $\ZERO \in \FF^{|\HHH|} [X]$, LHS has different degree than RHS unless both
  sides have degree $0$. This happens when $\p{h_2} (X) = \p{h'_2} (X)$ and
  $\p{g_2} (X) - \p{g'_2} (X)$.
\end{proof}

\subsection{Forking soundness}
\begin{lemma}
  Assume that an idealised $\marlinprot$ verifier fails with probability at most
  $\epsid(\secpar)$ and probability that a $\ppt$ adversary breaks dlog is
  bounded by $\epsdlog(\secpar)$. Then $\marlinprot$ is
  $(..., 2, |\HHH| + b + 1)$-forking sound.
\end{lemma}
\begin{proof}
  \michals{8.9}{Need to check the degrees}
  The proof goes similarly to the respective proofs for $\plonk$ and
  $\sonic$. That is, let $\srs$ be $\marlinprot$'s SRS and denote by $\srs_1$
  all SRS's $\GRP_1$-elements. Let $\tdv$ be an algebraic adversary that
  produces a statement $\inp$ and a $(1, |\HHH| + b + 1, 1, 1)$-tree of
  acceptable transcripts $\tree$. Note that in all transcripts the instance
  $\inp$, proof elements
  $\sigma_1, \gone{\p{w}(\chi), \p{z_A}(\chi), \p{z_B}(\chi), \p{z_C}(\chi),
    \p{h_0}(\chi), \p{s}(\chi)}, \gone{\p{g_1}(\chi), \p{h_1}(\chi)}$
  % $\sigma_1, \gone{\p{w}(\chi), \p{z_A}(\chi), \p{z_B}(\chi), \p{z_C}(\chi),
  %   \p{h_0}(\chi), \p{s}(\chi)}, \alpha, \eta_A, \eta_B, \eta_C,
  % \gone{\p{g_1}(\chi), \p{h_1}(\chi)}, \beta_1, \sigma_2, \gone{\p{g_2}(\chi),
  % \p{h_2}(\chi)}, \beta_2, \simga_2, \gone{\p{g_3}(\chi), \p{h_3}(\chi)}$
  and challenges $\alpha, \eta_1, \eta_2, \eta_3$ are common as the transcripts
  share the first $3$ messages. The tree branches after the third message of the
  protocol where the challenge $\beta_1$ is presented, thus tree $\tree$ is
  build using different values of $\beta_1$.

  We consider two games.

  \ncase{Game 0} In this game the adversary wins if all the transcripts it
  produced are acceptable by the ideal verifier,
  i.e.~$\vereq_{\inp, \zkproof}(X) = 0$, cf.~\cref{eq:marlin_ver_eq}, yet the extractor
  fails to extract a valid witness out of them.

  Probability of $\tdv$ winning this game is $\epsid(\secpar)$ as the protocol
  $\marlinprot$, instantiated with the idealised verification equation, is
  perfectly sound except with negligible probability of the idealised verifier
  failure $\epsid(\secpar)$. Hence for a valid proof $\zkproof$ for a statement
  $\inp$ there exists a witness $\wit$, such that $\REL(\inp, \wit)$ holds. Note
  that since the $\tdv$ produces $(|\HHH| + b + 1)$ acceptable transcripts for
  different challenges $\beta_1$, it obtains the same number of different
  evaluations of polynomials $\p{z_A}, \p{z_B}, \p{z_C}$.

  Since the transcripts are acceptable by an idealised verifier, the equality
  $\p{z_A} (X) \p{z_B} (X) - \p{z_C} (X) = \p{h_0} (X) \ZERO_\HHH (X)$
  holds. Hence, $\p{z_A}, \p{z_B}, \p{z_C}$ encodes the valid witness for the
  proven statement. Since $\p{z_A}, \p{z_B}, \p{z_C}$ are of degree at most
  $(|\HHH| + b)$ and there is more than $(|\HHH| + b + 1)$ their evaluations
  known, $\extt$ can recreate their coefficients by interpolation and reveal the
  witness with probability $1$. Hence, the probability that extraction fails in
  that case is upper-bounded by probability of an idealised verifier failing
  $\epsid(\secpar)$, which is negligible.

  \ncase{Game 1} In this game the adversary additionally wins if it produces a
  transcript in $\tree$ such that $\vereq_{\inp, \zkproof}(\chi) = 0$, but
  $\vereq_{\inp, \zkproof}(X) \neq 0$. That is, the ideal verifier does not
  accept the proof, but the real verifier does.

  \ncase{Game 0 to Game 1} Assume the adversary wins in Game 1, but
  does not win in Game 0. We show that such adversary may be used to break the
  $\dlog$ assumption. More precisely, let $\tdv$ be an adversary that for
  relation $\REL$ and randomly picked $\srs \sample \kgen(\REL)$ produces a tree
  of acceptable transcripts such that the winning condition of the game
  holds. Let $\rdvdlog$ be a reduction that gets as input an
  $(|\HHH| + b + 1, 1)$-dlog instance $\gone{1, \ldots, ...}, \gtwo{\chi}$ and
  is tasked to output $\chi$. The reduction proceeds as follows---it gives the
  input instance to the adversary as the SRS. Let $(1, \tree)$ be the output
  returned by $\adv$. Let $\inp$ be a relation proven in $\tree$.  Consider a
  transcript $\zkproof \in \tree$ such that $\vereq_{\inp, \zkproof}(X) \neq 0$,
  but $\vereq_{\inp, \zkproof}(\chi) = 0$. Since the adversary is algebraic, all
  group elements included in $\tree$ are extended by their representation as a
  combination of the input $\GRP_1$-elements. Hence all coefficients of the
  verification equation polynomial $\vereq_{\inp, \zkproof}(X)$ are known and
  $\rdvdlog$ can find its zero points. Since
  $\vereq_{\inp, \zkproof}(\chi) = 0$, the targeted discrete log value $\chi$ is
  among them.  Hence, the probability that this event happens is upper-bounded
  by $\epsdlog(\secpar)$. \qed

\end{proof}

\subsection{Honest-verifier zero knowledge}
\begin{lemma}
  $\marlinprot$ is honest verifier zero-knowledge.
\end{lemma}
\begin{proof}
  The simulator follows the protocol, except it picks the challenges
  $\beta_1, \beta_2, \beta_3$ not randomly, but to make sure the verification
  equation holds. That is, the simulator
  \begin{enumerate}
  \item Picks witness $\wit$ at random, sets $\p{z_A}, \p{z_B}$ as polynomials
    interpolating $A (\inp^\top, \wit^{\top})^{\top}$ and $\p{z_C}$ such that
    $\p{z_A} (\beta_1) \p{z_B} (\beta_1) - \p{z_C} (\beta_1) = \p{h_0} (\beta_1)
    \ZERO_\HHH (\beta_1)$. Then it follows the protocol till the point $\beta_1$
    is sent.
  \item Instead of picking $\beta_1$ at random it picks it such that
    $ \p{s} (\beta_1) + \p{r}(\alpha, \beta_1) (\sum_M \eta_M \p{z_M} (\beta_1)
    ) - \sigma_2 \p{z} (\beta_1) = \p{h_1} (\beta_1) \ZERO_\HHH (\beta_1) +
    \beta_1 \p{g_1} (\beta_1) + \sigma_1 / |\HHH|$. The it follows the protocol
    till $\beta_2$ is picked.
  \item Instead of picking $\beta_2$ at random, it picks it such that
    $\p{r} (\alpha, \beta_2) \sigma_3 = \p{h_2} (\beta_2) \ZERO_\HHH(\beta_2) +
    \beta_2 \p{g_2} (\beta_2) + \sigma_2/|\HHH|$ holds. Then it continues till
    the challenge $\beta_3$ is picked.
  \item Instead of picking $\beta_3$ at random, it picks it such that
    $\p{h_3} (\beta_3 ) \ZERO_\KKK (\beta_3) = \p{a}(\beta_3) - \p{b} (\beta_3)
    (\beta_3 \p{g_3} (\beta_3) + \sigma_3/|\KKK|)$ holds.
  \end{enumerate}
\end{proof}
\subsection{From forking soundness and unique response property to forking
  simulation extractability of $\marlinprotfs$}

\section{Further work}
We identify a number of problems which we left as further work. First of all,
the generalised version of the forking lemma presented in this paper can be
generalised even further to include protocols where forking soundness holds for
protocols where $\extt$ extracts a witness from a $(n_1, \ldots, n_\mu)$-tree of
acceptable transcripts, where more than one $n_j > 1$. I.e.~to include
protocols that for witness extraction require transcripts that branch at more
than one point.

Although we picked $\plonk$ and $\sonic$ as examples for our framework, it is
not limited to SRS-based NIZKs. Thus, it would be interesting to apply it to
known so-called transparent zkSNARKs like Bulletproofs \cite{SP:BBBPWM18},
Aurora \cite{EC:BCRSVW19} or AuroraLight \cite{EPRINT:Gabizon19a}.

Since the rewinding technique and the forking lemma used to show simulation
extractability of $\plonkprotfs$ and $\sonicprotfs$ come with security loss,
it would be interesting to show SE of these protocols directly in the
algebraic group model.

Although we focused here only on zkSNARKs, it is worth to
investigating other protocols that may benefit from our framework, like
e.g.~identification schemes.

Last, but not least, this paper would benefit greatly if a more tight version
of the generalised forking lemma was provided. However, we have to note here
that some of the inequalities used in the proof are already tight, i.e.~for
specific adversaries, some of the inequalities are already equalities.

% \section*{Acknowledgement}
% The second author thanks Antoine Rondelet for helpful discussions.

%\begin{spacing}{0.92}
\bibliographystyle{abbrv}
\bibliography{cryptobib/abbrev3,cryptobib/crypto,additional_bib}
%\end{spacing}
% \clearpage
\appendix
%{\Huge{Supplementary Material}} 

\section{Omitted protocols descriptions}
\subsection{Polynomial commitment schemes}
\label{sec:pcom}
\cref{fig:pcomp,fig:pcoms} present variants of KZG polynomial commitment schemes
used in \plonk{} and \sonic{}. The key generation algorithm $\kgen$ takes as
input a security parameter $\secparam$ and a parameter $\maxdeg$ which
determines the maximal degree of the committed polynomial. We assume that
$\maxdeg$ can be read from the output SRS.


We emphasize the following properties of a secure polynomial commitment
$\PCOM$:
\begin{description}
\item[Evaluation binding:] A $\ppt$ adversary $\adv$ which outputs a commitment
  $\vec{c}$ and evaluation points $\vec{z}$ has at most negligible chances to open
  the commitment to two different evaluations $\vec{s}, \vec{s'}$. That is, let
  $k \in \NN$ be the number of committed polynomials, $l \in \NN$ number of
  evaluation points, $\vec{c} \in \GRP^k$ be the commitments, $\vec{z} \in
  \FF_p^l$ be the arguments the polynomials are evaluated at, $\vec{s},\vec{s}'
  \in \FF_p^k$ the evaluations, and $\vec{o},\vec{o}' \in \FF_p^l$ be the
  commitment openings. Then for every $\ppt$ adversary $\adv$
	\[
		\Pr
			\left[
			\begin{aligned}
				& \verify(\srs, \vec{c}, \vec{z}, \vec{s}, \vec{o}) = 1,  \\ 
				& \verify(\srs, \vec{c}, \vec{z}, \vec{s}', \vec{o}') = 1, \\
				& \vec{s} \neq \vec{s}'
			\end{aligned}
			\,\left|\,\vphantom{\begin{aligned}
                  & \\
                  & \\
                  &
                \end{aligned}}
			\begin{aligned}
				& \srs \gets \kgen(\secparam, \maxdeg),\\
				& (\vec{c}, \vec{z}, \vec{s}, \vec{s}', \vec{o}, \vec{o}') \gets \adv(\srs)
			\end{aligned}
			\right.\right] \leq \negl\,.
	\]

\end{description}
	
We say that $\PCOM$ has the unique opening property if the following holds:
\begin{description}
\item[Opening uniqueness:] Let $k \in \NN$ be the number of committed
  polynomials, $l \in \NN$ number of evaluation points, $\vec{c} \in \GRP^k$ be
  the commitments, $\vec{z} \in \FF_p^l$ be the arguments the polynomials are
  evaluated at, $\vec{s} \in \FF_p^k$ the evaluations, and $\vec{o} \in \FF_p^l$
  be the commitment openings. Then for every $\ppt$ adversary $\adv$
	\[
		\Pr
			\left[
			\begin{aligned}
				& \verify(\srs, \vec{c}, \vec{z}, \vec{s}, \vec{o}) = 1,  \\ 
				& \verify(\srs, \vec{c}, \vec{z}, \vec{s}, \vec{o'}) = 1, \\
				& \vec{o} \neq \vec{o'}
			\end{aligned}
			\,\left|\, \vphantom{\begin{aligned}
                  & \\
                  & \\
                  &
                \end{aligned}}
			\begin{aligned}
				& \srs \gets \kgen(\secparam, \maxdeg),\\
				& (\vec{c}, \vec{z}, \vec{s}, \vec{o}, \vec{o'}) \gets \adv(\srs)
			\end{aligned}
			\right.\right] \leq \negl\,.
	\]
\end{description}
Intuitively, opening uniqueness assures that there is only one valid opening
for the committed polynomial and given evaluation point. This property is
crucial in showing forking simulation-extractability of $\plonk$ and $\sonic$. We show
that the $\plonk$'s and $\sonic$'s polynomial commitment schemes satisfy this
requirement in \cref{lem:pcomp_op} and \cref{lem:pcoms_unique_op}
respectively.

We also formalize notion of $k$-hiding property of a polynomial commitment scheme
\begin{description}
\item[Hiding] Let $\HHH$ be a set of size $\maxdeg + 1$ and $\ZERO_\HHH$ its
  vanishing polynomial. We say that a polynomial scheme is \emph{hiding} with
  security $\epsh(\secpar)$ if for every $\ppt$ adversary $\adv$, $k \in \NN$,
  probability
  \begin{align*}
    \Pr\left[
    \begin{aligned}
      & b' = b
      \end{aligned}
        \,\left|\,
        \begin{aligned}
    & (\srs, \maxdeg) \sample \kgen(\secparam),
    (f_0, f_1, c, k, b') \gets \adv^{\oraclec}(\srs), f_0, f_1 \in \FF^{\maxdeg}
    [X]
    \end{aligned}
    \right.\right] \leq \frac{1}{2} + \eps(\secpar)
  \end{align*}
  Here, $\oraclec$ is a challenge oracle that
  \begin{compactenum}
    \item takes polynomials $f_0, f_1$ provided by the adversary and parameter $k$,
    \item samples bit $b$,
    \item samples vector $\vec{a} \in \FF^k$,
    \item computes polynomial,
      $f'_b (X) = f_b + \ZERO_\HHH (X) (a_0 + a_1 X + \ldots a_{k - 1} X^{k -
        1})$,
    \item outputs polynomial commitment $c = f'_b (\chi)$,
    \item on adversary's evaluation query $x$ it adds $x$ to initially empty set
      $Q_x$ and if $|Q_x| \leq k$, it provides $f'_b (x)$.
    \end{compactenum}
  \end{description}

\begin{description}
\item[Commitment of knowledge]  For every $\ppt$ adversary $\adv$ who produces
  commitment $c$, evaluation $s$ and opening $o$ there
  exists a $\ppt$ extractor $\ext$ such that
\[
  \Pr \left[
    \begin{aligned}
      & \deg \p{f} \leq \maxdeg\\
      & c = \com(\srs, \p{f}),\\
      & \verify(\srs, c, z, s, o) = 1
    \end{aligned}
    \,\left|\,
      \vphantom{
        \begin{aligned}
          & \\
          & \\
          &
        \end{aligned}
        }
    \begin{aligned}
      & \srs \gets \kgen(\secparam, \maxdeg),\\
      & c \gets \adv(\srs),
      z \sample \FF_p \\
      & (s, o) \gets \adv(\srs, c, z), \\
      & \p{f} = \ext_\adv(\srs, c)\\
    \end{aligned}
  \right.\right]
  \geq 1 - \epsk(\secpar).
\]
In that case we say that $\PCOM$ is $\epsk(\secpar)$-knowledge.
\end{description}
Intuitively when a commitment scheme is ``of knowledge'' then if an
adversary produces a (valid) commitment $c$, which it can open, then it also
knows the underlying polynomial $\p{f}$ which commits to that value.
\cite{CCS:MBKM19} shows, using AGM, that $\PCOMs$ is a commitment of knowledge.
The same reasoning could be used to show that property for $\PCOMp$.

% We require $\PCOM$ to have the following properties:
\begin{figure}
\centering
	\begin{pcvstack}[center,boxed]
		\begin{pchstack}
			\procedure{$\kgen(\secparam, \maxdeg)$}
			{
			\chi \sample \FF^2_p \\ [\myskip]
			\pcreturn \gone{1, \ldots, \chi^{\numberofconstrains + 2}}, \gtwo{\chi}\\ [\myskip]
				\hphantom{\hspace*{5.5cm}}	
        %\hphantom{\pcind \p{o}_i(X) \gets \sum_{j = 1}^{t_i} \gamma_i^{j - 1}
        %\frac{\p{f}_{i,j}(X) - \p{f}_{i, j}(z_i)}{X - z_i}}
      }
			
			\pchspace
			
			\procedure{$\com(\srs, \vec{\p{f}}(X))$}
			{ 
				\pcreturn \gone{\vec{c}} = \gone{\vec{\p{f}}(\chi)}\\ [\myskip]
				\hphantom{\pcind \pcif 
					\sum_{i = 1}^{\abs{\vec{z}}} r_i \cdot \gone{\sum_{j = 1}^{t_j}
					\gamma_i^{j - 1} c_{i, j} - \sum{j = 1}^{t_j} s_{i, j}} \bullet
				\gtwo{1} + }
			}
		\end{pchstack}
		% \pcvspace
    
		\begin{pchstack}
			\procedure{$\open(\srs, \vec{\gamma}, \vec{z}, \vec{s}, \vec{\p{f}}(X))$}
			{
			\pcfor i \in \range{1}{\abs{\vec{z}}} \pcdo\\ [\myskip]
      \pcind \p{o}_i(X) \gets \sum_{j = 1}^{t_i} \gamma_i^{j - 1}
      \frac{\p{f}_{i,j}(X) - \p{f}_{i, j}(z_i)}{X - z_i}\\ [\myskip] \pcreturn
      \vec{o} = \gone{\vec{\p{o}}(\chi)}\\ [\myskip]
				\hphantom{\hspace*{5.5cm}}	
			}
			
			\pchspace
			
			\procedure{$\verify(\srs, \gone{c}, \vec{z}, \vec{s}, \gone{\p{o}(\chi)})$}
			{
				\vec{r} \gets \FF_p^{\abs{\vec{z}}}\\ [\myskip]
				\pcfor i \in \range{1}{\abs{\vec{z}}} \pcdo \\ [\myskip]
				\pcind \pcif 
          \sum_{i = 1}^{\abs{\vec{z}}} r_i \cdot \gone{\sum_{j = 1}^{t_j}
          \gamma_i^{j - 1} c_{i, j} - \sum{j = 1}^{t_j} s_{i, j}} \bullet
          \gtwo{1} + \\ [\myskip] \pcind \sum_{i = 1}^{\abs{\vec{z}}} r_i z_i
          o_i
          \bullet \gtwo{1} \neq \gone{- \sum_{i = 1}^{\abs{\vec{z}}} r_i o_i }
          \bullet \gtwo{\chi} \pcthen  \\
					\pcind \pcreturn 0\\ [\myskip]
					\pcreturn 1.
			}
		\end{pchstack}
	\end{pcvstack}
	\caption{$\PCOMp$ polynomial commitment scheme.}
	\label{fig:pcomp}
  \end{figure}

\begin{figure}
\centering
	\begin{pcvstack}[center,boxed]
		\begin{pchstack}
			\procedure{$\kgen(\secparam, \maxdeg)$} {
				\alpha, \chi \sample \FF^2_p \\ [\myskip]
				\pcreturn \gone{\smallset{\chi^i}_{i = -\multconstr}^{\multconstr},
          \smallset{\alpha \chi^i}_{i = -\multconstr, i \neq
            0}^{\multconstr}},\\
        \pcind \gtwo{\smallset{\chi^i, \alpha \chi^i}_{i =
            -\multconstr}^{\multconstr}}, \gtar{\alpha}\\
				%\markulf{03.11.2020}{} \\
			%	\hphantom{\pcind \p{o}_i(X) \gets \sum_{j = 1}^{t_i} \gamma_i^{j - 1} \frac{\p{f}_{i,j}(X) - \p{f}_{i, j}(z_i)}{X - z_i}}
				\hphantom{\hspace*{5.5cm}}	
		}
			
			\pchspace
			
			\procedure{$\com(\srs, \maxconst, \p{f}(X))$} {
				\p{c}(X) \gets \alpha \cdot X^{\dconst - \maxconst} \p{f}(X) \\ [\myskip]
				\pcreturn \gone{c} = \gone{\p{c}(\chi)}\\ [\myskip]
				\hphantom{\pcind \pcif \sum_{i = 1}^{\abs{\vec{z}}} r_i \cdot
          \gone{\sum_{j = 1}^{t_j} \gamma_i^{j - 1} c_{i, j} - \sum_{j = 1}^{t_j}
            s_{i, j}} \bullet \gtwo{1} + } }
		\end{pchstack}
		% \pcvspace
    
		\begin{pchstack}
			\procedure{$\open(\srs, z, s, f(X))$}
			{
				\p{o}(X) \gets \frac{\p{f}(X) - \p{f}(z)}{X - z}\\ [\myskip]
				\pcreturn \gone{\p{o}(\chi)}\\ [\myskip]
				\hphantom{\hspace*{5.5cm}}	
			}
			
			\pchspace
			
			\procedure{$\verify(\srs, \maxconst, \gone{c}, z, s, \gone{\p{o}(\chi)})$}
      {
        \pcif \gone{\p{o}(\chi)} \bullet \gtwo{\alpha \chi} + \gone{s - z
        \p{o}(\chi)} \bullet \gtwo{\alpha} = \\ [\myskip] \pcind \gone{c}
        \bullet \gtwo{\chi^{- \dconst + \maxconst}} \pcthen  \pcreturn 1\\
        [\myskip]
        \rlap{\pcelse \pcreturn 0.} \hphantom{\pcind \pcif \sum_{i =
            1}^{\abs{\vec{z}}} r_i \cdot \gone{\sum_{j = 1}^{t_j} \gamma_i^{j -
              1} c_{i, j} - \sum{j = 1}^{t_j} s_{i, j}} \bullet \gtwo{1} + } }
		\end{pchstack}
	\end{pcvstack}
	
	\caption{$\PCOMs$ polynomial commitment scheme.}
	\label{fig:pcoms}
\end{figure}

\section{Non-malleability of \plonk{}, omitted proofs and descriptions}
\label{sec:plonk_supp_mat}
\subsection{$\plonk$ protocol rolled out}
\label{sec:plonk_explained}
\newcommand{\vql}{\vec{q_{L}}}
\newcommand{\vqr}{\vec{q_{R}}}
\newcommand{\vqm}{\vec{q_{M}}}
\newcommand{\vqo}{\vec{q_{O}}}
\newcommand{\vx}{\vec{x}}
\newcommand{\vqc}{\vec{q_{C}}}
\oursubsub{The constrain system}
Assume $\CRKT$ is a fan-in two arithmetic circuit,
which fan-out is unlimited and has $\numberofconstrains$ gates and $\noofw$ wires
($\numberofconstrains \leq \noofw \leq 2\numberofconstrains$). \plonk's constraint
system is defined as follows:
\begin{itemize}
\item Let $\vec{V} = (\va, \vb, \vc)$, where $\va, \vb, \vc
  \in \range{1}{\noofw}^\numberofconstrains$. Entries $\va_i, \vb_i, \vc_i$ represent indices of left,
  right and output wires of circuits $i$-th gate.
\item Vectors $\vec{Q} = (\vql, \vqr, \vqo, \vqm, \vqc) \in
  (\FF^\numberofconstrains)^5$ are called \emph{selector vectors}:
  \begin{itemize}
  \item If the $i$-th gate is a multiplicative gate then $\vql_i = \vqr_i = 0$,
    $\vqm_i = 1$, and $\vqo_i = -1$. 
  \item If the $i$-th gate is an addition gate then $\vql_i = \vqr_i  = 1$, $\vqm_i =
    0$, and $\vqo_i = -1$. 
  \item $\vqc_i = 0$ always. 
  \end{itemize}
\end{itemize}

We say that vector $\vx \in \FF^\noofw$ satisfies constraint system if for all $i
\in \range{1}{\numberofconstrains}$
\[
  \vql_i \cdot \vx_{\va_i} + \vqr_i \cdot \vx_{\vb_i} + \vqo \cdot \vx_{\vc_i} +
  \vqm_i \cdot (\vx_{\va_i} \vx_{\vb_i}) + \vqc_i = 0. 
\]

\oursubsub{Algorithms rolled out}
\label{sec:plonk_explained}
\plonk{} argument system is universal. That is, it allows to verify computation
of any arithmetic circuit which has no more than $\numberofconstrains$
gates using a single SRS. However, to make computation efficient, for each
circuit there is allowed a preprocessing phase which extend the SRS with
circuit-related polynomial evaluations.

For the sake of simplicity of the security reductions presented in this paper, we
include in the SRS only these elements that cannot be computed without knowing
the secret trapdoor $\chi$. The rest of the SRS---the preprocessed input---can
be computed using these SRS elements thus we leave them to be computed by the
prover, verifier, and simulator.

\ourpar{$\plonk$ SRS generating algorithm $\kgen(\REL)$:}
The SRS generating algorithm picks at random $\chi \sample \FF_p$, computes
and outputs
\[
	\srs = \left(\gone{\smallset{\chi^i}_{i = 0}^{\numberofconstrains + 2}},
	\gtwo{\chi} \right).
\]

\ourpar{Preprocessing:}
Let $H = \smallset{\omega^i}_{i = 1}^{\numberofconstrains }$ be a
(multiplicative) $\numberofconstrains$-element subgroup of a field $\FF$
compound of $\numberofconstrains$-th roots of unity in $\FF$. Let $\lag_i(X)$ be
the $i$-th element of an $\numberofconstrains$-elements Lagrange basis. During
the preprocessing phase polynomials $\p{S_{id j}}, \p{S_{\sigma j}}$, for
$\p{j} \in \range{1}{3}$, are computed:
\begin{equation*}
  \begin{aligned}
    \p{S_{id 1}}(X) & = X,\vphantom{\sum_{i = 1}^{\noofc} \sigma(i) \lag_i(X),}\\
    \p{S_{id 2}}(X) & = k_1 \cdot X,\vphantom{\sum_{i = 1}^{\noofc} \sigma(i) \lag_i(X),}\\
    \p{S_{id 3}}(X) & = k_2 \cdot X,\vphantom{\sum_{i = 1}^{\noofc} \sigma(i) \lag_i(X),}
  \end{aligned}
  \qquad
\begin{aligned}
  \p{S_{\sigma 1}}(X) & = \sum_{i = 1}^{\noofc} \sigma(i) \lag_i(X),\\
  \p{S_{\sigma 2}}(X) & = \sum_{i = 1}^{\noofc}
  \sigma(\noofc + i) \lag_i(X),\\
  \p{S_{\sigma 3}}(X) & =\sum_{i = 1}^{\noofc} \sigma(2 \noofc + i) \lag_i(X).
\end{aligned}
\end{equation*}
Coefficients $k_1$, $k_2$ are such that $H, k_1 \cdot H, k_2 \cdot H$ are
different cosets of $\FF^*$, thus they define $3 \cdot \noofc$
different elements. \cite{EPRINT:GabWilCio19} notes that it is enough to set
$k_1$ to a quadratic residue and $k_2$ to a quadratic non-residue.

Furthermore, we define polynomials $\p{q_L}, \p{q_R}, \p{q_O}, \p{q_M}, \p{q_C}$
such that
\begin{equation*}
  \begin{aligned}
  \p{q_L}(X) & = \sum_{i = 1}^{\noofc} \vql_i \lag_i(X), \\
  \p{q_R}(X) & = \sum_{i = 1}^{\noofc} \vqr_i \lag_i(X), \\
  \p{q_M}(X) & = \sum_{i = 1}^{\noofc} \vqm_i \lag_i(X),
\end{aligned}
\qquad
\begin{aligned}
  \p{q_O}(X) & = \sum_{i = 1}^{\noofc} \vqo_i \lag_i(X), \\
  \p{q_C}(X) & = \sum_{i = 1}^{\noofc} \vqc_i \lag_i(X). \\
  \vphantom{\p{q_M}(X)  = \sum_{i = 1}^{\noofc} \vqm_i \lag_i(X),}
\end{aligned}
\end{equation*}

\ourpar{$\plonk$ prover
  $\prover(\srs, \inp, \wit = (\wit_i)_{i \in \range{1}{3 \cdot
      \noofc}})$.}
\begin{description}
\item[Round 1] Sample $b_1, \ldots, b_9 \sample \FF_p$; compute
  $\p{a}(X), \p{b}(X), \p{c}(X)$ as
	\begin{align*}
		\p{a}(X) &= (b_1 X + b_2)\p{Z_H}(X) + \sum_{i = 1}^{\noofc} \wit_i \lag_i(X) \\
		\p{b}(X) &= (b_3 X + b_4)\p{Z_H}(X) + \sum_{i = 1}^{\noofc} \wit_{\noofc + i} \lag_i(X) \\
		\p{c}(X) &= (b_5 X + b_6)\p{Z_H}(X) + \sum_{i = 1}^{\noofc} \wit_{2 \cdot \noofc + i} \lag_i(X) 
	\end{align*}
	Output polynomial commitments $\gone{\p{a}(\chi), \p{b}(\chi), \p{c}(\chi)}$.
	
	\item[Round 2]
	Get challenges $\beta, \gamma \in \FF_p$
	\[
		\beta = \ro(\zkproof[0..1], 0)\,, \qquad \gamma = \ro(\zkproof[0..1], 1)\,.
	\]
	Compute permutation polynomial $\p{z}(X)$
	\begin{multline*}
		\p{z}(X) = (b_7 X^2 + b_8 X + b_9)\p{Z_H}(X) + \lag_1(X) + \\
			+ \sum_{i = 1}^{\noofc - 1} 
			\left(\lag_{i + 1} (X) \prod_{j = 1}^{i} 
			\frac{
			(\wit_j +\beta \omega^{j - 1} + \gamma)(\wit_{\noofc + j} + \beta k_1 \omega^{j - 1} + \gamma)(\wit_{2 \noofc + j} +\beta k_2 \omega^{j- 1} + \gamma)}
			{(\wit_j+\sigma(j) \beta + \gamma)(\wit_{\noofc + j} + \sigma(\noofc + j)\beta + \gamma)(\wit_{2 \noofc + j} + \sigma(2 \noofc + j)\beta + \gamma)}\right)
	\end{multline*}
	Output polynomial commitment $\gone{\p{z}(\chi)}$
		
	\item[Round 3]
	Get the challenge $\alpha = \ro(\zkproof[0..2])$, compute the quotient polynomial 
	\begin{align*}
	& \p{t}(X)  = \\
	& (\p{a}(X) \p{b}(X) \selmulti(X) + \p{a}(X) \selleft(X) + 
	\p{b}(X)\selright(X) + \p{c}(X)\seloutput(X) + \pubinppoly(X) + \selconst(X)) 
	\frac{1}{\p{Z_H}(X)} +\\
	& + ((\p{a}(X) + \beta X + \gamma) (\p{b}(X) + \beta k_1 X + \gamma)(\p{c}(X) 
	+ \beta k_2 X + \gamma)\p{z}(X)) \frac{\alpha}{\p{Z_H}(X)} \\
	& - (\p{a}(X) + \beta \p{S_{\sigma 1}}(X) + \gamma)(\p{b}(X) + \beta 
	\p{S_{\sigma 2}}(X) + \gamma)(\p{c}(X) + \beta \p{S_{\sigma 3}}(X) + 
	\gamma)\p{z}(X \omega))  \frac{\alpha}{\p{Z_H}(X)} \\
	& + (\p{z}(X) - 1) \lag_1(X) \frac{\alpha^2}{\p{Z_H}(X)}
	\end{align*}
	Split $\p{t}(X)$ into degree less then $\noofc$ polynomials $\p{t_{lo}}(X), \p{t_{mid}}(X), \p{t_{hi}}(X)$, such that
	\[
		\p{t}(X) = \p{t_{lo}}(X) + X^{\noofc} \p{t_{mid}}(X) + X^{2 \noofc} \p{t_{hi}}(X)\,.
	\]
	Output $\gone{\p{t_{lo}}(\chi), \p{t_{mid}}(\chi), \p{t_{hi}}(\chi)}$.
	
	\item[Round 4]
	Get the challenge $\chz \in \FF_p$, $\chz = \ro(\zkproof[0..3])$.
	Compute opening evaluations
	\begin{align*}
      \p{a}(\chz), \p{b}(\chz), \p{c}(\chz), \p{S_{\sigma 1}}(\chz), \p{S_{\sigma 2}}(\chz), \p{t}(\chz), \p{z}(\chz \omega),
	\end{align*}
	Compute the linearisation polynomial
	\[
		\p{r}(X) = 
		\begin{aligned}
      & \p{a}(\chz) \p{b}(\chz) \selmulti(X) + \p{a}(\chz) \selleft(X) + \p{b}(\chz) \selright(X) + \p{c}(\chz) \seloutput(X) + \selconst(X) \\
      & + \alpha \cdot \left( (\p{a}(\chz) + \beta \chz + \gamma) (\p{b}(\chz) + \beta k_1 \chz + \gamma)(\p{c}(\chz) + \beta k_2 \chz + \gamma) \cdot \p{z}(X)\right) \\
      & - \alpha \cdot \left( (\p{a}(\chz) + \beta \p{S_{\sigma 1}}(\chz) + \gamma) (\p{b}(\chz) + \beta \p{S_{\sigma 2}}(\chz) + \gamma)\beta \p{z}(\chz\omega) \cdot \p{S_{\sigma 3}}(X)\right) \\
      & + \alpha^2 \cdot \lag_1(\chz) \cdot \p{z}(X)
		\end{aligned}
	\]
	Output $\p{a}(\chz), \p{b}(\chz), \p{c}(\chz), \p{S_{\sigma 1}}(\chz), \p{S_{\sigma 2}}(\chz), \p{t}(\chz), \p{z}(\chz \omega), \p{r}(\chz).$
	
	\item[Round 5]
	Compute the opening challenge $v \in \FF_p$, $v = \ro(\zkproof[0..4])$.
	Compute the openings for the polynomial commitment scheme 
	\begin{align*}
	& \p{W_\chz}(X) = \frac{1}{X - \chz} \left(
	\begin{aligned}
		& \p{t_{lo}}(X) + \chz^\noofc \p{t_{mid}}(X) + \chz^{2 \noofc} \p{t_{hi}}(X) - \p{t}(\chz)\\
		& + v(\p{r}(X) - \p{r}(\chz)) \\
		& + v^2 (\p{a}(X) - \p{a}(\chz))\\
		& + v^3 (\p{b}(X) - \p{b}(\chz))\\
		& + v^4 (\p{c}(X) - \p{c}(\chz))\\
		& + v^5 (\p{S_{\sigma 1}}(X) - \p{S_{\sigma 1}}(\chz))\\
		& + v^6 (\p{S_{\sigma 2}}(X) - \p{S_{\sigma 2}}(\chz))
	\end{aligned}
	\right)\\
	& \p{W_{\chz \omega}}(X) = \frac{\p{z}(X) - \p{z}(\chz \omega)}{X - \chz \omega}
\end{align*}
	Output $\gone{\p{W_{\chz}}(\chi), \p{W_{\chz \omega}}(\chi)}$.
\end{description}

\ncase{$\plonk$ verifier $\verifier(\srs, \inp, \zkproof)$}\ \newline
The \plonk{} verifier works as follows
\begin{description}
	\item[Step 1] Validate all obtained group elements.
	\item[Step 2] Validate all obtained field elements.
	\item[Step 3] Validate the instance
      $\inp = \smallset{\wit_i}_{i = 1}^\instsize$.
	\item[Step 4] Compute challenges $\beta, \gamma, \alpha, \chz, v,
      u$ from the transcript.
	\item[Step 5] Compute zero polynomial evaluation
      $\p{Z_H} (\chz) =\chz^\noofc - 1$.
	\item[Step 6] Compute Lagrange polynomial evaluation
      $\lag_1 (\chz) = \frac{\chz^\noofc -1}{\noofc (\chz - 1)}$.
	\item[Step 7] Compute public input polynomial evaluation
      $\pubinppoly (\chz) = \sum_{i \in \range{1}{\instsize}} \wit_i
      \lag_i(\chz)$.
	\item[Step 8] Compute quotient polynomials evaluations
	\begin{multline*}
    \p{t} (\chz) = \frac{1}{\p{Z_H}(\chz)} \Big(
    \p{r} (\chz) + \pubinppoly(\chz) - (\p{a}(\chz) + \beta \p{S_{\sigma 1}}(\chz) + \gamma) (\p{b}(\chz) + \beta \p{S_{\sigma 2}}(\chz) + \gamma) \\
    (\p{c}(\chz) + \gamma)\p{z}(\chz \omega) \alpha - \lag_1 (\chz) \alpha^2
    \Big) \,.
	\end{multline*}
	\item[Step 9] Compute batched polynomial commitment
	$\gone{D} = v \gone{r} + u \gone {z}$ that is
	\begin{align*}
		\gone{D} & = v
		\left(
		\begin{aligned}
          & \p{a}(\chz)\p{b}(\chz) \cdot \gone{\selmulti} + \p{a}(\chz)  \gone{\selleft} + \p{b}  \gone{\selright} + \p{c}  \gone{\seloutput} + \\
          & + (	(\p{a}(\chz) + \beta \chz + \gamma) (\p{b}(\chz) + \beta k_1 \chz + \gamma) (\p{c} + \beta k_2 \chz + \gamma) \alpha  + \lag_1(\chz) \alpha^2)  + \\
			% &   \\
          & - (\p{a}(\chz) + \beta \p{S_{\sigma 1}}(\chz) + \gamma) (\p{b}(\chz)
          + \beta \p{S_{\sigma 2}}(\chz) + \gamma) \alpha \beta \p{z}(\chz
          \omega) \gone{\p{S_{\sigma 3}}(\chi)})
		\end{aligned}
		\right) + \\
		& + u \gone{\p{z}(\chi)}\,.
	\end{align*}
	\item[Step 10] Computes full batched polynomial commitment $\gone{F}$:
	\begin{align*}
      \gone{F} & = \left(\gone{\p{t_{lo}}(\chi)} + \chz^\noofc \gone{\p{t_{mid}}(\chi)} + \chz^{2 \noofc} \gone{\p{t_{hi}}(\chi)}\right) + u \gone{\p{z}(\chi)} + \\
               & + v
                 \left(
		\begin{aligned}
			& \p{a}(\chz)\p{b}(\chz) \cdot \gone{\selmulti} + \p{a}(\chz)  \gone{\selleft} + \p{b}(\chz)   \gone{\selright} + \p{c}(\chz)  \gone{\seloutput} + \\
			& + (	(\p{a}(\chz) + \beta \chz + \gamma) (\p{b}(\chz) + \beta k_1 \chz + \gamma) (\p{c}(\chz)  + \beta k_2 \chz + \gamma) \alpha  + \lag_1(\chz) \alpha^2)  + \\
			% &   \\
			& - (\p{a}(\chz) + \beta \p{S_{\sigma 1}}(\chz) + \gamma) (\p{b}(\chz) + \beta \p{S_{\sigma 2}}(\chz) + \gamma) \alpha  \beta \p{z}(\chz \omega) \gone{\p{S_{\sigma 3}}(\chi)})
		\end{aligned}
		\right) \\
		& + v^2 \gone{\p{a}(\chi)} + v^3 \gone{\p{b}(\chi)} + v^4 \gone{\p{c}(\chi)} + v^5 \gone{\p{S_{\sigma 1}(\chi)}} + v^6 \gone{\p{S_{\sigma 2}}(\chi)}\,.
	\end{align*}
	\item[Step 11] Compute group-encoded batch evaluation $\gone{E}$
	\begin{align*}
		\gone{E}  = \frac{1}{\p{Z_H}(\chz)} & \gone{
		\begin{aligned}
			& \p{r}(\chz) + \pubinppoly(\chz) +  \alpha^2  \lag_1 (\chz) + \\
			& - \alpha \left( (\p{a}(\chz) + \beta \p{S_{\sigma 1}} (\chz) + \gamma) (\p{b}(\chz) + \beta \p{S_{\sigma 2}} (\chz) + \gamma) (\p{c}(\chz) + \gamma) \p{z}(\chz \omega) \right)
		\end{aligned}
           }\\
      + & \gone{v \p{r}(\chz) + v^2 \p{a}(\chz) + v^3 \p{b}(\chz) + v^4 \p{c}(\chz) + v^5 \p{S_{\sigma 1}}(\chz) + v^6 \p{S_{\sigma 2}}(\chz) + u \p{z}(\chz \omega) }\,.
	\end{align*}
\item[Step 12] Check whether the verification
 % $\vereq_\zkproof(\chi)$
  equation holds
	\begin{multline}
		\label{eq:ver_eq}
		\left( \gone{\p{W_{\chz}}(\chi)} + u \cdot \gone{\p{W_{\chz
                \omega}}(\chi)} \right) \bullet
		\gtwo{\chi} - %\\
		\left( \chz \cdot \gone{\p{W_{\chz}}(\chi)} + u \chz \omega \cdot
          \gone{\p{W_{\chz \omega}}(\chi)} + \gone{F} - \gone{E} \right) \bullet
        \gtwo{1} = 0\,.
	\end{multline}
  The verification equation is a batched version of the verification equation
  from \cite{AC:KatZavGol10} which allows the verifier to check openings of
  multiple polynomials in two points (instead of checking an opening of a single
  polynomial at one point).
\end{description}

\ncase{$\plonk$ simulator $\simulator_\chi(\srs, \td= \chi, \inp)$}\ \newline
The \plonk{} simulator proceeds as an honest prover would, except:
\begin{enumerate}
  \item In the first round, it sets $\wit = (\wit_i)_{i \in \range{1}{3 \noofc}}
    = \vec{0}$, and at random picks $b_1, \ldots, b_9$. Then it proceeds with
    that all-zero witness.
  \item In Round 3, it computes polynomial $\pt(X)$ honestly, however uses
    trapdoor $\chi$ to compute commitments
    $\p{t_{lo}}(\chi), \p{t_{mid}}(\chi), \p{t_{hi}}(\chi)$.
  \end{enumerate}


\section{Additional preliminaries, lemmas and proofs}
\subsection{Dlog assumptions}
\label{sec:dlog_assumptions}
\begin{definition}[$(q_1, q_2)\mhyph\dlog$ assumption]
	Let $\adv$ be a $\ppt$ adversary that gets as input
  $\gone{1, \chi, \ldots, \chi^{q_1}}, \gtwo{1, \chi, \ldots, \chi^{q_2}}$, for
  some randomly picked $\chi \in \FF_p$, then
	\[
		\condprob{\chi \gets \adv(\gone{1, \chi, \ldots, \chi^{q_1}}, \gtwo{1, \chi,
        \ldots, \chi^{q_2} })}{\chi \sample \FF_p} \leq \negl.
	\]
\end{definition}

\begin{definition}[$(q_1, q_2)\mhyph\ldlog$ assumption]
  Let $\adv$ be a $\ppt$ adversary that gets as input
  $\gone{\chi^{-q_1}, \ldots, 1, \chi, \ldots, \chi^{q_1}}, \gtwo{\chi^{-q_2},
    \ldots, 1, \chi, \ldots, \chi^{q_2}}$, for some randomly picked
  $\chi \in \FF_p$, then
	\[
    \condprob{\chi \gets \adv(\gone{\chi^{-q_1}, \ldots, 1, \chi, \ldots,
        \chi^{q_1}}, \gtwo{\chi^{-q_2}, \ldots, 1, \chi, \ldots, \chi^{q_2}
      })}{\chi \sample \FF_p} \leq \negl.
	\]
\end{definition}

\subsection{Uber assumption}
\label{sec:uber_assumption}
\ourpar{BBG uber assumption.}
Also, to be able to show computational honest verifier zero knowledge of
$\plonk$ in the standard model, what is required by our reduction, we rely on the
\emph{uber assumption} introduced by Boneh et
al.~\cite{EC:BonBoyGoh05} as presented by Boyen in \cite{PAIRING:Boyen08}.

Let $r, s, t, c \in \NN \setminus \smallset{0}$, Consider vectors of polynomials
$\pR \in \FF_p[X_1, \ldots, X_c]^r$, $\pS \in \FF_p[X_1, \ldots, X_c]^s$ and
$\pT \in \FF_p[X_1, \ldots, X_c]^t$. Write $\pR = \left( \p{r}_1, \ldots,
  \p{r}_r \right)$, $\pS = \left( \p{s}_1, \ldots, \p{s}_s \right)$ and $\pT =
\left( \p{t}_1, \ldots, \p{t}_t \right)$ for polynomials $\p{r}_i, \p{s}_j,
\p{t}_k$.

For a function $f$ and vector $(x_1, \ldots, x_c)$ we write $f(\pR)$ to
denote application of $f$ to each element of $\pR$, i.e.
\(
	f(\pR) = \left( f(\p{r}_1 (x_1, \ldots, x_c), \ldots, f(\p{r}_r
	(x_1, \ldots, x_c) \right).
\)
Similarly for applying $f$ to $\pS$ and $\pT$.

\begin{definition}[Independence of $\pR, \pS, \pT$]
	\label{def:independence}
	Let $\pR, \pS, \pT$ be defined as above. We say that polynomial $\p{f} \in
  \FF_p[X_1, \ldots, X_c]$ is \emph{dependent} on $\pR, \pS, \pT$ if there
  exists $rs + t$ constants $a_{i, j}, b_k$ such that $ \p{f} = \sum_{i = 1}^{r}
  \sum_{j = 1}^{s} a_{i, j} \p{r}_i \p{s}_j + \sum_{k = 1}^{t} b_k \p{t}_k. $ We
  say that $\p{f}$ is \emph{independent} if it is not dependent.
\end{definition}

To show (standard-model) zero knowledge of $\plonk$ we utilize a generalization
of Boneh-Boyen-Goh's \emph{uber assumption} \cite{EC:BonBoyGoh05} stated as
follows (the changed element has been put into a \dbox{dashbox})
\begin{definition}[$(\pR, \pS, \pT, \p{F}, 1)$-uber assumption]
	\label{def:uber_assumption}
	Let $\pR, \pS, \pT$ be defined as above,
    $(x_1, \ldots, x_c, y_1, \ldots, y_{d}) \sample \FF_p^{c + d}$ and let
    $\p{F}$ be a cardinality-$d$ set of pair-wise independent polynomials which are also
    independent of $(\pR, \pS, \pT)$, cf.~\cref{def:independence}.  Then, for
    any $\ppt$ adversary $\adv$
	\begin{multline*}
      \Pr\left[\adv(\gone{\pR(x_1, \ldots x_c)}, \gtwo{\pS(x_1, \ldots, x_c)},
		\gtar{\pT(x_1, \ldots, x_c)}, \dboxed{\gone{\p{F}(x_1, \ldots, x_c)}}) = 1\right] \approx_\secpar \\
      \Pr\left[\adv(\gone{\pR(x_1, \ldots x_c)}, \gtwo{\pS(x_1, \ldots, x_c)},
        \gtar{\pT(x_1, \ldots, x_c)}, \dboxed{\gone{y_1, \ldots, y_{d}}}) =
        1\right].
	\end{multline*}
  \end{definition}

  Compared to the original uber assumptions, there are two major changes. First,
  we require not target group $\GRP_T$ elements to be indistinguishable, but
  elements of $\GRP_1$. Second, Boneh et al.'s assumption works for
  distinguishers who are given only one challenge polynomial $\p{f}$,
  i.e.~$\abs{\p{F}} = 1$.
  
We show security of our version of the uber assumption using the generic group
model as introduced by Shoup \cite{EC:Shoup97} where all group elements are
represented by random binary strings of length $\secpar$. That is, there are
random encodings $\xi_1, \xi_2, \xi_T$ which are injective functions from
$\ZZ_p^+$ to $\bin^{\secpar}$. We write
$\GRP_i = \smallset{\xi_i(x) \mid x \in \ZZ_p^+}$, for
$i \in \smallset{1, 2, T}$. For the sake of clarity  we denote by $\xi_{i, j}$
the $j$-th encoding in group $\GRP_i$.

Let
$\p{P}_i = \smallset{p_1, \ldots, p_{\tau_i}} \subset \FF_p[X_1, \ldots, X_n]$,
for $i \in \smallset{1, 2, T}, \tau_i, n \in \NN$, be sets of multivariate
polynomials. Denote by $\p{P}_i(x_1, \ldots, x_n)$ a set of evaluations of
polynomials in $\p{P_i}$ at $(x_1, \ldots, x_n)$. Denote by
$L_i = \smallset{(p_j, \xi_{i, j}) \mid j \leq \tau_i}$.

Let $\adv$ be an algorithm that is given encodings $\xi_{i, j_i}$ of polynomials
in $\p{P}_i$ for $i \in \smallset{1, 2, T}, j_i = \tau_i$. There is an oracle $\oracleo$
that allows to perform $\adv$ the following queries:
\begin{description}
\item[Group operations in $\GRP_1, \GRP_2, \GRP_T$:] On input
  $(\xi_{i, j}, \xi_{i, j'}, i, op)$, $j, j' \leq \tau_i$,
  $op \in \smallset{\msg{add}, \msg{sub}}$, $\oracleo$ sets $\tau'_i \gets \tau_i + 1$,
  computes
  $p_{i, \tau'_i} = p_{i, j}(x_1, \ldots, x_n) \pm p_{i, j'}(x_1, \ldots, x_n)$
  respectively to $op$. If there is an element  $p_{i, k} \in L_i$ such 
  that $p_{i, k} = p_{\tau'_i}$, then the oracle returns encoding of $p_{i,
    k}$. Otherwise it sets the encoding $\xi_{i, \tau'_i}$ to a new unused
  random string, adds $(p_{i, \tau'_i}, \xi_{i, \tau'_i})$ to $L_i$, and returns
  $\xi_{i, \tau'_i}$.
\item[Bilinear pairing:] On input $(\xi_{1, j}, \xi_{2, j'})$ the oracle sets
  $\tau' \gets \tau_T + 1$ and computes
  $r_{\tau'} \gets p_{i, j}(x_1, \ldots, x_n) \cdot p_{i, j'}(x_1, \ldots,
  x_n)$. If $r_{\tau'} \in L_T$ then return encoding found in the list $L_T$,
  else pick a new unused random string and set $\xi_{T, \tau'}$ to it. Return
  the encoding to the algorithm.
\end{description}

Given that, we are ready to show security of our variant of the Boneh et
al.~uber assumption. The proof goes similarly to the original proof given in
\cite{EC:BonBoyGoh05} with minor differences.

\begin{theorem}[Security of the uber assumption]
  \label{thm:uber_assumption}
  Let $\p{P}_i \in \FF_p[X_1, \ldots, X_n]^{m_i}$, for
  $i \in \smallset{1, 2, T}$ be $\tau_i$ tuples of $n$-variate polynomials over
  $\FF_p$ and let $\p{F} \in \FF_p[X_1, \ldots, X_n]^m$. Let
  $\xi_0, \xi_1, \xi_T$, $\GRP_1, \GRP_2, \GRP_T$ be as defined above. If
  polynomials $f \in \p{F}$ are pair-wise independent and are independent of
  $\p{P}_1, \p{P}_2, \p{P}_T$, then for any $\adv$ that makes up to $q$ queries to the
  GGM oracle holds:
  \begin{equation*}
    \begin{split}
     \left|\,
    \Pr\left[
    \adv\left(
      \begin{aligned}
        \xi_1(\p{P}_1(x_1, \ldots, x_n)), \\
        \xi_2(\p{P}_2(x_1, \ldots, x_n)), \\
        \xi_T(\p{P}_T(x_1, \ldots, x_n)), \\
        \xi_{1}(\p{F}_0), \xi_{1}(\p{F}_1)
      \end{aligned}
    \right) = b
    \, \left|\,
      \begin{aligned}
        x_1, \ldots, x_n, y_1, \ldots, y_m \sample \FF_p,\\
        b \sample \bin, \\
        \p{F}_b \gets \p{F}(x_1, \ldots, x_n),\\
        \p{F}_{1 - b} \gets (y_1, \ldots, y_m)
      \end{aligned}
    \right.  \right] - \frac{1}{2} \, \right| \\
     \leq \frac{d(q + m_1 + m_2 + m_T +
      m)^2 }{2p}
    \end{split}
  \end{equation*}
\end{theorem}
\begin{proof}
  Let $\cdv$ be a challenger that plays with $\adv$ in the following
  game. $\cdv$ maintains three lists
  \[
    L_i = \smallset{(p_j, \xi_{i, j}) \mid j \in \range{1}{\tau_i}},
  \]
  for $i \in \smallset{1, 2, T}$. Invariant $\tau$ states that
  $\tau_1 + \tau_2 + \tau_T = \tau + m_1 + m_2 + m$.

  Challenger $\cdv$ answers $\adv$'s oracle queries. However, it does it a bit
  differently that the oracle $\oracleo$ would:
  \begin{description}
  \item[Group operations in $\GRP_1, \GRP_2, \GRP_T$:] On input
    $(\xi_{i, j}, \xi_{i, j'}, i, op)$, $j, j' \leq \tau_i$,
    $op \in \smallset{\msg{add}, \msg{sub}}$, $\cdv$ sets
    $\tau' \gets \tau_i + 1$, computes
    $p_{i, \tau'}(X_1, \ldots, X_n) = p_{i, j}(X_1, \ldots, X_n) \pm p_{i,
      j'}(X_1, \ldots, X_n)$ respectively to $op$. If there is a polynomial
    $p_{i, k}(X_1, \ldots, X_n) \in L_i$ such that
    $p_{i, k}(X_1, \ldots, X_n) = p_{\tau'}(X_1, \ldots, X_n)$, then the
    challenger returns encoding of $p_{i, k}$. Otherwise it sets the encoding
    $\xi_{i, \tau'}$ to a new unused random string, adds
    $(p_{i, \tau'}, \xi_{i, \tau'})$ to $L_i$, and returns $\xi_{i, \tau'}$.
  \item[Bilinear pairing:] On input $(\xi_{1, j}, \xi_{2, j'})$ the challenger
    sets $\tau' \gets \tau_T + 1$ and computes
    $r_{\tau'}(X_1, \ldots, X_n) \gets p_{i, j}(X_1, \ldots, X_n) \cdot p_{i,
      j'}(X_1, \ldots, X_n)$. If $r_{\tau'}(X_1, \ldots, X_n) \in L_T$, $\cdv$
    returns encoding found in the list $L_T$. Else it picks a new unused random
    string and set $\xi_{T, \tau'}$ to it. Finally it returns the encoding to
    the algorithm.
\end{description}
  
After at most $q$ queries to the oracle, the adversary returns a bit $b'$. At
that point the challenger $\cdv$ chooses randomly $x_1, \ldots, x_n, y_1 \ldots, y_m$,
random bit $b$, and sets $X_i = x_i$, for $i \in \range{1}{n}$, and $Y_i = y_i$,
for $i \in \range{1}{m}$; furthermore, $\p{F}_b \gets \p{F}(x_1, \ldots, x_n)$
and $\p{F}_{1 - b} \gets (y_1, \ldots, y_m)$. Note that $\cdv$ simulates
perfectly unless the chosen values $x_1, \ldots, x_n, y_1, \ldots, y_m$ result
in equalities between polynomial evaluations that are not equalities between the
polynomials. That is, the simulation is perfect unless for some $i, j, j'$ holds
\[
  p_{i, j}(x_1, \ldots, x_n) - p_{i, j'}(x_1, \ldots, x_n) = 0,
  \]
  for $p_{i, j}(X_1, \ldots, X_n) \neq p_{i, j'}(X_1, \ldots, X_n)$.  Denote by
  $\bad$ an event that at least one of the three conditions holds. When $\bad$
  happens, the answer $\cdv$ gives to $\adv$ differs from an answer that a real
  oracle would give. We bound the probability that $\bad$ occurs in two steps.

  First we set $\p{F}_b = \p{F}(X_1, \ldots, X_n)$. Note that symbolic
  substitutions do not introduce any new equalities in $\GRP_1$. That is, if for
  all $j, j'$ holds $p_{1, j} \neq p_{1, j'}$, then $p_{1, j} \neq p_{1, j'}$
  even after setting $\p{F}_b = \p{F}(X_1, \ldots, X_n)$. This follows since all
  polynomials in $\p{F}$ are pairwise independent and $\p{F}$ independent on
  $\p{P}_1, \p{P}_2, \p{P}_T$. Indeed, $p_{1, j} - p_{1, j'}$ is a polynomial of
  the form
  \[
    \sum_{j = 1}^{m_1}a_j p_{1, j} + \sum_{j = 1}^{m} b_j f_j (X_1, \ldots, X_n),
  \]
  for some constants $a_j, b_j$. If the polynomial is non-zero, but setting
  $\p{F}_b = \p{F}(X_1, \ldots, X_n)$ makes this polynomial vanish, then some
  $f_k$ must be dependent on some $\p{P}_1, \p{F} \setminus \smallset{f_k}$.

  Now we set $X_1 \ldots, X_n, \p{F}_{1 - b}$ and bound probability that for
  some $i$ and $j, j'$ holds
  $(p_{i, j}(x_1, \ldots, x_n) - p_{i, j'}(x_1, \ldots, x_n) = 0$ for
  $p_{i, j} \neq p_{i, j'}$. By the construction, the maximum total degree of
  these polynomials is
  $d = \max(d_{\p{P}_1}+ d_{\p{P}_2}, d_{\p{P}_T}, d_{\p{F}})$, where $d_f$ is
  the total degree of some polynomial $f$ and for a set of polynomials
  $F = \smallset{f_1, \ldots, f_k}$, we write
  $d_F = \smallset{d_{f_1}, \ldots, d_{f_k}}$. Thus, for a given $j, j'$ probability that a random assignment to
  $X_1, \ldots, X_n, Y_1, \ldots, Y_n$ is a root of $p_{i, j} - p_{i, j'}$ is,
  by the Schwartz-Zippel lemma, bounded by $\infrac{d}{p}$, which is
  negligible. There is at most $2 \cdot {q + m_0 + m_1 + m  \choose 2}$ such
  pairs $p_{i, j}, p_{i, j'}$ we have that
  \[
    \prob{\bad} \leq  {q + m_0 + m_1 + m  \choose 2} \cdot \frac{2d}{p} \leq (q
    + m_0 + m_1 + m)^2 \frac{d}{p}.
  \]

  As noted, if $\bad$ does not occur then the simulation is perfect. Also the
  bit $b$ has been chosen independently on the $\adv$'s view, thus $\condprob{b
    = b'}{\neg \bad} = \infrac{1}{2}$. Hence,
  \[
    \begin{aligned}
      \prob{b = b'} & \leq \condprob{b = b'}{\neg \bad}(1 - \prob{\bad}) + \prob{\bad} =
      \frac{1}{2} + \frac{\prob{\bad}}{2} \\
      \prob{b = b'} & \geq \condprob{b = b'}{\neq \bad}(1 - \prob{\bad}) =
      \frac{1}{2} - \frac{\prob{\bad}}{2}.
    \end{aligned}
  \]
  Finally,
  \[
    \abs{\Pr[b = b'] - \frac{1}{2}} \leq \prob{\bad}/2 \leq (q
    + m_0 + m_1 + m)^2 \frac{d}{2p}
  \]
  as required.
\end{proof}

\subsection{Special simulation-extractability of sigma protocols and forking lemma}
\label{sec:forking_lemma}
\begin{theorem}[Special simulation extractability of the Fiat--Shamir transform
  \cite{INDOCRYPT:FKMV12}]
	Let $\sigmaprot = (\prover, \verifier, \simulator)$ be a non-trivial sigma
  protocol with unique responses for a language $\LANG \in \npol$. In the random
  oracle model, the NIZK proof system $\sigmaprot_\fs = (\prover_\fs,
  \verifier_\fs, \simulator_{\fs})$ resulting by applying the Fiat--Shamir
  transform to $\sigmaprot$ is special simulation extractable with extraction error
  $\eta = q/h$ for the simulator $\simulator$. Here, $q$ is the number of random
  oracle queries and $h$ is the number of elements in the range of $\ro$.
\end{theorem}

The theorem relies on the following \emph{general forking lemma} \cite{JC:PoiSte00}.

\begin{lemma}[General forking lemma, cf.~\cite{INDOCRYPT:FKMV12,CCS:BelNev06}]
	\label{lem:forking_lemma}
	Fix $q \in \ZZ$ and a set $H$ of size $h > 2$. Let $\zdv$ be a $\ppt$
  algorithm that on input $y, h_1, \ldots, h_q$ returns $(i, s)$, where $i
  \in\range{0}{q}$ and $s$ is called a \emph{side output}. Denote by $\ig$ a
  randomised instance generator. We denote by $\accProb$ the probability
	\[
		\condprob{i > 0}{y \gets \ig; h_1, \ldots, h_q \sample H; (i, s) \gets
		\zdv(y, h_1, \ldots, h_q)}\,.
	\]
	Let $\forking_\zdv(y)$ denote the algorithm described in
  \cref{fig:forking_lemma}, then the probability $\frkProb$ defined as $
  \frkProb := \condprob{b = 1}{y \gets \ig; (b, s, s') \gets \forking_{\zdv}(y)}
  $ holds
	\[
		\frkProb \geq \accProb \brak{\frac{\accProb}{q} - \frac{1}{h}}\,.
	\]
	%
	\begin{figure}
		\centering
		\fbox{
		\procedure{$\forking_\zdv (y)$}
		{
			\rho \sample \RND{\zdv}\\
			h_1, \ldots, h_q \sample H\\
			(i, s) \gets \zdv(y, h_1, \ldots, h_q; \rho)\\
			\pcif i = 0\ \pcreturn (0, \bot, \bot)\\
			h'_{i}, \ldots, h'_{q} \sample H\\
			(i', s') \gets \zdv(y, h_1, \ldots, h_{i - 1}, h'_{i}, \ldots,  h'_{q};
			\rho)\\
			\pcif (i = i') \land (h_{i} \neq h'_{i})\ \pcreturn (1, s, s')\\
			\pcind \pcelse \pcreturn (0, \bot, \bot)
		}}
		\caption{Forking algorithm $\forking_\zdv$}
		\label{fig:forking_lemma}
\end{figure}
\end{lemma}

\subsection{Proof of the generalized forking lemma (\cref{lem:generalised_forking_lemma})}
\label{sec:forking_proof}
\begin{proof}
First denote by $\accProb(y)$ and $\frkProb(y)$ the following probabilities
\begin{align*}
\accProb(y) & =  \condprob{i \neq 0}{h_1, \ldots, h_q \sample H;\ (i, s)
\gets \zdv(y, h_1, \ldots, h_q)}\,.\\
	\frkProb(y) & = \condprob{b = 1}{(b, \vec{s}) \gets
\genforking_{\zdv}^{m}(y, h_1, \ldots, h_q)}\,.
\end{align*}

We start by claiming that for all $y$ 
\begin{equation}\label{eq:frkProb_y}
	\frkProb(y) \geq 
	\frac{\accProb(y)^m}{q^{m - 1}} - \accProb(y) \cdot \left(1 -
  \frac{h!}{(h - m)! \cdot h^{m}}\right)
	\end{equation}
Then with the expectation taken over $y \sample \ig$, we have
\begin{align}
	\frkProb & = \expected{\frkProb(y)} \geq
	\expected{\frac{\accProb(y)^m}{q^{m - 1}} -  \accProb(y) \cdot \left(1 -
  \frac{h!}{(h - m)! \cdot h^{m}}\right)} \label{eq:use_eq1}\\
	& \geq \frac{\expected{\accProb(y)}^m}{q^{m - 1}} -
	\expected{\accProb(y)} \cdot \left(1 - \frac{h!}{(h - m)! \cdot
  h^{m}}\right) \label{eq:by_lemma_jensen}\\
	& = \frac{\accProb^m}{q^{m - 1}} -  \accProb \cdot \left(1 -
  \frac{h!}{(h - m)! \cdot h^{m}}\right)\label{eq:by_accProb}\,.
\end{align}
Where \cref{eq:use_eq1} comes from \cref{eq:frkProb_y};
\cref{eq:by_lemma_jensen} comes from linearity of expected value and \cref{lem:jensen}; and
\cref{eq:by_accProb} holds by the fact that $\expected{\accProb(y)} =
\accProb$.

We now show \cref{eq:frkProb_y}.
Denote by $J = \range{1}{m}^2 \setminus \smallset{(j, j)}_{j \in \range{1}{m}}$. 
For any input $y$, with probabilities taken over the coin tosses of
$\genforking_{\zdv}^{m}$ we have
\begin{align*}
	\frkProb (y) & = \prob{i_j = i_{j'} \land i_j \geq 1 \land
h_{i_j}^{j} \neq h_{i_{j'}}^{j'} \text{ for } (j, j') \in J}	\\
	& \geq \prob{i_j = i_{j'} \land i_j \geq 1 \text{ for } (j, j') \in J} %\\
   - \prob{i_j \geq 1 \land h_{i_j}^{j} = h_{i_{j'}}^{j'} \text{ for some } (j, j') \in J}\\
	& = \prob{i_j = i_{j'} \land i_j \geq 1 \text{ for } (j, j') \in J} -
	\prob{i_j \geq 1} \cdot 
  \left(1 - \frac{h!}{(h - m)! \cdot h^{m}}\right) \\ 
	& = \prob{i_j = i_{j'} \land
	i_j \geq 1 \text{ for } (j, j') \in J} - \accProb(y) \cdot \left(1 -
\frac{h!}{(h - m)! \cdot h^{m}}\right)\,.
\end{align*}

Probability that for some $(j, j') \in J$ and $i_j = i_{j'}$ holds
$h_{i_j}^{j} \neq h_{i_{j'}}^{j'}$ equals 
\[
  \frac{h \cdot (h - 1)
\cdot \ldots \cdot (h - m - 1)}{h^m} = \frac{h!}{(h - m)! \cdot h^m}.
\]
That is, it equals the number
of all $m$-element strings where each element is different divided by
the number of all $m$-element strings, where elements are taken from a
set of size $h$. 

It remains to show that $\prob{i_j = i_{j'} \land i_j \geq 1 \text{ for } (j,
  j') \in J} \geq \infrac{\accProb(y)^m}{q^{m - 1}}$. Let $\RND{\zdv}$ denote
the set from which $\zdv$ picks its coins at random. For each $\iota \in
\range{1}{q}$ let $X_\iota \colon \RND{\zdv} \times H^{\iota - 1} \to [0, 1]$ be
defined by setting $X_\iota(\rho, h_1, \ldots, h_{\iota - 1})$ to
\[
  \condprob{i = \iota}{h_\iota, \ldots, h_q \sample H; (i, s) \gets \zdv(y, h_1,
    \ldots, h_q; \rho)}
\]
for all $\rho \in \RND{\zdv}$ and $h_1, \ldots, h_{\iota - 1} \in H$. Consider
$X_\iota$ be a random variable over the uniform distribution on its domain. Then
\begin{align*}
	& \prob{i_j = i_{j'} \land i_j \geq 1 \text{ for } (j, j') \in J} 
	 = \sum_{\iota = 1}^{q} \prob{i_1 = \iota \land \ldots \land i_m = \iota} \\
	& = \sum_{\iota = 1}^{q} \prob{i_1 = \iota} \cdot \condprob{i_2 = \iota}{i_1 = \iota} \cdot \ldots \cdot \condprob{i_m = \iota}{i_1 = \ldots = i_{m - 1} = \iota} \\
	& = \sum_{\iota = 1}^{q} \sum_{\rho, h_1, \ldots, h_{\iota - 1}} X_{\iota}
   (\rho, h_1, \ldots, h_{\iota - 1})^{m} \cdot \frac{1}{\abs{\RND{\zdv}} \cdot \abs{H}^{\iota - 1}}
   = \sum_{\iota = 1}^{q} \expected{X_\iota^m} \,.
\end{align*}
Importantly, $\sum_{\iota = 1}^q \expected{X_{\iota}} = \accProb(y)$.

By \cref{lem:jensen} we get
\[
	\sum_{\iota = 1}^{q} \expected{X_\iota^m} \geq \sum_{\iota = 1}^{q} \expected{X_\iota}^m\,.
\]
Note that for e.g.~$X_i = 1$, $i \in \range{1}{q}$ the inequality becomes equality, that is, it is tight.
 
We now use the H\"older inequality, cf.~\cref{lem:holder}, for $x_i = \expected{X_i}$, $y_i = 1$, $p = m$, and $q = m/(m - 1)$ obtaining
\begin{gather}
	\left(\sum_{i = 1}^{q} \expected{X_i}\right)^{m}  \leq \left(\sum_{i = 1}^{q} \expected{X_i}^m\right) \cdot q^{m - 1}\\
	\frac{1}{q^{m - 1}} \cdot \accProb(y)^{m} \leq \sum_{i = 1}^{q} \expected{X_i}^m\,.
\end{gather}
Finally, we get
\[
	\frkProb(y) \geq \frac{\accProb(y)^m}{q^{m - 1}} - 
	 \accProb(y) \cdot \left(1 - \frac{h!}{(h - m)! \cdot h^m}\right)\,.
\]
\qed
\end{proof}
\begin{lemma}\label{lem:jensen}
	Let $\RND{\zdv}$ denote the set from which $\zdv$ picks its coins at random.
	For each $\iota \in \range{1}{q}$ let $X_\iota \colon \RND{\zdv} \times
	H^{\iota - 1} \to [0, 1]$ be defined by setting $X_\iota(\rho, h_1, \ldots,
h_{\iota - 1})$ to 
\[
  \condprob{i = \iota}{h_\iota, \ldots, h_q \sample H; (i, s) \gets \zdv(y, h_1,
    \ldots, h_q; \rho)}
	\] 
	for all $\rho \in \RND{\zdv}$ and $h_1, \ldots, h_{\iota - 1} \in H$. Consider
  $X_\iota$ as a random variable over the uniform distribution on its domain.
  Then $\expected{X_\iota^m} \geq \expected{X_\iota}^m$.
\end{lemma}
\begin{proof}
	First we recall the Jensen inequality \cite{W:Weissten20}, if for some random
  variable $X$ holds $\abs{\expected{X}} \leq \infty$ and $f$ is a Borel convex
  function then
	\[
		f(\expected{X}) \leq \expected{f(X)}\,.
	\] 
	Finally, we note that $\abs{\expected{X}} \leq \infty$ and taking to the
  $m$-th power is a Borel convex function on $[0, 1]$ interval. \qed
\end{proof}

\begin{lemma}[H\"older's inequality. Simplified.]\label{lem:holder}
	Let $x_i, y_i$, for $i \in \range{1}{q}$, and $p, q$ be real numbers such that
  $1/p + 1/q = 1$. Then
	\begin{equation}
    \label{eq:tightness}
		\sum_{i = 1}^{q} x_i y_i \leq \left(\sum_{i = 1}^{q}
      x_i^p\right)^{\frac{1}{p}} \cdot \left(\sum_{i = 1}^{q}
      y_i^p\right)^{\frac{1}{q}}\,.
	\end{equation}
\end{lemma}

\begin{remark}[Tightness of the H\"older inequality]
	In is important to note that Inequality (\ref{eq:tightness}) is tight. More
  precisely, for $\expected{X_i} = x$, $i \in \range{1}{q}$ we have
	\begin{gather*}
		\sum_{i = 1}^q x = \left(\sum_{i = 1}^{q} x^m\right)^\frac{1}{m} \cdot \left(\sum_{i = 1}^{q} 1^{\frac{m}{m - 1}}\right)^{\frac{m - 1}{m}} \\
		qx = \left(qx^m\right)^\frac{1}{m} \cdot q^{\frac{m - 1}{m}} \\
		(qx)^m = qx^m \cdot q^{m - 1} \\
		(qx)^m = (qx)^m\,.
	\end{gather*}
\end{remark}

\begin{lemma}
  \label{lem:root_prob}
  Let $\p{f}(X)$ be a random degree-$d$ polynomial over $\FF_p[X]$. Then the
  probability that $\p{f}(X)$ has roots in $\FF_p$ is at least $\infrac{1}{d!}$.
\end{lemma}
\begin{proof}
  First observe that there is $p^{d}$ canonical polynomials in $\FF_p[X]$.  Each
  of the polynomials may have up to $d$ roots. Consider polynomials which are
  reducible to polynomials of degree $1$, i.e.~polynomials that have all $d$
  roots. The roots can be picked in $\bar{C}^{p}_{d}$ ways, where
  $\bar{C}^{n}_{k}$ is the number of $k$-elements combinations with repetitions
  from $n$-element set. That is,
  \[
    \bar{C}^n_k = \binom{n + k - 1}{k}\,.
  \]
  Thus, the probability that a randomly picked polynomial has all $d$ roots is
  \begin{multline*}
    p^{-d} \cdot \bar{C}^p_d = p^{-d} \cdot \binom{p + d - 1}{d} =
    p^{-d} \cdot \frac{(p + d - 1)!}{(p + d - 1 - d)! \cdot d!} = \\
    p^{-d} \cdot \frac{(p + d - 1) \cdot \ldots \cdot p \cdot (p - 1)!}{(p - 1)!
      \cdot d!} = p^{-d} \cdot \frac{(p + d - 1)\cdot
      \ldots \cdot p}{d!} \\
    \geq p^{-d} \cdot {\frac{p^d}{d!}} = \frac{1}{d!}
  \end{multline*}
  \qed
\end{proof}

\input{updatable}

\section{From a late updatable SE to SE}
\michals{13.09}{Here I want to show an idea of a reduction of a late updatable
  SE to SE}

Let $\plonk$ be a forking simulation extractable proof system. Let $\bdv$ be a
SE adversary. Let $\adv$ be an \emph{algebraic} LUSE adversary. We show that existence of an
extractor for $\bdv$, $\ext_\bdv$, assures existence of $\ext_\adv$ -- extractor
for $\adv$.

Here we simplify $\plonk$ a bit and assume that polynomials are written in
standard bases instead of Lagrange. That is, for instance, $\p{a} (X) = \sum_{i
  = 0}^n a_i X^i + \ZERO_\HHH (X) (b_1 + b_2 X)$
 
The argument goes as follows.  Let $\srs_0 \to \srs_1 \to \ldots \srs_n$ be the
sequence of SRS co-produced by $\adv$. Let $\srs_k$ be the last SRS
honestly computed.

The adversary $\bdv$ proceeds as follows:
\begin{enumerate}
\item Get as input an SRS $srs$ honestly created using some trapdoor $\chi$.
\item Internally run adversary $\adv$. Process $\adv$'s SRS update queries.
\item Guess index $k$ of the last honest update, set $\srs_k = \srs$. (From now
  on we will denote this SRS by $\srs$.)
\item Let $\adv$ update $\srs$ according to its wishes. Let's the final SRS be
  $\srs_n$ computed using some trapdoor $\alpha \chi$. ($\alpha$ picked by $\adv$)
\item Since $\adv$ is algebraic, $\bdv$ learns $\alpha$.
\item $\bdv$ processes $\adv$ simulation queries (that should be done according
  to $\srs_n$).
  \begin{enumerate}
  \item On instance and witness $\wit'$ (note that in $\plonk$ the instance is a
    part of the witness) compute $\wit$ such that $\wit_i = \alpha^i \wit_i$,
    for $i = 1 .. (n - 1)$; ($\bdv$ gives as instance/witness the coefficients
    of some polynomial $\p{\tilde{a}'}$ which commitment is its evaluation at
    $\alpha \chi$. We need to express it the basis related to $\chi$ as the
    obtained polynomial $\p{\tilde{a}}$ will be committed at $\chi$. We observe
    $\p{\tilde{a}'} (\alpha \chi) = \p{\tilde{a}} (\chi)$, for some $\p{\tilde{a}}$ we are now
    computing) $\wit_{i + n} = \alpha^i \wit_{i + n}$,
    $\wit_{i + 2n} = \alpha^i \wit_{i+ 2n}$ (the last two translations are
    since we need to compute $\p{\tilde{b}}$ from $\p{\tilde{b}'}$ and $\p{\tilde{c}}$ from $\p{\tilde{c}'}$)
  \item Provide $\wit$ to the simulator.
  \item Get simulated proof $\gone{\p{a} (\chi), \p{b} (\chi), \p{c} (\chi)}$
    , $\beta, \gamma$, $\p{z} (\chi)$,
    $\alpha$, $\p{t} (\chi)$, $\chz$,
    $\p{a} (\chz), \ldots, \p{t} (\chz), \p{z} (\chz \omega)$, $\delta$. The
    simulator (which uses trapdoor, that's fine here and simplifies things)
    simply takes random polynomials $\p{a}, \p{b}, \p{c}$, random challenges,
    compute $\p{z}, \p{t}$ in regard with the picked polynomials and
    challenges. Commits to polynomials. $\simulator$ uses trapdoor to commit to
    $\p{t}$ (it is infeasible for an adversary which doesn't know the instance's
    witness.)
  \item Translate the proof.
    \begin{enumerate}
    \item Since simulator for $\adv$ would pick random polynomials $\p{a'},
      \p{b'}, \p{c'}$ and send $\gone{\p{a'} (\alpha \chi), \p{b'} (\alpha
        \chi), \p{c'} (\alpha \chi)}$. Just send $\p{a} (\chi), \p{b} (\chi),
      \p{c} (\chi)$. That is $\p{a'} (\alpha \chi) = \p{a} (\chi)$ and 
      $\p{a'} (\alpha X) = a'_0 + a'_1( \alpha X) + \ldots + a'_n (\alpha^n
      X^n)= a_0 + a_1 \alpha X + \ldots + (a_n \alpha^n) X^n$. Output
      commitments to primed polynomials.
    \item The polynomials are the same, hence RO answers $\beta, \gamma$ are the
      same as well.
    \item Polynomial $\p{z}$ can also be picked by the simulator at random,
      hence $\p{z'} (\alpha \chi) := \p{z} (\chi)$. Output $\gone{\p{z'}
        (\alpha \chi)}$
    \item Again, the random oracle response is the same: $\alpha$.
    \item Since $\p{t'} (X)$ is determined by $\p{a'} (X), \p{b'} (X), \p{c'}
      (X), \p{z'} (X)$ and some publicly known polynomials, and $\gone{\p{a'}
        (\alpha \chi)} = \gone{\p{a} (\chi)}, \ldots$ we set $\p{t'} (\alpha
      \chi) = \p{t} (\chi)$ and output $\gone{\p{t'} (\alpha \chi)}$.
    \item Get evaluation challenge $\chz$ and compute $\chz' = \chz \alpha$
      (need to adjust evaluation point to have the same evaluation values for
      primed and non-primed polynomials). 
    \item Get evaluations $\p{a} (\chz), \ldots$; output them as evaluations
      $\p{a'} (\chz'), \ldots$.
    \item Get the opening challenge $\nu$ \emph{program $\adv$'s oracle to
        output $\nu$} \michals{13.09}{we changed the partial transcript hence we need to
      program the oracle. Is that a problem?}
  \item Compute evaluations' openings. Observe that (here simplification -- we
    show correctness of evaluation to $\p{a}$ but batched version should work similarly)
    \end{enumerate}
    \begin{align}
      W_{\chz} (\chi) & = \frac{\p{a} (\chi) - \p{a} (\chz)}{\chi - \chz} = \\
                      & \frac{\p{a'} (\alpha \chi) - \p{a'} (\alpha \chz)}{\chi - \chz} = \\
                      & \alpha \frac{\p{a'} (\alpha \chi) - \p{a'} (\alpha \chz)}{\alpha \chi -
                        \alpha \chz} \\
                      & = \alpha W_{\chz \alpha} (\alpha \chi).
    \end{align}
    Hence $\bdv$ sends to $\adv$ opening $W_{\chz \alpha} (\alpha \chi) =
    \frac{1}{\alpha} W_{\chz} (\chi)$.
  \item Also, the verification of the correctness of the opening
    holds. \michals{13.09}{Note, for the batched version we need to query random
    oracle to get batching coefficients. Here we need to program RO again, to
    have the same coeffs.}
\end{enumerate}
\item Now we need to show that proof output by $\adv$ for $\srs_n$ can be
  translated to a proof in $\srs$. This is done similarly to the above. All RO
  queries $\adv$ makes to create the proof are answered honestly by $\bdv$
  except the query that gives challenge $\chz'$. More precisely, when $\adv$
  passes a partial transcript produced by $\adv$ to get $\chz'$ it gets RO's
  answer $\chz$ and set $\chz' = \alpha \chz$.

  Unique response property assures that the output proof will not share the
  first 3 rounds with some simulated proof. Hence we do not need to worry about
  mismatch between the programmed random oracle and the real random
  oracle. (That is, the fact that $\bdv$ programmed RO to have challenge $\chz'$
  instead of $\chz$ will not be noticed.)  Given proof
  $\zkproof' = \gone{\p{a'} (\alpha \chi), \p{b'} (\alpha \chi), \p{c'} (\alpha
    \chi)} \ldots$ for instance $\inp'$, output by $\adv$ adversary $\bdv$
  proceeds as follows
  \begin{enumerate}
  \item Translate $\inp'$ into a corresponding relation in $\srs$: $\inp$. More
    precisely for known $\wit'_i$ set $\wit_i = \wit'_i \alpha^i$,
    $\wit_{i + n} = \wit'_{i + n} \alpha^i$,
    $\wit_{i + 2n} = \wit'_{i + 2n} \alpha^i$ for $i \in \range{1}{\noofc}$.
  \item Get commitments $\gone{\p{a'} (\alpha \chi), \p{b'} (\alpha \chi), \p{c'} (\alpha
      \chi)}$ and pass them to the random oracle as $\gone{\p{a} (\chi), \p{b} (\chi),
    \p{c} (\chi)}$, get challenge $\beta, \gamma$. 
\item Add the commitments and challenges to the proof $\zkproof$.
  \item Set $\beta' = \beta$ and $\gamma' = \gamma$. Pass the challenges to $\adv$.
  \item Get commitment $\gone{\p{z'} (\alpha \chi)}$ and pass it as
    $\gone{\p{z} (\chi)}$, get challenge $\alpha$.
  \item Add the commitments and challenges to the proof $\zkproof$.
  \item Set $\alpha' = \alpha$ and pass it to $\adv$.
  \item Get commitments
    $\gone{\p{t'_{lo}} (\alpha \chi), \p{t'_{mid}} (\alpha \chi), \p{t'_{hi}}
    (\alpha \chi)}$ and pass them the random oracle as
    $\gone{\p{t_{lo}} (\chi), \p{t_{mid}} (\chi), \p{t_{hi}} (\chi)}$. Get
    challenge $\chz$.
  \item Add the commitments and challenges to the proof $\zkproof$.
  \item Set $\chz' = \alpha \chz$ and give $\chz'$ to $\adv$.
  \item Get evaluations $\p{a'} (\chz'), \ldots$.
  \item Add evaluations to the proof $\zkproof$.
  \item Pass the partial transcript to the random oracle and get challenge $\nu$.
  \item Set $\nu' = \nu$ and pass it to $\adv$.
  \item Get polynomial openings $\gone{W_{\chz'} (\alpha \chi)}$ and $\gone{W_{{\chz'}
        \omega} (\alpha \chi)}$.
  \item Set $\gone{W_\chz (\chi)} = \gone{\alpha W_{\chz'} (\alpha \chi)}$, and
    $\gone{W_{\chz \omega} (\chi)} = \gone{\alpha W_{{\chz'}
        \omega} (\alpha \chi)}$.
  \end{enumerate}
\end{enumerate}
Since the $\adv$'s proof is acceptable, $\bdv$'s proof is acceptable as
well. Hence there is an extractor $\ext_\bdv$ that outputs witness $\wit$ given:
$\bdv$, its randomness $r_\bdv$, $Q$ -- the list of simulated proofs, $Q_\ro$ -- the list
of random oracle responses.

$\ext_\adv$ is constructed as follows: $r_\adv = r_\bdv$, $Q'$ -- is a list of
simulated proofs, but w.r.t.~translations $\bdv$ made, $Q'_\ro$ is a list of
random oracle responses, but, as in the case of $Q'$, with changes introduced by $\bdv$.
\end{document}
