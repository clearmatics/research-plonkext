% !TeX spellcheck = en_UK
% \let\accentvec\vec              
\documentclass[runningheads,11pt]{llncs}
 % \documentclass[runningheads]{amsart}
\let\spvec\vec \let\vec\accentvec

\usepackage{amssymb,amsmath} \let\vec\spvec \usepackage{lmodern}

\usepackage[T1]{fontenc}

\newcommand{\iflipics}[1] {} \newcommand{\iflncs}[1] {#1}

\def\vec#1{\mathchoice{\mbox{\boldmath$\displaystyle#1$}}
{\mbox{\boldmath$\textstyle#1$}} {\mbox{\boldmath$\scriptstyle#1$}}
{\mbox{\boldmath$\scriptscriptstyle#1$}}}

\DeclareFontFamily{U}{mathx}{\hyphenchar\font45}
\DeclareFontShape{U}{mathx}{m}{n}{<-> mathx10}{}
\DeclareSymbolFont{mathx}{U}{mathx}{m}{n}
\DeclareMathAccent{\widebar}{0}{mathx}{"73}

% lncs size (as printed in books, with small margins):
 % \usepackage[paperheight=23.5cm,paperwidth=15.5cm,text={13.2cm,20.3cm},centering]{geometry}
 %\usepackage{fullpage}

\newcommand{\ifamsart}[1] {} \ifamsart{ \newtheorem{theorem}{Theorem}%[section]
  \newtheorem{proposition}[theorem]{Proposition}
  \newtheorem{lemma}[theorem]{Lemma} \newtheorem{corollary}[theorem]{Corollary}
  \theoremstyle{definition} \newtheorem{definition}[theorem]{Definition}
  \newtheorem{example}[theorem]{Example} }
\usepackage{soulutf8} \soulregister\cite7 \soulregister\ref7
\soulregister\pageref7 \usepackage{hyperref}
\usepackage[color=yellow]{todonotes} \hypersetup{final} \usepackage{mathrsfs}
\usepackage[advantage,asymptotics,adversary,sets,keys,ff,lambda,primitives,events,operators,probability,logic,mm,complexity]{cryptocode}
%\pcbodylinesep=0.15\baselineskip % MK: got an undefined control sequence error

\usepackage[capitalise]{cleveref}
%\crefname{appendix}{Supp.~Mat.}{Sup.~Mat.}
%\Crefname{appendix}{Supp.~Mat.}{Sup.~Mat.}
\usepackage{cite} 
\usepackage{booktabs}
\usepackage{paralist}
\usepackage[innerleftmargin=5pt,innerrightmargin=5pt]{mdframed}
%\usepackage{setspace}
\usepackage{caption}
\captionsetup{belowskip=0pt}
%\captionsetup[figure]{font={stretch=2}}
% \usepackage{subcaption}
\usepackage{bm}
\usepackage{url}
\usepackage{dirtytalk}
\usepackage[margin=0.7in,a4paper]{geometry}
%\usepackage{fullpage}
\usepackage[normalem]{ulem}
\usepackage{dashbox}
\newcommand\dboxed[1]{\dbox{\ensuremath{#1}}}

\include{macros}

\title{On Simulation-Extractability of Universal zkSNARKs}

%\author{Anonymous submission to PKC}
\author{Markulf Kohlweiss\inst{1,2} \and Michał Zając\inst{3}} 
%\iflncs{
\institute{University of Edinburgh, Edinburgh, UK \and IOHK \\
\email{mkohlwei@inf.ed.ac.uk} \and Clearmatics, London, UK \\
\email{m.p.zajac@gmail.com}} 

\allowdisplaybreaks

\begin{document} \sloppy \maketitle

\begin{abstract} 
	In this paper we show that a wide class of (computationally) special-sound
	proofs of knowledge which have unique response property and are
	standard-model zero-knowledge are (non-black-box) simulation-extractable
	when made non-interactive by the Fiat--Shamir transform.  We prove that two
	efficient updatable universal zkSNARKs---\plonk{}~\cite{EPRINT:GabWilCio19}
	and $\sonic$~\cite{CCS:MBKM19}---meet these requirements and conclude by
	showing their simulation-extractability.  As a side result we also show that
	relying security on rewinding and Fiat--Shamir transform often comes at a
	great price of inefficient (yet still polynomial time) knowledge extraction
	and the security loss introduced by these techniques should always be taken
	into account. 
\end{abstract}

\section{Introduction} 
\subsection{Motivation} 
\paragraph{The rise of updatable
zkSNARKs.} 
In recent years we have seen indisputable progress in building efficient
zero-knowledge proof systems,
e.g.~\cite{AC:Groth10a,TCC:Lipmaa12,EC:GGPR13,SP:PHGR13,AC:Lipmaa13,AC:DFGK14,EC:Groth16,SP:BBBPWM18}
to name a few. Special attention has been devoted to the design of new
\emph{zero-knowledge succinct non-interactive arguments of knowledge} (zkSNARKs)
with short (ideally constant-length) proofs. Succinctness makes zkSNARKs
especially useful in real-life system deployment,
e.g.~\cite{REPO:Zcash20,ARXIV:RonZaj19,REPO:Zeth20,REPO:Celo20,REPO:Aztec20}
what in turn raises a question---\emph{Are zkSNARKs' security models useful in
  real life?} This question is particularly pertinent when one realises that
all zkSNARKs with constant-sized proofs are shown secure in the structured reference
string (SRS) model where the SRS used by the parties---i.e.~a prover that shows
veracity of a statement using its private input called \emph{witness} and a
verifier that verifies it---necessarily comes with a trapdoor which can be used to break
(knowledge) soundness; that is, convince the verifier to accept a false
statement or a statement for which the prover does not know the witness. Hence
 secure real-life deployment of zkSNARKs require answers to questions such as---\emph{How can the parties be sure that the trapdoor has not leaked?}
\emph{Who is the party that generates the SRS?} \emph{Can this party be
  trusted?} The questions are especially
important in zero knowledge proofs used in distributed systems, like
e.g.~blockchains, where assuming that a trusted party generates the SRS may be unrealistic and subvert the whole purpose of a decentralised system.

One of the first to tackled this question were Bellare et al.~\cite{AC:BelFucSca16} who
proposed notions of \emph{subversion zero knowledge} and \emph{subversion
soundness}. The former states that zero knowledge property persists even when
the SRS is produced by a malicious party. The latter states that the proof
system remains sound in the same situation. Importantly, Bellare et al.~shown
that no system can be simultaneously subversion zero-knowledge and subversion-sound.
Abdolmaleki et al.~\cite{AC:ABLZ17}, and independently Fuchsbauer
\cite{PKC:Fuchsbauer18}, shown that the most
efficient zkSNARK for QAP\footnote{QAP stands for Quadratic Arithmetic
Program. QAP is currently the most efficient representation of arithmetic
circuits for showing their validity.} by Groth \cite{EC:Groth16} can be made
subversion zero-knowledge by introducing minor modifications to its SRS and without
sacrificing its efficiency. 
%\markulf{12/11/2020}{Not actually related to soundness.}

Although efficient, Groth's zkSNARK comes with a drawback---the SRS is
relation-dependent. That is, if one wants to show that two different arithmetic
circuits have been evaluated correctly, one has to do that using two different
SRS-s.
% We say that such a zkSNARK is not universal. 
Since SRS generation is a troublesome process, it is desired to have it
performed once, not every time a new circuit validity has to be shown. zkSNARKs
that utilise a single SRS for all circuits of a given size are called
\emph{universal}. A constant size universal zkSNARK was proposed by Groth et
al.~\cite{C:GKMMM18}. This particular proof system also introduced a novel
security property called \emph{updatable soundness}. Because of the Bellare et
al.~\cite{AC:BelFucSca16} impossibility result, one cannot wish for a system
that is simultaneously subversion zero-knowledge and subversion-sound. Groth et
al.~work around this problem by proposing a notion which allows zkSNARK provers
and verifiers to \emph{update} the SRS, i.e.~to take a SRS and modify it in a
well-defined and verifiable way to obtain a new SRS. Updates guarantee that if
at least one of the SRS-updating parties is honest then the proof system is
(knowledge) sound. Although inefficient, as the SRS length is quadratic to the
size of the proven statements, \cite{C:GKMMM18} set a new paradigm for designing
zkSNARKs.

The first universal zkSNARK with updatable and short SRS was $\sonic$ proposed
by Maller et al.~in \cite{CCS:MBKM19}. Eventually, Gabizon et al.~designed
$\plonk$ \cite{EPRINT:GabWilCio19} which currently is the most efficient
updatable universal zkSNARK. Independently, Chiesa et al.~\cite{EC:CHMMVW20}
proposed $\textsf{Marlin}$ with efficiency comparable to $\plonk$.
%
All these protocols utilise strong cryptographic assumptions like the algebraic
group model (AGM) and the random oracle model (ROM) to show their security. They
also use their own representation of circuits instead of the ``standard''
quadratic arithmetic programs (QAP). However, these protocols are also one of
the most interesting schemes for practitioners due to their practicality
stemming from the efficient proving and verification algorithms, the constant proof size, and the
universality of the SRS, as well as for their security model that diminishes the
need for a trusted party to provide a SRS.

% In this paper we provide a framework which shows that any zero-knowledge proof
% that fulfils some well-defined requirements is simulation-extractable. To show
% that the set of matching protocols is not empty we prove that both $\plonk$
% and $\sonic$ are simulation-extractable. 

\paragraph{On the importance of the simulation extractability.}
Although zkSNARKs are shown to satisfy a (standard) knowledge soundness
definition, simulation-extractability (SE) is the property that should be
required from zkSNARKs used in practice. This is since, arguably, in the real
life one simply cannot assume that the adversary who tries to break security of
a system does not have access to any proofs provided by other parties using the
same zero-knowledge scheme. On the contrary, in the most popular applications of
zkSNARKs, like privacy-preserving blockchains, proofs made by all
blockchain-participants are (usually) public. Thus, it is only reasonable to
require a zero-knowledge proof system to be resilient to attacks that utilise
proofs generated by different parties.



\paragraph{State of the art---simulation-extractable updatable universal zkSNARKs.} 
Up to our best knowledge, there are zkSNARKs that are simulation-extractable~\cite{C:GroMal17,EPRINT:BowGab18,EPRINT:AtaBag19,EC:Groth16}
and zkSNARKs that are universal~\cite{C:GKMMM18,CCS:MBKM19,EPRINT:GabWilCio19,EC:CHMMVW20}, however there are no known zkSNARKs that enjoy
both of these properties out-of-the-box. Obviously, given a universal zkSNARK
one could lift it to be simulation-extractable using techniques described
e.g.~in \cite{EPRINT:KZMQCP15,CCS:AbdRamSla20}, but such a lift comes with
inevitably efficiency loss.  
%On the other hand, there is no known lift that
%would take a zkSNARK and make it universal. Since updatable zkSNARKs are quite
%restrictive regarding format of their SRSs, making such lift seems as a very
%difficult task. 
The same applies for updatable zkSNARKs. No out-of-the-box
simulation-extractable updatable zkSNARKs are
currently known and there is no transformation that could take a SE zkSNARK
and make it updatable (even though transformations like \cite{CCS:AbdRamSla20}
preserves updatability).
%\markulf{12/11/2020}{arguably there is a transformation
%  that takes a snark and makes it universal: compute the reference string for a
%  universal circuit. There is also an optimized transformation for a variant of
%  Groth16, called Mirage. There is a SE transformation that preserves updateabilit}

\paragraph{On the popularity and power of Fiat-Shamir.} Although theoretically unsound~\cite{FOCS:GolKal03}, the Fiat-Shamir heuristic is a popular design tool when it comes to constructing zkSNARKs. Many works, including~\cite{CCS:MBKM19,EPRINT:GabWilCio19,EC:CHMMVW20}, design interactive protocols and prove them secure. However, they then only conjecturing the security for their non-interactive variants by employing the Fiat-Shamir heuristic. A more rigorous approach is to prove security of the Fiat-Shamir heuristic in the Random Oracle model.

\markulf{22.03.2021}{Describe Fiat-Shamir}
\subsection{Our contribution}
First of all, we show that a class of computationally special-sound proofs of
knowledge that are zero-knowledge 
%\markulf{12/11/2020}{Should this be HVZK, also
% in abstract?} 
in the standard model and have a unique response property \emph{are
  simulation-extractable out-of-the box} in the Random Oracle model when the Fiat--Shamir transformation is
applied to them. Although a similar problem has been already tackled by Faust et
al.~\cite{INDOCRYPT:FKMV12}, we extend it to a much wider class of protocols,
e.g.~our result is not restricted to $3$-message sigma protocols only.

To show that our result is useful and practical we prove that two of the most
efficient updatable and universal zkSNARKs---$\plonk$ and $\sonic$---are
simulation-extractable. To obtain this result, without having to change anything at all in these protocols, we had to overcome a number of challenges, as we explain below.

Before we continue, we note that \plonk{} and \sonic{}---as originally presented
in \cite{EPRINT:GabWilCio19} and \cite{CCS:MBKM19}---are interactive proofs of
knowledge made non-interactive by the Fiat--Shamir transform. In the following,
we denote the underlying interactive protocols by $\plonkprot$ (for $\plonk$)
and $\sonicprot$ (for $\sonic$) and the resulting, non-interactive ones, by
$\plonkprotfs$ and $\sonicprotfs$, respectively.

\subsubsection{Special soundness.} 
First, following \cite{INDOCRYPT:FKMV12}, we need to show that $\plonkprot$ and
$\sonicprot$ are special-sound. However the standard definition of special
soundness could not be met. First of all, the definition requires extraction of
a witness from any two transcripts, each containing three messages and sharing
the first message. For $\plonkprot$ and $\sonicprot$ that is not enough. The
definition had to be tuned to cover protocols that have more messages than just
three. Furthermore, the number of transcripts required is much
greater. Concretely, $(\numberofconstrains + 3)$---where $\numberofconstrains$ is the
number of constraints in the proven circuit---for $\plonkprot$ and $(\multconstr
+ \linconstr + 1)$---where $\multconstr$ and $\linconstr$ are the numbers of
multiplicative and linear constraints---for $\sonicprot$. Hence, we do not have
a \emph{pair of transcripts}, but a \emph{tree of transcripts}.

Secondly, both protocols rely on structured reference strings which come with
trapdoors that allow an adversary who knows them to produce multiple valid
proofs even without knowing the witness or for false statements. Recall that the
standard special soundness definition requires witness extraction from
\emph{any} pair of acceptable transcripts that share a common root---even those
that contain an acceptable proof for an incorrect statement. In this paper we
define a weaker version of special soundness---a computational special
soundness. That is, we show that either it is possible to extract a witness from
a tree of acceptable transcripts or one could use the adversary that produced
the tree to break some underlying computational assumption.

\subsubsection{Unique response property.} Another property that has to be proven
is the unique response property which states, as expressed in
\cite{C:Fischlin05}, that except for the first message, all messages sent by the
prover are deterministic (intuitively,~the prover does not use its randomness
except from the first message). As previously, we also could not use this
definition right out of the box simply because $\plonkprot$ does not follow
it---both first and the second prover's messages are randomised. We thus propose
a generalisation of the definition which states that a protocol is $\ur{i}$ if
the prover is deterministic starting from its $i$-th message. This property is
fulfilled by $\plonkprot$ with $i = 3$.

To be able to show the unique response property (for both of the protocols) we
also had to show that the modified KZG polynomial commitment schemes
\cite{AC:KatZavGol10} proposed in \cite{EPRINT:GabWilCio19} and
\cite{CCS:MBKM19} have a \emph{unique opening property} which states that for a
polynomial $\p{f}(X)$ evaluated at some point $z$ it should be infeasible for
any PPT adversary to provide two different (even honest) but acceptable openings
of the commitment.

\subsubsection{HVZK.}%
In order to show our result we also show that (interactive) $\plonkprot$ and
$\sonicprot$ are honest verifier zero-knowledge in the standard model, i.e.~the
simulator is able to produce a transcript indistinguishable from a transcript
produced by an honest prover and verifier without any additional knowledge,
esp.~without knowing the SRS trapdoor. Although both $\sonic$ and $\plonk$ are
shown to be zero-knowledge, the proofs provided by their authors utilise
trapdoors. For our reduction to work, we need simulators that provide
indistinguishable proofs relying only on reordering the messages
and picking suitable verifier's challenges. That is, any PPT party should be
able to produce a simulated proof by its own. (Note that this property does not
necessary break soundness of the protocol as the simulator is required only to
produce a transcript and is not involved in a real conversation with a real
verifier). This property allows us to build simulators for $\plonkprotfs$ and
$\sonicprotfs$ that rely only on programmability of the random oracle.

\subsubsection{Generalisation of the general forking lemma.}
Consider an interactive $3$-message special-sound protocol $\ps$ and its
non-interactive version $\ps_\fs$ obtained by the Fiat--Shamir transform. The
general forking lemma provides an instrumental lower bound for probability of
extracting a witness from an adversary who provides two proofs for the same
statement that share the first message. Since $\plonkprot$ and $\sonicprot$ have
more than $3$ messages and are not special-sound, the forking lemma, as stated
in \cite{CCS:BelNev06}, cannot be used directly. We thus propose a modification
that covers multi-message protocols where witness extraction requires more
transcripts than merely two.  Unfortunately, we also observe that the security
gap grows with the number of transcripts and the probability that the extractor
succeeds diminishes significantly. (That said, we have to note that the security
loss is polynomial, albeit big.)

We note that some modern zkSNARKs, like \cite{SP:BBBPWM18,CCS:MBKM19}, rely on
the Fiat--Shamir transform and the forking lemma heavily. First, an interactive
protocol is proposed and its security and (a variant of) special-soundness analysed. Second,
one uses an argument that the Fiat--Shamir transform can be used to get a
protocol that is non-interactive and shares the same security properties.

  
	% \michals{4.11.20}{ Does knowledge soundness have the same tightness? For
	% 	plonk there is no such issue with ks as its ks is shown in the algebraic
	% 	group model; sonic uses both witness extended emulation and the AGM, thus
	% I guess it is affected}
We see our generalized forking lemma as contributing to a critical assessment of
this approach. The analysis of the interactive protocol is not enough and one
has to consider the security loss implied by the generalisation of the forking
lemma or disclose a transformation that does not suffer from the generalisation
inefficiency. We note that the security loss may also apply when knowledge
soundness is proven. That is the case for $\sonic$, which security proof relies
on so-called witness-extended emulation. Authors of $\plonk$ worked around this
problem by showing their protocol security directly in the algebraic group model.

\subsubsection{Towards simulation-extractability.} Given our modified, less
restrictive, definition for special-soundness and the unique response property,
and our generalised forking lemma we are able to show the announced
result---simulation extractability of $\plonkprotfs$ and $\sonicprotfs$. The
proof is inspired by simulation-extractability and simulation-soundness proofs
from \cite{INDOCRYPT:FKMV12}, with major modifications, which were required as
\cite{INDOCRYPT:FKMV12} considers only sigma protocols
that are undoubtedly simpler protocols than the considered proof systems.
% Since the proof highly relies on the (generalised) forking lemma, the security
% lost it introduces is considerable.

% \subsubsection{Efficient simulation-soundness.}
% Given that the security reduction for simulation-extractability introduces a
% security gap we also present a proof for $\plonkprot_\fs$ simulation soundness
% which utilises the algebraic group model and is tight. It remains an open
% question how to show simulation extractability tightly, e.g.~using AGM.

% \subsection{Comparison with Bootle et al.~\cite{EC:BCCGP16}}
% \newcommand{\treefinder}{\pcadvstyle{T}}
% \newcommand{\provers}{\prover^*}
% Bootle et al.~\cite{EC:BCCGP16} provides a ``forking lemma'' showing that a
% protocol which has an extractor $\ext$ able to extract a witness from a tree of
% acceptable transcripts follows also notion of witness-extended emulation.
% An interestin question is whether their version of the forking lemma could be
% used to show security of protocols which were made non-interactive by the
% Fiat--Shamir transformation. We provide a simple adversary which shows that the
% answer is---not directly. \hl{In the following part of the paper we show that
%   allowing for some efficiency loss the result of Bootle et al.~\emph{can} be
%   used for non-interactive protocols.}

% \michals{15.02}{This lemma is here only temporarily}

% \begin{lemma}
%   Let $\proofsystem$ be an $(2\mu + 1)$ public-coin interactive proofsystem and
%   $\proofsystem_\fs$ be Fiat--Shamir transformed $\proofsystem$, $\ext$ be a
%   witness extraction algorithm which always succeeds to extract a witness from a
%   $(n_1, \ldots, n_\mu)$-tree of accepting transcripts and $\provers$ be a
%   (possibly malicious) prover for $\proofsystem_\fs$.
%   Then the tree finding algorithm $\treefinder$ presented at \cite[Lemma
%   1]{EC:BCCGP16} builds a tree of accepting transcript with probability $0$.
% \end{lemma}
% \begin{proof}
% Let $\provers$ be an prover who proceeds as follows. On input $(\inp, \wit)
% \in \REL$ it computes the first $i$ rounds of the protocol honestly, then it breaks
% and start the protocol over. With the second run, it finishes and outputs an
% honest proof for $\inp$ using $\wit$.

% Next, we analyse the probability that the tree-finder $\treefinder$ outputs a
% tree of accepting transcripts by rewinding $\provers$ and providing it with
% fresh challenges.

%   \qed
% \end{proof}


\subsection{Structure of the paper}
In the next section we present necessary preliminaries. Section 3 compounds of
new notions and theorems that instantiate our framework. Then, in Section 4, we
show our main result, that is a proof of simulation-soundness for a class of
zero-knowledge proofs of knowledge. In Section 5 and \cref{sec:sonic} we show
that $\plonk$ and $\sonic$ fulfils requirements of our framework and in fact is
simulation extractable.



\subsection{Related Work}
There are many results on simulation extractability for non-interactive
zero-knowledge proofs (NIZKs). First, Groth \cite{AC:Groth07} noticed that a
(black-box) simulation extractable NIZK is universally-composable (UC)
\cite{EPRINT:Canetti00}. Then Dodis et al.~\cite{AC:DHLW10} introduced a notion
of (black-box) \emph{true simulation extractability} and showed that no NIZK can
be UC-secure if it does not satisfy this property. In the context of zkSNARKs it is
important to mention such works as the first simulation-extractable zkSNARK by
Groth and Maller \cite{C:GroMal17} and SE zkSNARK for QAP by Lipmaa
\cite{EPRINT:Lipmaa19a}. Kosba's et al.~\cite{EPRINT:KZMQCP15} give a general
transformation from a NIZK to a black-box SE NIZK. Although their transformation
works for zkSNARKs as well, succinctness of the proof system is not preserved as
the statement's witness is encrypted. Recently, Abdolmaleki et
al.~\cite{CCS:AbdRamSla20} showed another transformation that obtains
non-black-box simulation extractability but also preserves succinctness of the
argument.

Independently, some authors focused on obtaining simulation extractability of
known zkSNARKs, like $\groth$ \cite{EC:Groth16}, by introducing minor
modifications and using stronger assumptions
\cite{EPRINT:BowGab18,EPRINT:AtaBag19}. Interestingly, although such
modifications hurt performance of the proof system, the resulting zkSNARKs are
still more efficient than the first SE zkSNARK \cite{C:GroMal17}, see
\cite{EPRINT:AtaBag19}. Recently, \cite{EPRINT:BKSV20} showed that the original
Groth's proof system from \cite{EC:Groth16} is weakly SE and randomisable.

\section{Preliminaries}
\label{sec:preliminaries}
Let $\ppt$ denote probabilistic polynomial-time and $\secpar \in \NN$ be the
security parameter. All adversaries are stateful. For an algorithm $\adv$, let
$\image (\adv)$ be the image of $\adv$ (the set of valid outputs of $\adv$), let
$\RND{\adv}$ denote the set of random tapes of correct length for $\adv$
(assuming the given value of $\secpar$), and let $r \sample \RND{\adv}$ denote
the random choice of the randomiser $r$ from $\RND{\adv}$. We denote by $\negl$
($\poly$) an arbitrary negligible (resp.~polynomial) function.

Probability ensembles $X = \smallset{X_\secpar}_\secpar$ and $Y =
\smallset{Y_\secpar}_\secpar$, for distributions $X_\secpar, Y_\secpar$, have
\emph{statistical distance} $\SD$ equal $\epsilon(\secpar)$ if $\sum_{a \in
  \supp{X_\secpar \cup Y_\secpar}} \abs{\prob{X_\secpar = a} - \prob{Y_\secpar =
    a}} = \epsilon(\secpar)$. We write $X \approx_\secpar Y$ if $\SD(X_\secpar,
Y_\secpar) \leq \negl$. For values $a(\secpar)$ and $b(\secpar)$ we write
$a(\secpar) \approx_\secpar b(\secpar)$ if $\abs{a(\secpar) - b(\secpar)} \leq
\negl$.

\newcommand{\samplespace}{\Omega}
\newcommand{\eventspace}{\mathcal{F}}
\newcommand{\probfunction}{\mu}

For a probability space $(\samplespace, \eventspace, \probfunction)$ and event
$\event{E} \in \eventspace$ we denote by $\nevent{E}$ an event that is
complementary to $\event{E}$,
i.e.~$\nevent{E} = \samplespace \setminus \event{E}$.

Denote by $\RELGEN = \smallset{\REL}$ a family of relations. We assume that if
$\REL$ comes with any auxiliary input, it is benign. Directly from the
description of $\REL$ one learns security parameter $\secpar$ and other
necessary information like public parameters $\pp$ containing description of a
group $\GRP$, if the relation is a relation of group elements (as it usually is
in case of zkSNARKs).

\paragraph{Bilinear groups.}
A bilinear group generator $\pgen (\secparam)$ returns public parameters $ \pp =
(p, \GRP_1, \GRP_2, \GRP_T, \pair, \gone{1}, \gtwo{1})$, where $\GRP_1$,
$\GRP_2$, and $\GRP_T$ are additive cyclic groups of prime order $p = 2^{\Omega
  (\secpar)}$, $\gone{1}, \gtwo{1}$ are generators of $\GRP_1$, $\GRP_2$, resp.,
and $\pair: \GRP_1 \times \GRP_2 \to \GRP_T$ is a non-degenerate
$\ppt$-computable bilinear pairing. We assume the bilinear pairing to be Type-3,
i.e., that there is no efficient isomorphism from $\GRP_1$ to $\GRP_2$ or from
$\GRP_2$ to $\GRP_1$. We use the by now standard bracket notation, i.e., we
write $\bmap{a}{\gi}$ to denote $a g_{\gi}$ where $g_{\gi}$ is a fixed generator
of $\GRP_{\gi}$. We denote $\pair (\gone{a}, \gtwo{b})$ as $\gone{a} \bullet
\gtwo{b}$. Thus, $\gone{a} \bullet \gtwo{b} = \gtar{a b}$. We freely use the
bracket notation with matrices, e.g., if $\vec{A} \vec{B} = \vec{C}$ then
$\vec{A} \grpgi{\vec{B}} = \grpgi{\vec{C}}$ and $\gone{\vec{A}}\bullet
\gtwo{\vec{B}} = \gtar{\vec{C}}$. Since every algorithm $\adv$ takes as input
the public parameters we skip them when describing $\adv$'s input. Similarly, we
do not explicitly state that each protocol starts with generating these
parameters by $\pgen$.

\subsection{Computational assumptions.}

\paragraph{Discrete-log assumptions.}
Security of $\plonk$ and $\sonic$ relies on two discrete-log based security
assumptions---$(q_1, q_2)$-$\dlog$ assumption and its extended with negative
exponents version $(q_1, q_2)$-$\ldlog$ assumption\footnote{Note that
  \cite{CCS:MBKM19} dubs their assumption \emph{a dlog assumption}. We changed
  that name to distinct it from the more standard dlog assumption used in
  \cite{EPRINT:GabWilCio19}. ``l'' in \emph{ldlog} relates to use of Laurent
  polynomials in the assumption.}.

\begin{definition}[$(q_1, q_2)\mhyph\dlog$ assumption]
	Let $\adv$ be a $\ppt$ adversary that gets as input $\gone{1, \chi, \ldots, \chi^{q_1}}, \gtwo{1, \chi, \ldots, \chi^{q_2}}$, for some randomly picked $\chi \in \FF_p$, then
	\[
		\condprob{\chi \gets \adv(\gone{1, \chi, \ldots, \chi^{q_1}}, \gtwo{1, \chi, \ldots, \chi^{q_2} })}{\chi \sample \FF_p} \leq \negl.
	\]
\end{definition}

\begin{definition}[$(q_1, q_2)\mhyph\ldlog$ assumption]
		Let $\adv$ be a $\ppt$ adversary that gets as input $\gone{\chi^{-q_1},
		\ldots, 1, \chi, \ldots, \chi^{q_1}}, \gtwo{\chi^{-q_2}, \ldots, 1, \chi, \ldots, \chi^{q_2}}$, for some randomly picked $\chi \in \FF_p$, then
	\[
			\condprob{\chi \gets \adv(\gone{\chi^{-q_1}, \ldots, 1, \chi, \ldots,
			\chi^{q_1}}, \gtwo{\chi^{-q_2}, \ldots, 1, \chi, \ldots, \chi^{q_2} })}{\chi \sample \FF_p} \leq \negl.
	\]
\end{definition}

\paragraph{BBG uber assumption.}
Also, to be able to show computational honest verifier zero knowledge of
$\plonk$ in the standard model, what is required by our reduction, we rely on the
\emph{uber assumption} introduced by Boneh et
al.~\cite{EC:BonBoyGoh05} as presented by Boyen in \cite{PAIRING:Boyen08}.
% Here we present not in its whole generality but rather fitted to our purpose.

Let $r, s, t, c \in \NN \setminus \smallset{0}$, Consider vectors of polynomials
$\pR \in \FF_p[X_1, \ldots, X_c]^r$, $\pS \in \FF_p[X_1, \ldots, X_c]^s$ and
$\pT \in \FF_p[X_1, \ldots, X_c]^t$. Write $\pR = \left( \p{r}_1, \ldots,
  \p{r}_r \right)$, $\pS = \left( \p{s}_1, \ldots, \p{s}_s \right)$ and $\pT =
\left( \p{t}_1, \ldots, \p{t}_r \right)$ for polynomials $\p{r}_i, \p{s}_j,
\p{t}_k$.

For a function $f$ and vector $(x_1, \ldots, x_c)$ we write $f(\pR)$ to
denote application of $f$ to each element of $\pR$, i.e.
\[
	f(\pR) = \left( f(\p{r}_1 (x_1, \ldots, x_c), \ldots, f(\p{r}_r
	(x_1, \ldots, x_c) \right).
\]
Similarly for applying $f$ to $\pS$ and $\pT$.

\begin{definition}[Independence of $\pR, \pS, \pT$]
	\label{def:independence}
	Let $\pR, \pS, \pT$ be defined as above. We say that polynomial $\p{f} \in
  \FF_p[X_1, \ldots, X_c]$ is \emph{dependent} on $\pR, \pS, \pT$ if there
  exists $rs + t$ constants $a_{i, j}, b_k$ such that $ \p{f} = \sum_{i = 1}^{r}
  \sum_{j = 1}^{s} a_{i, j} \p{r}_i \p{s}_j + \sum_{k = 1}^{t} b_k \p{t}_k. $ We
  say that $\p{f}$ is \emph{independent} if it is not dependent.
\end{definition}

\begin{definition}[$(\pR, \pS, \pT, \p{f}, T)$-uber assumption
	\cite{EC:BonBoyGoh05}]
	\label{def:uber_assumption_orig}
	Let $\pR, \pS, \pT$ be defined as above, $(x', x_1, \ldots, x_c) \sample
  \FF_p^{c + 1}$ and let $\p{f}$ be independent on $(\pR, \pS, \pT)$,
  cf.~\cref{def:independence}. Then, for any $\ppt$ adversary $\adv$
	\begin{multline*}
		\prob{\adv(\gone{\pR(x_1, \ldots x_c)}, \gtwo{\pS(x_1, \ldots, x_c)},
		\gtar{\pT(x_1, \ldots, x_c)}, \gtar{\p{f}(x_1, \ldots, x_c)}) = 1} \approx_\secpar \\ 
		\prob{\adv(\gone{\pR(x_1, \ldots x_c)}, \gtwo{\pS(x_1, \ldots, x_c)},
		\gtar{\pT(x_1, \ldots, x_c)}, \gtar{x'}) = 1}.  
	\end{multline*}
\end{definition}

For the sake of this paper we modify the assumption slightly, i.e.~we require
not target group $\GRP_T$ elements to be indistinguishable, but elements of
$\GRP_1$ (the changed element has been put into a \dbox{dashbox}), more precisely we require: 

\begin{definition}[$(\pR, \pS, \pT, \p{f}, 1)$-uber assumption]
	\label{def:uber_assumption}
	Let $\pR, \pS, \pT$ be defined as above,
    $(x', x_1, \ldots, x_c) \sample \FF_p^{c + 1}$ and let $\p{f}$ be
    independent on $(\pR, \pS, \pT)$, cf.~\cref{def:independence}.  Then, for
    any $\ppt$ adversary $\adv$
	\begin{multline*}
      \Pr\left[\adv(\gone{\pR(x_1, \ldots x_c)}, \gtwo{\pS(x_1, \ldots, x_c)},
		\gtar{\pT(x_1, \ldots, x_c)}, \dboxed{\gone{\p{f}(x_1, \ldots, x_c)}}) = 1\right] \approx_\secpar \\
       \Pr\left[\adv(\gone{\pR(x_1, \ldots x_c)}, \gtwo{\pS(x_1, \ldots, x_c)},
       \gtar{\pT(x_1, \ldots, x_c)}, \dboxed{\gone{x'}}) = 1\right].
	\end{multline*}
  \end{definition}

  \begin{lemma}
    Let $\pS$ contain a constant polynomial $\p{s}(X) = 1$, then
    $(\pR, \pS, \pT, \p{f}, T)$-uber assumption implies
    $(\pR, \pS, \pT, \p{f}, 1)$-uber assumption.
  \end{lemma}
  \begin{proof}
    Let $\adv_T$ be an adversary that breaks the
    $(\pR, \pS, \pT, \p{f}, T)$-uber assumption with some non-negligible
    advantage $\eps$, we build $\adv_{1}$ that breaks
    $(\pR, \pS, \pT, \p{f}, 1)$-uber assumption with the same advantage. Let
    $\adv_1$ get as its input
    $(\gone{\pR(x_1, \ldots x_c)}, \gtwo{\pS(x_1, \ldots, x_c)}, \gtar{\pT(x_1,
      \ldots, x_c)}, \gone{x'})$, for some random $x_1, \ldots, x_c$, and $x'$
    being either $\p{f}(x_1, \ldots, x_c)$ or a random element. Then $\adv_1$
    obtains $\gtar{x'}$ by computing $\gone{x'} \bullet \gtwo{1}$ and runs
    $\adv_T (\gone{\pR(x_1, \ldots x_c)}, \gtwo{\pS(x_1, \ldots, x_c)},
    \gtar{\pT(x_1, \ldots, x_c)}, \gtar{x'})$ which outputs its guess
    $b$. Adversary $\adv_1$ returns it and wins with probability $1/2 + \eps$.
\end{proof}

\paragraph{Proofs by Game-Hoping.}
Proofs by \emph{game hoping} is a method of writing proofs popularised by
e.g.~Shoup \cite{EPRINT:Shoup04} and Dent \cite{EPRINT:Dent06c}. The method
relies on the following lemma.

\begin{lemma}[Difference lemma,~{\cite[Lemma 1]{EPRINT:Shoup04}}]
	\label{lem:difference_lemma}
	Let $\event{A}, \event{B}, \event{F}$ be events defined in some probability
	space, and suppose that $\event{A} \land \nevent{F} \iff \event{B}
		\land \nevent{F}$.  Then 
	$
		\abs{\prob{\event{A}} - \prob{\event{B}}} \leq \prob{\event{F}}\,.
	$
\end{lemma}
\subsection{Algebraic Group Model}
The algebraic group model (AGM) introduced in \cite{C:FucKilLos18} lies between
the standard model and generic bilinear group model. In the AGM it is assumed
that an adversary $\adv$ can output a group element $\gnone{y} \in \GRP$ if
$\gnone{y}$ has been computed by applying group operations to group elements
given to $\adv$ as input. It is further assumed, that $\adv$ knows how to
``build'' $\gnone{y}$ from that elements. More precisely, the AGM requires that
whenever $\adv(\gnone{\vec{x}})$ outputs a group element $\gnone{y}$ then it
also outputs $\vec{c}$ such that $\gnone{y} = \vec{c}^\top \cdot
\gnone{\vec{x}}$. Both $\plonk$ and $\sonic$ have been shown secure using the
AGM. An adversary that works in the AGM is called \emph{algebraic}.

\subsection{Polynomial commitment.}
\label{sec:poly_com}
In the polynomial commitment scheme $\PCOM = (\kgen, \com, \open, \verify)$ the
committer $\committer$ can convince the receiver $\receiver$ that some polynomial
$\p{f}$ which $\committer$ committed to evaluates to $s$ at some point $z$
chosen by $\receiver$. $\plonk$ and $\sonic$ use variants of the KZG polynomial
commitment scheme \cite{AC:KatZavGol10}. We denote the first by $\PCOMp$, presented in
\cref{fig:pcomp}, and the latter by $\PCOMs$, presented in \cref{fig:pcoms}.
%
% We require $\PCOM$ to have the following properties:
\begin{figure}[t!]
	\begin{pcvstack}[center,boxed]
		\begin{pchstack}
			\procedure{$\kgen(\REL)$}
			{
			\chi \sample \FF^2_p \\ [\myskip]
			\pcreturn \gone{1, \ldots, \chi^{\numberofconstrains + 2}}, \gtwo{\chi}\\ [\myskip]
				\hphantom{\hspace*{5.5cm}}	
        %\hphantom{\pcind \p{o}_i(X) \gets \sum_{j = 1}^{t_i} \gamma_i^{j - 1}
        %\frac{\p{f}_{i,j}(X) - \p{f}_{i, j}(z_i)}{X - z_i}}
      }
			
			\pchspace
			
			\procedure{$\com(\srs, \vec{\p{f}}(X))$}
			{ 
				\pcreturn \gone{\vec{c}} = \gone{\vec{\p{f}}(\chi)}\\ [\myskip]
				\hphantom{\pcind \pcif 
					\sum_{i = 1}^{\abs{\vec{z}}} r_i \cdot \gone{\sum_{j = 1}^{t_j}
					\gamma_i^{j - 1} c_{i, j} - \sum{j = 1}^{t_j} s_{i, j}} \bullet
				\gtwo{1} + }
			}
		\end{pchstack}
		% \pcvspace
    
		\begin{pchstack}
			\procedure{$\open(\srs, \vec{\gamma}, \vec{z}, \vec{s}, \vec{\p{f}}(X))$}
			{
			\pcfor i \in \range{1}{\abs{\vec{z}}} \pcdo\\ [\myskip]
      \pcind \p{o}_i(X) \gets \sum_{j = 1}^{t_i} \gamma_i^{j - 1}
      \frac{\p{f}_{i,j}(X) - \p{f}_{i, j}(z_i)}{X - z_i}\\ [\myskip] \pcreturn
      \vec{o} = \gone{\vec{\p{o}}(\chi)}\\ [\myskip]
				\hphantom{\hspace*{5.5cm}}	
			}
			
			\pchspace
			
			\procedure{$\verify(\srs, \gone{c}, \vec{z}, \vec{s}, \gone{\p{o}(\chi)})$}
			{
				\vec{r} \gets \FF_p^{\abs{\vec{z}}}\\ [\myskip]
				\pcfor i \in \range{1}{\abs{\vec{z}}} \pcdo \\ [\myskip]
				\pcind \pcif 
          \sum_{i = 1}^{\abs{\vec{z}}} r_i \cdot \gone{\sum_{j = 1}^{t_j}
          \gamma_i^{j - 1} c_{i, j} - \sum{j = 1}^{t_j} s_{i, j}} \bullet
          \gtwo{1} + \\ [\myskip] \pcind \sum_{i = 1}^{\abs{\vec{z}}} r_i z_i
          o_i
          \bullet \gtwo{1} \neq \gone{- \sum_{i = 1}^{\abs{\vec{z}}} r_i o_i }
          \bullet \gtwo{\chi} \pcthen  \\
					\pcind \pcreturn 0\\ [\myskip]
					\pcreturn 1.
			}
		\end{pchstack}
	\end{pcvstack}
	\caption{$\PCOMp$ polynomial commitment scheme.}
	\label{fig:pcomp}
  \end{figure}

\begin{figure}[t!]
	\begin{pcvstack}[center,boxed]
		\begin{pchstack}
			\procedure{$\kgen(\REL)$} {
				\alpha, \chi \sample \FF^2_p \\ [\myskip]
				\pcreturn \gone{\smallset{\chi^i}_{i = -\multconstr}^{\multconstr},
          \smallset{\alpha \chi^i}_{i = -\multconstr, i \neq
            0}^{\multconstr}},\\
        \pcind \gtwo{\smallset{\chi^i, \alpha \chi^i}_{i =
            -\multconstr}^{\multconstr}}, \gtar{\alpha}\\
				%\markulf{03.11.2020}{} \\
			%	\hphantom{\pcind \p{o}_i(X) \gets \sum_{j = 1}^{t_i} \gamma_i^{j - 1} \frac{\p{f}_{i,j}(X) - \p{f}_{i, j}(z_i)}{X - z_i}}
				\hphantom{\hspace*{5.5cm}}	
		}
			
			\pchspace
			
			\procedure{$\com(\srs, \maxconst, \p{f}(X))$} {
				\p{c}(X) \gets \alpha \cdot X^{\dconst - \maxconst} \p{f}(X) \\ [\myskip]
				\pcreturn \gone{c} = \gone{\p{c}(\chi)}\\ [\myskip]
				\hphantom{\pcind \pcif \sum_{i = 1}^{\abs{\vec{z}}} r_i \cdot
          \gone{\sum_{j = 1}^{t_j} \gamma_i^{j - 1} c_{i, j} - \sum{j = 1}^{t_j}
            s_{i, j}} \bullet \gtwo{1} + } }
		\end{pchstack}
		% \pcvspace
    
		\begin{pchstack}
			\procedure{$\open(\srs, z, s, f(X))$}
			{
				\p{o}(X) \gets \frac{\p{f}(X) - \p{f}(z)}{X - z}\\ [\myskip]
				\pcreturn \gone{\p{o}(\chi)}\\ [\myskip]
				\hphantom{\hspace*{5.5cm}}	
			}
			
			\pchspace
			
			\procedure{$\verify(\srs, \maxconst, \gone{c}, z, s, \gone{\p{o}(\chi)})$}
      {
        \pcif \gone{\p{o}(\chi)} \bullet \gtwo{\alpha \chi} + \gone{s - z
        \p{o}(\chi)} \bullet \gtwo{\alpha} = \\ [\myskip] \pcind \gone{c}
        \bullet \gtwo{\chi^{- \dconst + \maxconst}} \pcthen  \pcreturn 1\\
        [\myskip]
        \rlap{\pcelse \pcreturn 0.} \hphantom{\pcind \pcif \sum_{i =
            1}^{\abs{\vec{z}}} r_i \cdot \gone{\sum_{j = 1}^{t_j} \gamma_i^{j -
              1} c_{i, j} - \sum{j = 1}^{t_j} s_{i, j}} \bullet \gtwo{1} + } }
		\end{pchstack}
	\end{pcvstack}
	
	\caption{$\PCOMs$ polynomial commitment scheme.}
	\label{fig:pcoms}
\end{figure}
  
We emphasize the following properties of a secure polynomial commitment
$\PCOM$:
\begin{description}
\item[Evaluation binding:] A $\ppt$ adversary $\adv$ which outputs a commitment
  $\vec{c}$ and evaluation points $\vec{z}$ has at most negligible chances to open
  the commitment to two different evaluations $\vec{s}, \vec{s'}$. That is, let
  $k \in \NN$ be the number of committed polynomials, $l \in \NN$ number of
  evaluation points, $\vec{c} \in \GRP^k$ be the commitments, $\vec{z} \in
  \FF_p^l$ be the arguments the polynomials are evaluated at, $\vec{s},\vec{s}'
  \in \FF_p^k$ the evaluations, and $\vec{o},\vec{o}' \in \FF_p^l$ be the
  commitment openings. Then for every $\ppt$ adversary $\adv$
	\[
		\Pr
			\left[
			\begin{aligned}
				& \verify(\srs, \vec{c}, \vec{z}, \vec{s}, \vec{o}) = 1,  \\ 
				& \verify(\srs, \vec{c}, \vec{z}, \vec{s}', \vec{o}') = 1, \\
				& \vec{s} \neq \vec{s}'
			\end{aligned}
			\,\left|\,\vphantom{\begin{aligned}
                  & \\
                  & \\
                  &
                \end{aligned}}
			\begin{aligned}
				& \srs \gets \kgen(\secparam),\\
				& (\vec{c}, \vec{z}, \vec{s}, \vec{s}', \vec{o}, \vec{o}') \gets \adv(\srs)
			\end{aligned}
			\right.\right] \leq \negl\,.
	\]

\end{description}
	
We say that $\PCOM$ has the unique opening property if the following holds:
\begin{description}
\item[Opening uniqueness:] Let $k \in \NN$ be the number of committed
  polynomials, $l \in \NN$ number of evaluation points, $\vec{c} \in \GRP^k$ be
  the commitments, $\vec{z} \in \FF_p^l$ be the arguments the polynomials are
  evaluated at, $\vec{s} \in \FF_p^k$ the evaluations, and $\vec{o} \in \FF_p^l$
  be the commitment openings. Then for every $\ppt$ adversary $\adv$
	\[
		\Pr
			\left[
			\begin{aligned}
				& \verify(\srs, \vec{c}, \vec{z}, \vec{s}, \vec{o}) = 1,  \\ 
				& \verify(\srs, \vec{c}, \vec{z}, \vec{s}, \vec{o'}) = 1, \\
				& \vec{o} \neq \vec{o'}
			\end{aligned}
			\,\left|\, \vphantom{\begin{aligned}
                  & \\
                  & \\
                  &
                \end{aligned}}
			\begin{aligned}
				& \srs \gets \kgen(\secparam),\\
				& (\vec{c}, \vec{z}, \vec{s}, \vec{o}, \vec{o'}) \gets \adv(\srs)
			\end{aligned}
			\right.\right] \leq \negl\,.
	\]
\end{description}
Intuitively, opening uniqueness assures that there is only one valid opening
for the committed polynomial and given evaluation point. This property is
crucial in showing simulation-extractability of $\plonk$ and $\sonic$. We show
that the $\plonk$'s and $\sonic$'s polynomial commitment schemes satisfy this
requirement in \cref{lem:pcomp_unique_op} and \cref{lem:pcoms_unique_op}
respectively.

\begin{description}
\item[Commitment of knowledge] For every $\ppt$ adversary $\adv$ who produces
  commitment $c$, evaluation point $z$, evaluation $s$ and opening $o$ there exists a
  $\ppt$ extractor $\ext$ such that
\[
  \Pr \left[
    \begin{aligned}
      & \p{f} = \ext_\adv(\srs, c),\\
      & c = \com(\srs, \p{f}),\\
      & \verify(\srs, c, z, s, o) = 1
    \end{aligned}
    \,\left|\,
      \vphantom{
        \begin{aligned}
          & \\
          & \\
          &
        \end{aligned}
        }
    \begin{aligned}
      & \srs \gets \kgen(\secparam),\\
      & (c, z, s, o) \gets \adv(\srs)
    \end{aligned}
  \right.\right]
  \geq 1 - \epsk(\secpar).
\]
\michals{26.03}{Do we need to assure that the extracted polynomial $f$ is of the
  right degree?}
In that case we say that $\PCOM$ is $\epsk$-knowledge.
\end{description}
Intuitively when a commitment scheme is a commitment of knowledge then if an
adversary produces a (valid) commitment $c$, which it can open, then it also
knows the underlying polynomial $\p{f}$ which commits to that value.
\cite{CCS:MBKM19} shows, using AGM, that $\PCOMs$ is a commitment of knowledge.
The same reasoning could be used to show that property for $\PCOMp$. We skip a
proof for that fact due to the lack of space.


\subsection{Zero knowledge}
In a zero-knowledge proof system, a prover convinces the verifier of veracity of
a statement without leaking any other information. The zero-knowledge property
is proven by constructing a simulator that can simulate the view of a cheating
verifier without knowing the secret information---witness---of the prover. A
proof system has to be sound as well, i.e.~for a malicious prover it should be
infeasible to convince a verifier on a false statement. Here, we focus on proof
systems that guarantee soundness against $\ppt$ malicious provers.

More precisely, let $\RELGEN(\secparam) = \smallset{\REL}$ be a family of
$\npol$ relations. Denote by $\LANG_\REL$ the language determined by $\REL$. Let
$\prover$ and $\verifier$ be $\ppt$ algorithms, the former called \emph{prover}
and the latter \emph{verifier}. We allow our proof system to have a setup,
i.e.~there is a $\kgen$ algorithm that takes as input the relation description
$\REL$ and outputs a common reference string $\srs$. We denote by
$\ip{\prover(\REL, \srs, \inp, \wit)}{\verifier(\REL, \srs,\inp)}$ a
\emph{transcript} (also called \emph{proof}) $\zkproof$ of a conversation
between $\prover$ with input $(\REL, \srs, \inp, \wit)$ and $\verifier$ with
input $(\REL, \srs, \inp)$. We write
$\ip{\prover (\REL, \srs, \inp, \wit)}{\verifier(\REL, \srs, \inp)} = 1$ if in
the end of the transcript the verifier $\verifier$ returns $1$ and say that
$\verifier$ accepts it. We sometimes abuse notation and write
$\verifier(\REL, \srs, \inp, \zkproof) = 1$ to denote a fact that $\zkproof$ is
accepted by the verifier. (This is especially handy when the proof system is
non-interactive, i.e.~the whole conversation between the prover and verifier
consists of a single message $\zkproof$ sent by $\prover$).

A proof system $\proofsystem = (\kgen, \prover, \verifier, \simulator)$ for $\RELGEN$ is required to have three properties: completeness, soundness and zero knowledge, which are defined as follows:
% \begin{description}
\paragraph{Completeness.}
%\item[Completeness]
  An interactive proof system $\proofsystem$ is
  \emph{complete} if an honest prover always convinces an honest verifier, that
  is for all $\REL \in \RELGEN(\secparam)$ and $(\inp, \wit) \in \REL$
	\[
		\condprob{\ip{\prover (\REL, \srs, \inp, \wit)}{\verifier (\REL, \srs,
        \inp)} = 1}{\srs \gets \kgen(\REL)} = 1\,.
	\]
    % \item[Soundness]
\paragraph{Soundness.}
    We say that $\proofsystem$ for $\RELGEN$ is \emph{sound} if no
  $\ppt$ prover $\adv$ can convince an honest verifier $\verifier$ to accept a
  proof for a false statement $\inp \not\in\LANG$. More precisely, for
  all $\REL \in \RELGEN(\secparam)$
	\[
      \condprob{\ip{\adv(\REL, \srs, \inp)}{\verifier(\REL, \srs, \inp)} =
        1}{\srs \gets \kgen(\REL), \inp \gets \adv(\REL, \srs); \inp \not\in
        \LANG_\REL} \leq \negl\,;
	\]
%\end{description}
Sometimes a stronger notion of soundness is required---except requiring that the
verifier rejects proofs of statements outside the language, we request from the
prover to know a witness corresponding to the proven statement. This property is
formalised by the so-called \emph{knowledge soundness}.
%\begin{description}
%\begin{description}
%\item[Knowledge soundness]
That is, we call an interactive proof system $\proofsystem$
\emph{knowledge-sound} if for any $\REL \in \RELGEN(\secparam)$ and a $\ppt$
adversary $\adv$
	% \begin{multline*}
	\[
	\Pr\left[
		\begin{aligned}
			& \verifier(\REL, \srs, \inp, \zkproof) = 1, \\
			& \REL(\inp, \wit) = 0
	 \end{aligned}
	  \,\left|\,
	 \begin{aligned}
		 & \srs \gets \kgen(\REL), \inp \gets \adv(\REL, \srs), \\
		 & (\wit, \zkproof) \gets \ext^{\ip{\adv(\REL, \srs, \inp)}{\verifier(\REL, \srs, \inp)}}(\REL, \inp)
	 \end{aligned}
	 \vphantom{\begin{aligned}
		 \adv (\zkproof) = 1, \\
		 \text{if $\zkproof{}$ is accepting} \\
		 \pcind \text{then $\REL(\inp, \wit)$}
	 \end{aligned}}\right.
	 \right] \leq \negl\,,
 % \end{multline*}
 \]
%\end{description}

 \paragraph{Zero knowledge.}
 We call a proof system $\proofsystem$ \emph{zero-knowledge} if for any
 $\REL \in \RELGEN(\secparam)$, $(\inp, \wit) \in \REL$, and adversary $\adv$
 there exists a $\ppt$ simulator $\simulator$ such that
	\begin{multline*}
	  \left\{\ip{\prover(\REL, \srs, \inp, \wit)}{\adv(\REL, \srs, \inp, \wit)}
      \,\left|\, \srs \gets \kgen(\REL)\COMMENT{, (\inp, \wit) \gets \adv(\REL,
          \srs)}\vphantom{\simulator^\adv}\right.\right\} \approx_\secpar
		%\\
		\left\{\simulator^{\adv}(\REL, \srs, \inp)\,\left|\, \srs \gets
        \kgen(\REL)\COMMENT{, (\inp, \wit) \gets \adv(\REL,
          \srs)}\vphantom{\simulator^\adv}\right.\right\}\,.
\end{multline*}
	%
We call zero knowledge \emph{perfect} if the distributions are equal and
\emph{computational} if they are indistinguishable for any $\nuppt$
distinguisher.

%\end{description}
An SRS $\srs$ comes with a secret string called \emph{trapdoor} $\td$ that
allows the simulator to produce a simulated proof. In that case algorithm
$\kgen(\REL)$ outputs $(\srs, \td)$ and $\td$ is given to the simulator. In this
paper we distinguish simulators that requires a trapdoor to simulate and those
that do not. We call the former \emph{SRS-simulators} and denote them by
$\simulator_\td$.

Occasionally, a weaker version of zero knowledge is sufficient. So called
\emph{honest verifier zero knowledge} (HVZK) assumes that the verifier's
challenges are picked at random from some predefined set. Although weaker, this
definition suffices in many applications. Especially, an interactive
zero-knowledge proof that is HVZK and \emph{public-coin} (i.e.~the verifier
outputs as challenges its random coins) can be made non-interactive and
zero-knowledge in the random oracle model by using the Fiat--Shamir
transformation.
	

\subsubsection{Sigma protocols}
A sigma protocol $\sigmaprot = (\prover, \verifier, \simulator)$ for a relation
$\REL \in \RELGEN(\secparam)$ is a special case of an interactive proof which
transcript compounds of three messages $(a, b, z)$, the middle being a challenge
provided by the verifier. Sigma protocols are honest verifier zero-knowledge in
the standard model and specially-sound. That is, there exists an extractor
$\ext$ which given two accepting transcripts $(a, b, z)$, $(a, b', z')$ for a
statement $\inp$ can recreate the corresponding witness if $b \neq b'$.
Formally,
%\begin{description}
\paragraph{Special soundness.} A sigma protocol $\sigmaprot$ is \emph{specially-sound}
  if for any adversary $\adv$ the probability
	\[
		\Pr\left[
		\begin{aligned}
				& \wit \gets \ext(\REL, \inp, (a, b, z), (a, b', z')),\\
				& \REL(\inp, \wit) = 0
		\end{aligned}
		\,\left|\,
		\begin{aligned}
          & (\inp, (a, b, z), (a, b', z')) \gets \adv(\REL), %\\
           b \neq b',  \\
          & \verifier(\REL, \inp, (a, b, z)) = %\\
           = \verifier(\REL, \inp, (a, b', z')) = 1, \\
		\end{aligned}
		\right.\right] \leq  \negl\,.
	\]
%\end{description}

Another property that sigma protocols may have is a unique response
property \cite{C:Fischlin05} which states that no $\ppt$ adversary can
produce two accepting transcripts that differ only on the last
element. More precisely, 
%\begin{description} 
\paragraph{Unique response property.} Let
$\sigmaprot = (\prover, \verifier, \simulator)$ be a sigma-protocol for
$\REL \in \RELGEN(\secparam)$ which proofs compound of three messages
$(a, b, z)$. We say that $\sigmaprot$ is has a unique response property if for
all $\ppt$ algorithms $\adv$ holds
\[ \condprob{\verifier (\REL, \inp, (a, b, z)) = \verifier (\REL, \inp, (a, b,
    z')) = 1}{(\inp, a, b, z, z') \gets \adv(\REL), z \neq z'} \leq \negl\,.  \]
%\end{description} 
If this property holds even against unbounded adversaries, it is called
\emph{strict}, cf.~\cite{INDOCRYPT:FKMV12}. Later on we call protocols that
follows this notion \emph{ur-protocols}. For the sake of completeness we note
that many sigma protocols, like e.g.~Schnorr's protocol \cite{C:Schnorr89},
fulfil this property.

\subsection{From interactive to non-interactive---Fiat--Shamir transformation}
Consider a $(2\mu + 1)$-message, public-coin, honest verifier zero-knowledge
interactive proof system
$\proofsystem = (\kgen, \prover, \verifier, \simulator)$ for
$\REL \in \RELGEN(\secparam)$.  Let $\zkproof$ be a proof performed by the
prover $\prover$ and verifier $\verifier$ compound of messages
$(a_1, b_1, \ldots, a_{\mu - 1}, b_{\mu}, a_{\mu + 1})$, where $a_i$ comes from
$\prover$ and $b_i$ comes from $\verifier$.  Denote by $\ro$ a random oracle.
Let $\proofsystem_\fs = (\kgen_\fs, \prover_\fs, \verifier_\fs, \simulator_\fs)$
be a proof system such that
\begin{itemize}
  \item $\kgen_\fs$ behaves as $\kgen$.
  \item $\prover_\fs$ behaves as $\prover$ except after sending message
    $a_i$, $i \in \range{1}{\mu}$, the prover does not wait for
    the message from the verifier but computes it locally setting $b_i
    = \ro(\zkproof[0..i])$, where $\zkproof[0..j] = (\inp, a_1, b_1, \ldots,
    a_{j - 1}, b_{j - 1}, a_j)$. (Importantly, $\zkproof[0..\mu + 1] =
    (\inp, \zkproof)$).
  \item $\verifier_\fs$ behaves as $\verifier$ but does not provide
    challenges to the prover's proof. Instead it computes the
    challenges locally as $\prover_\fs$ does. Then it verifies the
    resulting transcript $\zkproof$ as the verifier $\verifier$ would. 
  \item $\simulator_\fs$ behaves as $\simulator$, except when
    $\simulator$ picks challenge $b_i$, $\simulator_\fs$ programs the
    random oracle to output $b_i$ on $\zkproof[0, i]$.
  \end{itemize}

Fiat--Shamir heuristic states that $\proofsystem_\fs$ is zero-knowledge
non-interactive proof system for $\REL \in \RELGEN(\secparam)$.

\subsection{Simulation extractable NIZKs from sigma protocols}
Real life applications often require from a NIZK proof system to be
non-malleable. That is, no adversary seeing a proof $\zkproof$ for a statement
$\inp$ should be able to provide a new proof $\zkproof'$ related to $\zkproof$.
A strong version of non-malleability is formalised by so-called \emph{simulation extractability}
which assures that no adversary can produce a valid proof without knowing the
corresponding witness. This must hold even if the adversary is allowed to see
polynomially many simulated proofs for any statements it wishes.

\begin{definition}[Simulation-extractable NIZK, \cite{INDOCRYPT:FKMV12}]
	\label{def:simext}
	Let $\ps = (\kgen, \prover, \verifier, \simulator)$ be a computationally
  special-sound HVZK proof and $\ps_\fs = (\kgen_\fs, \prover_\fs,
  \verifier_\fs, \simulator_\fs)$ be $\ps$ transformed by the Fiat--Shamir
  transform. We say that $\ps_\fs$ is \emph{simulation-extractable} with
  \emph{extraction error} $\nu$ if for any $\ppt$ adversary $\adv$ that is given
  oracle access to a random oracle $\ro$ and simulator $\simulator_\fs$, and
  produces an accepting transcript of $\ps$ with probability $\accProb$, that is
	\[
		\accProb = \Pr \left[
		\begin{aligned}
			& \verifier_\fs(\REL, \srs, \inp_{\advse}, \zkproof_{\advse}) = 1,\\
			& (\inp_{\advse}, \zkproof_{\advse}) \not\in Q
		\end{aligned}
		\, \left| \,
		\begin{aligned}
			& \srs \gets \kgen(\REL), r \sample \RND{\advse}, \\
			& (\inp_{\advse}, \zkproof_{\advse}) \gets \advse^{\simulator_\fs,
			\ro} (\REL, \srs; r) 
		\end{aligned}
		\right.\right]\,,
	\]
	probability
	\[
		\frkProb = \Pr \left[
		\begin{aligned}
			& \verifier_\fs(\REL, \srs, \inp_{\advse}, \zkproof_{\advse}) = 1,\\
			& (\inp_{\advse}, \zkproof_{\advse}) \not\in Q,\\
			& \REL(\inp_{\advse}, \wit_{\advse}) = 1
		\end{aligned}
		\, \left| \,
		\begin{aligned}
			& \srs \gets \kgen(\REL), r \sample \RND{\advse},\\
			& (\inp_{\advse}, \zkproof_{\advse}) \gets \advse^{\simulator_\fs,
			\ro} (\REL, \srs; r) \\
			& \wit_{\advse} \gets \ext_\ss (\REL, \srs, \advse, r, \inp_{\advse}, \zkproof_{\advse},
			Q, Q_\ro,) 
		\end{aligned}
		\right.\right]
	\]
	is at at least 
	\[
		\frkProb \geq \frac{1}{\poly} (\accProb - \nu)^d - \eps(\secpar)\,,
	\]
	for some polynomial $\poly$, constant $d$ and negligible $\eps$ whenever
  $\accProb \geq \nu$. List $Q$ contains all $(\inp, \zkproof)$ pairs where
  $\inp$ is an instance provided to the simulator by the adversary and
  $\zkproof$ is the simulator's answer. List $Q_\ro$ contains all $\advse$'s
  queries to $\ro$ and $\ro$'s answers.
\end{definition}

Consider a sigma protocol $\sigmaprot = (\prover, \verifier, \simulator)$ that
is specially sound and has a unique response property. Let $\sigmaprot_\fs =
(\prover_\fs, \verifier_\fs, \simulator_\fs)$ be a NIZK obtained by applying the
Fiat--Shamir transform to $\sigmaprot$. Faust et al.~\cite{INDOCRYPT:FKMV12}
show that every such $\sigmaprot_\fs$ is simulation-extractable. This result is
presented in \cref{sec:forking_lemma} along with the instrumental forking lemma,
cf.~\cite{CCS:BelNev06}.

\section{Towards simulation extractability of multi-message protocols,
  definitions and lemmas}
\label{sec:se_definitions}
Unfortunately, Faust et al.'s result cannot be directly applied in our case
since the protocols we consider have more than three messages, require more than
just two transcripts for the extractor to work and are not special
sound. In this part we generalize the forking lemma, special
soundness, and the unique response property to make them compatible with
multi-message protocols.

\subsection{Generalised forking lemma.}
%\label{sec:forking_lemma}
First of all, although dubbed ``general'', \cref{lem:forking_lemma} is not
general enough for our purpose as it is useful only for protocols where witness
can be extracted from just two transcripts. To be able to extract a witness
from, say, an execution of $\plonkprot$ we need to obtain at least
$\numberofconstrains + 3$ valid proofs, and even more for $\sonicprot$. Here we
propose a generalisation of the general forking lemma that given probability of
producing an accepting transcript $\accProb$ lower-bounds the probability of
generating a \emph{tree of accepting transcripts} $\tree$, which allows to
extract a witness.

\begin{definition}[Tree of accepting transcripts, cf.~{\cite{EC:BCCGP16}}]
	\label{def:tree_of_accepting_transcripts}
	Consider a $(2\mu + 1)$-message interactive proof system $\ps$. An $(n_1,
  \ldots, n_\mu)$-tree of accepting transcript is a tree where each node on
  depth $i$, for $i \in \range{1}{\mu + 1}$, is an $i$-th prover's message in an
  acceptable transcript; edges between the nodes are labeled with verifier's
  challenges, such that no two edges on the same depth have the same
  label; and each node on depth $i$ has $n_{i} - 1$ siblings and $n_{i +
    1}$ children. Altogether, the tree consists of $N = \prod_{i = 1}^\mu n_i$
  branches, which makes $N$ acceptable transcripts. We require $N = \poly$.
\end{definition}


\begin{lemma}[General forking lemma II]
	\label{lem:generalised_forking_lemma}
	Fix $q \in \ZZ$ and set $H$ of size $h \geq m$. Let $\zdv$ be a $\ppt$
  algorithm that on input $y, h_1, \ldots, h_q$ returns $(i, s)$ where $i \in
  \range{0}{q}$ and $s$ is called a side output. Denote by $\ig$ a randomised
  instance generator. We denote by $\accProb$ the probability
	\[
		\condprob{i \neq 0}{ y \gets \ig;\ h_1, \ldots, h_q \sample H;\ (i, s)
		\gets \zdv(y, h_1, \ldots, h_q)}\,.
	\]
	Let $\genforking_{\zdv}^{m}$ denote the algorithm described in
  \cref{fig:genforking_lemma} then the probability $\frkProb := \condprob{b =
    1}{y \gets \ig;\ h_1, \ldots, h_{q} \sample H;\ (b, \vec{s}) \gets
    \genforking_{\zdv}^{m}(y, h_1, \ldots, h_q)}$ is at least
	\[
		\frac{\accProb^m}{q^{m - 1}} - \accProb \cdot \left(1 -
    \frac{h!}{(h - m)! \cdot h^{m}}\right).
	\]
		
	\begin{figure}[t]
		\centering
		\fbox{
		\procedure{$\genforking_{\zdv}^{m} (y,h_1^{1}, \ldots, h_{q}^{1})$}		
		{
		\rho \sample \RND{\zdv}\\
		(i, s_1) \gets \zdv(y, h_1^{1}, \ldots, h_{q}^{1}; \rho)\\
    i_1 \gets i\\
		\pcif i = 0\ \pcreturn (0, \bot)\\
		\pcfor j \in \range{2}{m}\\
		\pcind h_{1}^{j}, \ldots, h_{i - 1}^{j} \gets h_{1}^{j - 1}, \ldots,
		h_{i - 1}^{j - 1}\\
		\pcind h_{i}^{j}, \ldots, h_{q}^{j} \sample H\\
		\pcind (i_j, s_j) \gets \zdv(y, h_1^{j}, \ldots, h_{i - 1}^{j}, h_{i}^{j},
		\ldots, h_{q}^{j}; \rho)\\
		\pcind \pcif i_j = 0 \lor i_j \neq i\ \pcreturn (0, \bot)\\
    \pcif \exists (j, j') \in \range{1}{m}^2, j \neq j' : (h_{i}^{j} = h_{i}^{j'})\
	\pcreturn (0, \bot)\\
		\pcelse \pcreturn (1, \vec{s})
	}}
	\caption{Generalised forking algorithm $\genforking_{\zdv}^{m}$}
	\label{fig:genforking_lemma}
\end{figure}
\end{lemma}
\begin{proof}
First let denote by $\accProb(y)$ and $\frkProb(y)$ the following probabilities
\begin{align*}
\accProb(y) & =  \condprob{i \neq 0}{h_1, \ldots, h_q \sample H;\ (i, s)
\gets \zdv(y, h_1, \ldots, h_q)}\,.\\
	\frkProb(y) & = \condprob{b = 1}{(b, \vec{s}) \gets
\genforking_{\zdv}^{m}(y, h_1, \ldots, h_q)}\,.
\end{align*}

We start by claiming that for all $y$ 
\begin{equation}\label{eq:frkProb_y}
	\frkProb(y) \geq 
	\frac{\accProb(y)^m}{q^{m - 1}} - \accProb(y) \cdot \left(1 -
  \frac{h!}{(h - m)! \cdot h^{m}}\right)
	\end{equation}
Then with the expectation taken over $y \sample \ig$, we have
\begin{align}
	\frkProb & = \expected{\frkProb(y)} \geq
	\expected{\frac{\accProb(y)^m}{q^{m - 1}} -  \accProb(y) \cdot \left(1 -
  \frac{h!}{(h - m)! \cdot h^{m}}\right)} \label{eq:use_eq1}\\
	& \geq \frac{\expected{\accProb(y)}^m}{q^{m - 1}} -
	\expected{\accProb(y)} \cdot \left(1 - \frac{h!}{(h - m)! \cdot
  h^{m}}\right) \label{eq:by_lemma_jensen}\\
	& = \frac{\accProb^m}{q^{m - 1}} -  \accProb \cdot \left(1 -
  \frac{h!}{(h - m)! \cdot h^{m}}\right)\label{eq:by_accProb}\,.
\end{align}
Where \cref{eq:use_eq1} comes from \cref{eq:frkProb_y};
\cref{eq:by_lemma_jensen} comes from linearity of expected value and \cref{lem:jensen}; and
\cref{eq:by_accProb} holds by the fact that $\expected{\accProb(y)} =
\accProb$.

We now show \cref{eq:frkProb_y}.
Denote by $J = \range{1}{m}^2 \setminus \smallset{(j, j)}_{j \in \range{1}{m}}$. 
For any input $y$, with probabilities taken over the coin tosses of
$\genforking_{\zdv}^{m}$ we have
\begin{align*}
	\frkProb (y) & = \prob{i_j = i_{j'} \land i_j \geq 1 \land
h_{i_j}^{j} \neq h_{i_{j'}}^{j'} \text{ for } (j, j') \in J}	\\
	& \geq \prob{i_j = i_{j'} \land i_j \geq 1 \text{ for } (j, j') \in J} %\\
   - \prob{i_j \geq 1 \land h_{i_j}^{j} = h_{i_{j'}}^{j'} \text{ for some } (j, j') \in J}\\
	& = \prob{i_j = i_{j'} \land i_j \geq 1 \text{ for } (j, j') \in J} -
	\prob{i_j \geq 1} \cdot 
  \left(1 - \frac{h!}{(h - m)! \cdot h^{m}}\right) \\ 
	& = \prob{i_j = i_{j'} \land
	i_j \geq 1 \text{ for } (j, j') \in J} - \accProb(y) \cdot \left(1 -
\frac{h!}{(h - m)! \cdot h^{m}}\right)\,.
\end{align*}

Probability that for some $(j, j') \in J$ and $i_j = i_{j'}$ holds
$h_{i_j}^{j} \neq h_{i_{j'}}^{j'}$ equals 
\[
  \frac{h \cdot (h - 1)
\cdot \ldots \cdot (h - m - 1)}{h^m} = \frac{h!}{(h - m)! \cdot h^m}.
\]
That is, it equals the number
of all $m$-element strings where each element is different divided by
the number of all $m$-element strings, where elements are taken from a
set of size $h$. 

It remains to show that $\prob{i_j = i_{j'} \land i_j \geq 1 \text{ for } (j,
  j') \in J} \geq \infrac{\accProb(y)^m}{q^{m - 1}}$. Let $\RND{\zdv}$ denote
the set from which $\zdv$ picks its coins at random. For each $\iota \in
\range{1}{q}$ let $X_\iota \colon \RND{\zdv} \times H^{\iota - 1} \to [0, 1]$ be
defined by setting $X_\iota(\rho, h_1, \ldots, h_{\iota - 1})$ to
\[
  \condprob{i = \iota}{h_\iota, \ldots, h_q \sample H; (i, s) \gets \zdv(y, h_1,
    \ldots, h_q; \rho)}
\]
for all $\rho \in \RND{\zdv}$ and $h_1, \ldots, h_{\iota - 1} \in H$. Consider
$X_\iota$ be a random variable over the uniform distribution on its domain. Then
\begin{align*}
	& \prob{i_j = i_{j'} \land i_j \geq 1 \text{ for } (j, j') \in J} 
	 = \sum_{\iota = 1}^{q} \prob{i_1 = \iota \land \ldots \land i_m = \iota} \\
	& = \sum_{\iota = 1}^{q} \prob{i_1 = \iota} \cdot \condprob{i_2 = \iota}{i_1 = \iota} \cdot \ldots \cdot \condprob{i_m = \iota}{i_1 = \ldots = i_{m - 1} = \iota} \\
	& = \sum_{\iota = 1}^{q} \sum_{\rho, h_1, \ldots, h_{\iota - 1}} X_{\iota}
   (\rho, h_1, \ldots, h_{\iota - 1})^{m} \cdot \frac{1}{\abs{\RND{\zdv}} \cdot \abs{H}^{\iota - 1}}
   = \sum_{\iota = 1}^{q} \expected{X_\iota^m} \,.
\end{align*}
Importantly, $\sum_{\iota = 1}^q \expected{X_{\iota}} = \accProb(y)$.

By \cref{lem:jensen} we get
\[
	\sum_{\iota = 1}^{q} \expected{X_\iota^m} \geq \sum_{\iota = 1}^{q} \expected{X_\iota}^m\,.
\]
Note that for e.g.~$X_i = 1$, $i \in \range{1}{q}$ the inequality becomes equality, that is, it is tight.
 
We now use the H\"older inequality, cf.~\cref{lem:holder}, for $x_i = \expected{X_i}$, $y_i = 1$, $p = m$, and $q = m/(m - 1)$ obtaining
\begin{gather}
	\left(\sum_{i = 1}^{q} \expected{X_i}\right)^{m}  \leq \left(\sum_{i = 1}^{q} \expected{X_i}^m\right) \cdot q^{m - 1}\\
	\frac{1}{q^{m - 1}} \cdot \accProb(y)^{m} \leq \sum_{i = 1}^{q} \expected{X_i}^m\,.
\end{gather}
Finally, we get
\[
	\frkProb(y) \geq \frac{\accProb(y)^m}{q^{m - 1}} - 
	 \accProb(y) \cdot \left(1 - \frac{h!}{(h - m)! \cdot h^m}\right)\,.
\]
\qed
\end{proof}

To highlight importance of the generalised forking lemma we describe how we use
it in our simulation-extractability proof.  Let $\proofsystem$ be a
computationally special sound proof system where for an instance $\inp$ the
corresponding witness can be extracted from an
$(1, \ldots, 1, n_k, 1, \ldots, 1)$-tree of accepting transcripts.  Let $\advse$
be the simulation-extractability adversary that outputs an acceptable proof with
probability at least $\accProb$. (Although we use the same $\accProb$ to denote
probability of $\zdv$ outputing a non-zero $i$ and probability of $\advse$
outputing an acceptable proof we claim that these probabilities are exactly the
same what comes from how we define $\zdv$.)  Let $\advse$ produce an acceptable
proof $\zkproof_{\advse}$ for instance $\inp_{\advse}$; $r$ be $\advse$'s
randomness; $Q$ the list of queries submitted by $\advse$ along with simulator's
$\simulator_\zkproof$ answers; and $Q_\ro$ be the list of all random oracle
queries made by $\advse$.  All of these are given to the extractor $\ext$ that
internally runs the forking algorithm $\genforking_\zdv^{n_k}$.  Algorithm $\zdv$
takes $(\REL, \srs, \advse,
%\inp_\advse,
%\zkproof_\advse, 
Q, r)$ as input $y$ and $Q_\ro$ as input $h_1^1, \ldots,
h_q^1$. 
(For the sake of completeness, we allow $\genforking_\zdv^{n_k}$ to
pick $h^1_{l + 1}, \ldots, h^1_q$ responses if $Q_\ro$ has only $l < q$
elements.)  

Next, $\zdv$ runs internally $\advse(\REL, \srs; r)$ and responds to its random
oracle and simulator queries by using $Q_\ro$ and $Q$. Note that $\advse$ makes
the same queries as it did before it output $(\inp_{\advse}, \zkproof_{\advse})$
as it is run on the same random tape and with the same answers from the
simulator and random oracle. After $\advse$ finishes its acceptable proof
$\zkproof_{\advse}$, algorithm $\zdv$ outputs $(i, \zkproof_{\advse})$, where
$i$ is the index of a random oracle query submitted by $\advse$ to get the challenge after
$k$-th message from the prover---a message where the tree of transcripts
branches.
%We also note that we differ here from the description of the
%$\genforking_\zdv$ algorithm provided
%e.g.~in~\cite{INDOCRYPT:FKMV12,CCS:BelNev06} where the algorithm also picks the first set of random oracle
%queries' responses $h^1_1, \ldots, h^1_q$. We however note that this difference
%is only notational as random oracle queries' responses provided in $Q_\ro$ are
%random as well. 
Then, after the first run of $\advse$ is done, the extractor runs $\zdv$ again,
but this time it provides fresh random oracle responses $h^2_i, \ldots,
h^2_q$. Note that this is equivalent to rewinding $\advse$ to a point just
before $\advse$ is about to ask its $i$-th random oracle
query. Probability that the adversary produces an acceptable transcript with the
fresh random oracle responses is at least $\accProb$. This continues until the
required number of transcripts is obtained. 

We note that in the original forking lemma the forking algorithm $\forking$,
cf.~\cref{fig:forking_lemma}, gets only as input $y$ and elements $h^1_1, \ldots,
h^1_q$ are randomly picked from $H$ internally by $\forking$. However, assuming
that $h^1_1, \ldots, h^1_q$ are random oracle responses, thus are random, makes
the change only notational.

We also note that the general forking lemma proposed in
\cref{lem:generalised_forking_lemma} works for protocols which have extractable
witness from a $(1, \ldots, 1, n_k, 1, \ldots, 1)$-tree of acceptable
transcripts. This limitation however does not affect the main result of this
paper, i.e.~showing that both $\plonk$ and $\sonic$ are simulation extractable.

\subsection{Unique-response protocols.}
Another problem comes with another assumption required by Faust et al.---the
unique response property of the transformed sigma protocol. The original
Fischlin's formulation, although suitable for applications presented in
\cite{C:Fischlin05,INDOCRYPT:FKMV12}, is not enough in our case. First of all,
the property assumes that the protocol has three messages, with the middle being
the challenge from the verifier. That is not the case we consider here. Second,
it is not entirely clear how to generalize the property. Should one require that
after the first challenge from the verifier the prover's responses are fixed?
That could not work since the prover needs to answer differently on different
verifier's challenges, as otherwise the protocol could have fewer
rounds. Another problem rises when the protocol contains some
message---obviously, except the first one---where the prover randomises its
message. In that case unique-responsiveness can not hold as well. Last but not
least, the protocols we consider here are not designed to be in the standard
model, but utilises SRS what also complicates things considerably.

We walk around these obstacles by providing a generalised notion of the unique
response property. More precisely, we say that a $(2\mu + 1)$-message protocol
has \emph{unique responses from $i$}, and call it an $\ur{i}$-protocol, if it
follows the definition below:

\begin{definition}[$\ur{i}$-protocol]
\label{def:wiur}
Let $\proofsystem$ be a $(2\mu + 1)$-message proof system $\ps = (\kgen,
\prover, \verifier, \simulator)$. Let $\proofsystem_\fs$ be $\proofsystem$ after the
Fiat--Shamir transform, Denote by $a_1, \ldots, a_{\mu}, a_{\mu + 1}$ protocol messages
output by the prover, We say that $\proofsystem$ has \emph{unique responses
  from $i$ on} if for any $\ppt$ adversary $\adv$:
\[
	\prob{
		\begin{aligned}
		&	\inp, \vec{a} = (a_1, \ldots, a_{\mu + 1}), \vec{a'} = (a'_1, \ldots,
    a'_{\mu + 1})
		\gets \adv^\ro(\REL, \srs), \\
    & \vec{a} \neq \vec{a'}, a_1, \ldots, a_{i} = a'_1,
    \ldots, a'_{i}, \\
		& \verifier^\ro_\fs (\REL, \srs, \inp, \vec{a}) =
		\verifier^\ro_\fs(\REL, \srs, \inp, \vec{a'}) = 1
		\end{aligned}
		\ \left|\  
	\vphantom{\begin{aligned}
	&	\vec{a} = (a_0, b_0, \ldots, a_j, b_j, a_\mu), \vec{a'} = (a'_0, b'_0, \ldots, a'_j,
	b'_j a'_\mu) \gets \adv(\REL, \srs), \vec{a} \neq \vec{a'}, \\
	& b_k = b'_k, k \in \range{1, \mu - 1},\\ a_l = a'_l, l \in
\range{1}{j}, j > i 
	\end{aligned}}
\srs \gets \kgen_\fs(\REL) \right.
} \leq \negl.
\]
\end{definition}
Intuitively, a protocol is $\ur{i}$ if it is infeasible for a $\ppt$ adversary
to produce a pair of acceptable and different proofs $\zkproof$, $\zkproof'$
that are the same on  first $i$ messages. 
% after $i$-th prover's message, all
% $\prover$'s further messages are determined by the witness it knows, the
% messages it already send and received and the future challenges from the
% verifier.
We note that the definition above is independent on whether the proof
system $\proofsystem$ utilises SRS (and compounds of the SRS-generating $\kgen$
algorithm) or not.

\subsection{Computational special soundness}
Note that the special soundness property (as usually defined) holds for
all---even computationally unbounded---adversaries. Unfortunately, since a
simulation trapdoors for $\plonkprot$ and $\sonicprot$ exist, the protocols
cannot be special sound in that regard. This comes since an unbounded adversary
could reveal the trapdoor and build a number of simulated proofs for a fake
statement. Hence, we provide a weaker, yet sufficient, definition of
\emph{computational special soundness}. More precisely, we state that an
adversary that is able to answer correctly multiple challenges either knows the
witness or can be used to break some computational assumption.

\begin{definition}[Computational special soundness]
  Let $\proofsystem = (\kgen, \prover, \verifier, \simulator)$ be an $(2 \mu +
  1)$-message proof system for a relation $\REL$. We say that $\proofsystem$ is
  $(n_1, \ldots, n_\mu)$-\emph{computationally special sound} if for every $\ppt$ adversary
  $\adv(\REL, \srs)$, where $\srs \gets \kgen(\REL)$, that produces an accepting
  $(n_1, \ldots, n_\mu)$-tree of transcripts $\tree$ for a statement $\inp$
  there exists an extractor $\extt$ that given $\tree$ outputs $\wit$ such that
  $\REL(\inp, \wit) = 1$ with probability $1 - \epsss$, for some negligible $\epsss$.
\end{definition}

Since we do not utilise the classical special soundness (that holds for all,
even unbounded, adversaries) all references to that property should be
understood as references to its computational version.

\section{Simulation-extractability---the general result}
Equipped with definitional framework of \cref{sec:se_definitions} we are ready
to present the main result of this paper---simulation extractability of
multi-round protocols.

\begin{theorem}[Simulation-extractable multi-message protocols]
\label{thm:se}
Let $\ps = (\kgen, \prover, \verifier, \simulator)$ be an interactive
$(2 \mu + 1)$-message proof system for $\RELGEN(\secparam)$ that is honest
verifier zero-knowledge in the standard model\footnote{Crucially, we require
  that one can provide an indistinguishable simulated proof without any
  additional knowledge, as e.g~knowledge of a SRS trapdoor.}, has $\ur{k}$
property with security $\epsur$, is $(n_1, \ldots, n_\mu)$-special
sound for $n_i = 1, i \in \range{1}{\mu} \setminus \smallset{k}$ and $n_k = n$.

Let $\ro\colon \bin^{*} \to \bin^{\secpar}$ be a random oracle. 
Then $\psfs$ is simulation-extractable with extraction error $\epsur$
against $\ppt$ algebraic adversaries that makes up to $q$ random oracle queries and
returns an acceptable proof with probability at least $\accProb$. 
The extraction probability $\extProb$ is at least
\[
	\extProb \geq \frac{1}{q^{n - 1}} (\accProb - \epsur)^{n} -\eps\,,
\]
for some negligible $\eps$.	
\end{theorem}
\begin{proof}		
  The proof goes by game hoping. The games are controlled by an environment
  $\env$ that internally runs a simulation extractability adversary $\advse$,
  provides it with access to a random oracle and simulator, and when necessary
  rewinds it. The games differ by various breaking points, i.e.~points where the
  environment decides to abort the game.

  Denote by $\zkproof_{\advse}, \zkproof_{\simulator}$ proofs returned by the
  adversary and the simulator respectively. We use $\zkproof[i]$ to denote
  prover's message in the $i$-th round of the proof (counting from 1), i.e.~$(2i
  - 1)$-th message exchanged in the protocol. $\zkproof[i].\ch$ denotes the
  challenge that is given to the prover after $\zkproof[i]$, and
  $\zkproof[i..j]$ to denote all messages of the proof including challenges
  between rounds $i$ and $j$, but not challenge $\zkproof[j].\ch$. When it is
  not explicitly stated we denote the proven instance $\inp$ by $\zkproof[0]$
  (however, there is no following challenge $\zkproof[0].\ch$).

  Without loss of generality, we assume that whenever the accepting proof
  contains a response to a challenge from a random oracle, then the
  adversary queried the oracle to get it. It is straightforward to transform any
  adversary that violates this condition into an adversary that makes these
  additional queries to the random oracle and wins with the same probability.

  \ngame{0} This is a simulation extraction game played between an adversary
  $\advse$ who has given access to a random oracle $\ro$ and simulator
  $\psfs.\simulator$. There is also an extractor $\ext$ that, from a proof
  $\zkproof_{\advse}$ for instance $\inp_{\advse}$ output by the adversary and from
   transcripts of $\advse$'s operations is tasked to extract a witness
  $\wit_{\advse}$ such that $\REL(\inp_{\advse}, \wit_{\advse})$ holds. $\advse$ wins
  if it manages to produce an acceptable proof and the extractor fails to reveal
  the corresponding witness. In the following game hops we upper-bound the
  probability that this happens.

  \ngame{1} This is identical to $\game{0}$ except that now the game is aborted
  if there is a simulated proof $\zkproof_\simulator$ for $\inp_{\advse}$ such
  that $(\inp_{\advse}, \zkproof_\simulator[1..k]) = (\inp_{\advse},
  \zkproof_{\advse}[1..k])$. That is, the adversary in its final proof
  reuses a part of a simulated proof it saw before and the proof is acceptable.
  Denote that event by $\event{\errur}$.

  \ncase{$\game{0} \mapsto \game{1}$} We have, \( \prob{\game{0} \land
    \nevent{\errur}} = \prob{\game{1} \land \nevent{\errur}} \) and, from the
  difference lemma, cf.~\cref{lem:difference_lemma},
  \[ \abs{\prob{\game{0}} - \prob{\game{1}}} \leq \prob{\event{\errur}}\,. \]
  Thus, to show that the transition from one game to another introduces only
  minor change in probability of $\advse$ winning it should be shown that
  $\prob{\event{\errur}}$ is small.

  We can assume that $\advse$ queried the simulator on the instance it wishes to
  output---$\inp_{\advse}$. We show a reduction $\rdvur$ that utilises $\advse$,
  who outputs a valid proof for $\inp_{\advse}$, to break the $\ur{k}$ property of
  $\ps$. Let $\rdvur$ run $\advse$ internally as a black-box:
\begin{itemize}
	\item The reduction answers both queries to the simulator $\psfs.\simulator$ and to the random oracle. 
	It also keeps lists $Q$, for the simulated proofs, and $Q_\ro$ for the random oracle queries. 
\item When $\advse$ makes a fake proof $\zkproof_{\advse}$ for $\inp_{\advse}$,
  $\rdvur$ looks through lists $Q$ and $Q_\ro$ until it finds
  $\zkproof_{\simulator}[0..k]$ such that
  $\zkproof_{\advse}[0..k] = \zkproof_{\simulator}[0..k]$
  and a random oracle query $\zkproof_{\simulator}[k].\ch$ on
  $\zkproof_{\simulator}[0..k]$.
	\item $\rdvur$ returns two proofs for $\inp_{\advse}$:
	\begin{align*}
		\zkproof_1 = (\zkproof_{\simulator}[1..k],
		\zkproof_{\simulator}[k].\ch, \zkproof_{\simulator}[k + 1..\mu + 1])\\
		\zkproof_2 = (\zkproof_{\simulator}[1..k],
		\zkproof_{\simulator}[k].\ch, \zkproof_{\advse}[k + 1..\mu + 1])
	\end{align*}
	\end{itemize}  
	If $\zkproof_1 = \zkproof_2$, then $\advse$ fails to break simulation
  extractability, as $\zkproof_2 \in Q$. On the other hand, if the proofs are
  not equal, then $\rdvur$ breaks $\ur{k}$-ness of $\ps$, what may happen with
  some negligible probability $\epsur$ only, hence \( \prob{\event{\errur}} \leq
  \epsur\,. \)
	
  \ngame{2} This is identical to $\game{1}$ except that now the environment
  aborts also when it fails to build a $(1, \ldots, 1, n, 1, \ldots, 1)$-tree
  of accepting transcripts $\tree$ by rewinding $\advse$. Denote that event by
  $\event{\errfrk}$. 

  \ncase{$\game{1} \mapsto \game{2}$} Note that for every acceptable proof
  $\zkproof_{\advse}$, we may assume that whenever $\advse$ outputs in Round $k$
  message $\zkproof_{\advse}[k]$, then
  $(\inp_{\advse}, \zkproof_{\advse}[1..k])$ random oracle query that was made
  by the adversary, not the simulator\footnote{\cite{INDOCRYPT:FKMV12} calls
    these queries \emph{fresh}.}, i.e.~there is no simulated proof
  $\zkproof_\simulator$ on $\inp_\simulator$ such that
  $(\inp_{\advse}, \zkproof_{\advse} [1..k]) = (\inp_\simulator,
  \zkproof_\simulator[1..k])$. Otherwise, the game would be already interrupted
  by the error event in Game $\game{1}$.  As previously,
\[
  \abs{\prob{\game{1}} - \prob{\game{2}}} \leq \prob{\event{\errfrk}}\,.
\]

We describe our extractor $\ext$ here. The extractor takes as input relation
$\REL$, SRS $\srs$, $\advse$'s code, its randomness $r$, the output instance
$\inp_{\advse}$ and proof $\zkproof_{\advse}$, as well as the list $Q$ of
simulated proofs (and their instances) and the list of random oracle queries and
responses $Q_\ro$. Then, $\ext$ starts a forking algorithm
$\genforking^{n}_\zdv(y,h_1, \ldots, h_q)$ for
$y = (\REL, \srs, \advse, r, \inp_{\advse}, \zkproof_{\advse}, Q)$ where we set
$h_1, \ldots, h_q$ to be the consecutive queries from list $Q_\ro$. We run
$\advse$ internally in $\zdv$.% which returns the proof $\zkproof$ and index $i$
%of the random oracle query that $\advse$ used to answer $\zkproof$'s $k$-th challenge. 

To assure that in the first execution of $\zdv$ the adversary $\advse$ produce
the same $(\inp_{\advse}, \zkproof_{\advse})$ as in the extraction game, $\zdv$
provides $\advse$ with the same randomness $r$ and answers queries to the random
oracle and simulator with responses pre-recorded responses in $Q_\ro$ and $Q$.
%
Note, that since the view of the adversary run inside $\zdv$ is the same as its
view with access to real random oracle and simulator, it produces exactly the
same output. After the first run, $\zdv$ outputs the index $i$ of a random oracle
query that was used by $\advse$ to compute the challenge $\zkproof[k].\ch =
\ro(\zkproof_{\advse}[0..k])$ it had to answer in the $(k + 1)$-th round and
adversary's transcript, denoted by $s_1$ in $\genforking$'s description. If no
such query took place $\zdv$ outputs $i = 0$.

Then new random oracle responses are picked for queries indexed by
$i, \ldots, q$ and the adversary is rewound to the point just prior it gets the
response to RO query $\zkproof_{\advse}[0..k]$. The adversary gets a random
oracle response from a new set of responses $h^2_i, \ldots, h^2_q$. If the
adversary requests a simulated proof after seeing $h^2_i$ then $\zdv$ computes
the simulated proof on its own. Eventually, $\zdv$ outputs index $i'$ of a query
that was used by the adversary to compute $\ro(\zkproof_{\advse}[0..k])$, and a
new transcript $s_2$. $\zdv$ is run $n + 1$ times with different random oracle
responses. If a tree $\tree$ of $n + 1$ transcripts is build then $\ext$
runs internally the tree extractor $\extt(\tree)$ and outputs what it returns.

We emphasize here the importance of the unique response property. If it does not
hold then in some $j$-th execution of $\zdv$ the adversary could reuse a
challenge that it learnt from observing proofs in $Q$. In that case, $\zdv$
would output $i = 0$, making extractor fail. Fortunately, the case that the
adversary breaks the unique response property has already been covered by the
abort condition in $\game{1}$.

Denote by $\waccProb$ the probability that $\advse$ outputs a proof that is
accepted and does not break $\ur{k}$-ness of $\ps$.  Denote by $\waccProb'$
probability that algorithm $\zdv$, defined in the lemma, produces an accepting
proof with a fresh challenge after Round $k$. Given the discussion above, we can
state that $\waccProb = \waccProb'$.
 % Assuming that $\advse$ does
% not break the unique response property is necessary to be able to use the
% forking lemma. Denote by $\waccProb'$ probability that algorithm $\zdv$, defined
% in the lemma, produces an accepting proof with a fresh challenge after Round $k$.
% %
% Otherwise, if $\ur{k}$-ness does not hold and the adversary could, say,
% rerandomise a proof $\zkproof$ it obtained from the simulator then it could
% force $\zdv$ to output $i = 0$ (i.e.~not accept the proof) making extractor
% fail. The unique response property takes care of that, hence that event has
% already been covered by the abort condition in $\game{1}$. If an adversary would
% like to reuse a simulated proof it saw, it would end up with exactly the same
% proof which can also can not break simulation extractability, thus
% $\waccProb=\waccProb'$.

Next, from the generalised forking lemma, cf.~\cref{lem:generalised_forking_lemma},
\[
	\prob{\event{\errfrk}} \leq 1 -
	\waccProb \cdot \left(\frac{\waccProb^{n - 1}}{q^{n - 1}} +
	\frac{(2^\secpar) !}{(2^\secpar - n)! \cdot
(2^\secpar)^{n}} - 1\right)\,.
\]
For the sake of simplicity we loose this approximation a bit and state
\[
	\prob{\event{\errfrk}} \leq 1 -
	\left(\frac{\waccProb^{n}}{q^{n - 1}} +
		\waccProb \cdot \left(\frac{2^\secpar - n}{2^\secpar}\right)^{n} -
	\waccProb\right)\,.
\]
\ngame{3} This is identical to $\game{2}$ except that now the game is aborted if
$\extt(\tree)$ run by $\ext$ fails to extract the witness. Denote that event by
$\event{\errss}$.

\ncase{$\game{2} \mapsto \game{3}$}	
As previously, 
\[
	\abs{\prob{\game{2}} - \prob{\game{3}}} \leq \event{\errss}\,.
\]
Since $\ps$ is special-sound the probability of $\event{\errss}$ is
upper-bounded by some negligible $\eps_\ss$.

Game $\game{3}$ is aborted when it is impossible to extract the correct witness
from $\tree$, hence the adversary $\advse$ cannot win.  Thus, by the game-hoping
argument,
\[
	\abs{\prob{\game{0}} - \prob{\game{3}}} \leq 1 -
	\left(\frac{\waccProb^{n}}{q^{n - 1}} + \waccProb \cdot
	\left(\frac{2^\secpar - n}{2^\secpar}\right)^{n} - \waccProb\right) + \epsur + \epsss\,.
\]
Thus the probability that extractor $\extss$ succeeds is at least
\[
	\frac{\waccProb^{n}}{q^{n - 1}} + 
	\waccProb \cdot
	\left( \frac{2^\secpar - n}{2^\secpar}\right)^{n} -
\waccProb - \epsur - \epsss\,.
\]
Since $\waccProb$ is probability of $\advse$ outputting acceptable transcript
that does not break $\ur{k}$-ness of $\ps$, then $\waccProb \geq \accProb -
\epsur$, where $\accProb$ is the probability of $\advse$ outputing an acceptable
proof as defined in \cref{def:simext}. It thus holds
\[
	\label{eq:frk}
	\extProb \geq \frac{(\accProb - \epsur)^{n}}{q^{n - 1}} -
	\underbrace{(\accProb - \epsur) \cdot \left( 1 -
      \left(\frac{2^\secpar - n}{2^\secpar}\right)^{n}\right)
- \epsur - \epsss}_{\eps}\,.
\]
Note that the part of \cref{eq:frk} denoted by $\eps$ is negligible as
$\epsur, \epsss$ are negligible, and $\left(\infrac{(2^\secpar
- n)}{2^\secpar}\right)^{n}$ is overwhelming.
Thus, 
\[
	\extProb \geq \frac{1}{q^{n - 1}} (\accProb - \epsur)^{n} -\eps\,.
\] 
thus
$\psfs$ is simulation extractable with extraction error $\epsur$.
\qed
\end{proof}

\section{Simulation extractability of $\plonkprotfs$} 
In this section we show that $\plonkprotfs$ is simulation-extractable. To that
end, we proceed as follows. First we show that the version of the KZG polynomial
commitment scheme that is proposed in the \plonk{} paper has the unique opening
property, cf.~\cref{sec:poly_com} and \cref{lem:pcomp_unique_op}. This is then
used to show that $\plonkprot$ has the $\ur{3}$ property,
cf.~\cref{lem:plonkprot_ur}.

Next, we show that $\plonkprot$ is computationally special-sound. That is, given a
number of acceptable transcripts which match on the first 3 rounds of the
protocol we can either reveal a correct witness for the proven statement or use
one of the transcripts to break the $\dlog$ assumption. This result is shown in
the AGM, cf.~\cref{lem:plonkprot_ss}.

Given special-soundness of $\plonkprot$, we use the fact that it is also
$\ur{3}$ and show, in a similar fashion to \cite{INDOCRYPT:FKMV12}, that it is
simulation-extractable. That is, we build reductions that given a simulation
extractability adversary $\advse$ either break the protocol's unique response
property or break the $\dlog$ assumption, if extracting a valid witness from a
tree of transcripts is impossible. See \cref{thm:plonkprotfs_se}.

\subsection{$\plonk$ protocol rolled out}
\newcommand{\vql}{\vec{q_{L}}}
\newcommand{\vqr}{\vec{q_{R}}}
\newcommand{\vqm}{\vec{q_{M}}}
\newcommand{\vqo}{\vec{q_{O}}}
\newcommand{\vx}{\vec{x}}
\newcommand{\vqc}{\vec{q_{C}}}
\subsubsection{The constrain system}
Assume $\CRKT$ is a fan-in two arithmetic circuit,
which fan-out is unlimited and has $\numberofconstrains$ gates and $\noofw$ wires
($\numberofconstrains \leq \noofw \leq 2\numberofconstrains$). \plonk's constraint
system is defined as follows:
\begin{itemize}
\item Let $\vec{V} = (\va, \vb, \vc)$, where $\va, \vb, \vc
  \in \range{1}{\noofw}^\numberofconstrains$. Entries $\va_i, \vb_i, \vc_i$ represent indices of left,
  right and output wires of circuits $i$-th gate.
\item Vectors $\vec{Q} = (\vql, \vqr, \vqo, \vqm, \vqc) \in
  (\FF^\numberofconstrains)^5$ are called \emph{selector vectors}:
  \begin{itemize}
  \item If the $i$-th gate is a multiplicative gate then $\vql_i = \vqr_i = 0$,
    $\vqm_i = 1$, and $\vqo_i = -1$. 
  \item If the $i$-th gate is an addition gate then $\vql_i = \vqr_i  = 1$, $\vqm_i =
    0$, and $\vqo_i = -1$. 
  \item $\vqc_i = 0$ always. 
  \end{itemize}
\end{itemize}

We say that vector $\vx \in \FF^\noofw$ satisfies constraint system if for all $i
\in \range{1}{\numberofconstrains}$
\[
  \vql_i \cdot \vx_{\va_i} + \vqr_i \cdot \vx_{\vb_i} + \vqo \cdot \vx_{\vc_i} +
  \vqm_i \cdot (\vx_{\va_i} \vx_{\vb_i}) + \vqc_i = 0. 
\]

\subsubsection{Algorithms rolled out}
\label{sec:plonk_explained}
\plonk{} argument system is universal. That is, it allows to verify computation
of any arithmetic circuit which has no more than $\numberofconstrains$
gates using a single SRS. However, to make computation efficient, for each
circuit there is allowed a preprocessing phase which extend the SRS with
circuit-related polynomial evaluations.

For the sake of simplicity of the security reductions presented in this paper, we
include in the SRS only these elements that cannot be computed without knowing
the secret trapdoor $\chi$. The rest of the SRS---the preprocessed input---can
be computed using these SRS elements thus we leave them to be computed by the
prover, verifier, and simulator.

\paragraph{$\plonk$ SRS generating algorithm $\kgen(\REL)$:}
The SRS generating algorithm picks at random $\chi \sample \FF_p$, computes
and outputs
\[
	\srs = \left(\gone{\smallset{\chi^i}_{i = 0}^{\numberofconstrains + 2}},
	\gtwo{\chi} \right).
\]

\paragraph{Preprocessing:}
Let $H = \smallset{\omega^i}_{i = 1}^{\numberofconstrains }$ be a
(multiplicative) $\numberofconstrains$-element subgroup of a field $\FF$
compound of $\numberofconstrains$-th roots of unity in $\FF$. Let $\lag_i(X)$ be
the $i$-th element of an $\numberofconstrains$-elements Lagrange basis. During
the preprocessing phase polynomials $\p{S_{id j}}, \p{S_{\sigma j}}$, for
$\p{j} \in \range{1}{3}$, are computed:
\begin{equation*}
  \begin{aligned}
    \p{S_{id 1}}(X) & = X,\vphantom{\sum_{i = 1}^{\noofc} \sigma(i) \lag_i(X),}\\
    \p{S_{id 2}}(X) & = k_1 \cdot X,\vphantom{\sum_{i = 1}^{\noofc} \sigma(i) \lag_i(X),}\\
    \p{S_{id 3}}(X) & = k_2 \cdot X,\vphantom{\sum_{i = 1}^{\noofc} \sigma(i) \lag_i(X),}
  \end{aligned}
  \qquad
\begin{aligned}
  \p{S_{\sigma 1}}(X) & = \sum_{i = 1}^{\noofc} \sigma(i) \lag_i(X),\\
  \p{S_{\sigma 2}}(X) & = \sum_{i = 1}^{\noofc}
  \sigma(\noofc + i) \lag_i(X),\\
  \p{S_{\sigma 3}}(X) & =\sum_{i = 1}^{\noofc} \sigma(2 \noofc + i) \lag_i(X).
\end{aligned}
\end{equation*}
Coefficients $k_1$, $k_2$ are such that $H, k_1 \cdot H, k_2 \cdot H$ are
different cosets of $\FF^*$, thus they define $3 \cdot \noofc$
different elements. \cite{EPRINT:GabWilCio19} notes that it is enough to set
$k_1$ to a quadratic residue and $k_2$ to a quadratic non-residue.

Furthermore, we define polynomials $\p{q_L}, \p{q_R}, \p{q_O}, \p{q_M}, \p{q_C}$
such that
\begin{equation*}
  \begin{aligned}
  \p{q_L}(X) & = \sum_{i = 1}^{\noofc} \vql_i \lag_i(X), \\
  \p{q_R}(X) & = \sum_{i = 1}^{\noofc} \vqr_i \lag_i(X), \\
  \p{q_M}(X) & = \sum_{i = 1}^{\noofc} \vqm_i \lag_i(X),
\end{aligned}
\qquad
\begin{aligned}
  \p{q_O}(X) & = \sum_{i = 1}^{\noofc} \vqo_i \lag_i(X), \\
  \p{q_C}(X) & = \sum_{i = 1}^{\noofc} \vqc_i \lag_i(X). \\
  \vphantom{\p{q_M}(X)  = \sum_{i = 1}^{\noofc} \vqm_i \lag_i(X),}
\end{aligned}
\end{equation*}

\paragraph{$\plonk$ prover
  $\prover(\REL, \srs, \inp, \wit = (\wit_i)_{i \in \range{1}{3 \cdot
      \noofc}})$.}
\begin{description}
\item[Round 1] Sample $b_1, \ldots, b_9 \sample \FF_p$; compute
  $\p{a}(X), \p{b}(X), \p{c}(X)$ as
	\begin{align*}
		\p{a}(X) &= (b_1 X + b_2)\p{Z_H}(X) + \sum_{i = 1}^{\noofc} \wit_i \lag_i(X) \\
		\p{b}(X) &= (b_3 X + b_4)\p{Z_H}(X) + \sum_{i = 1}^{\noofc} \wit_{\noofc + i} \lag_i(X) \\
		\p{c}(X) &= (b_5 X + b_6)\p{Z_H}(X) + \sum_{i = 1}^{\noofc} \wit_{2 \cdot \noofc + i} \lag_i(X) 
	\end{align*}
	Output polynomial commitments $\gone{\p{a}(\chi), \p{b}(\chi), \p{c}(\chi)}$.
	
	\item[Round 2]
	Get challenges $\beta, \gamma \in \FF_p$
	\[
		\beta = \ro(\zkproof[0..1], 0)\,, \qquad \gamma = \ro(\zkproof[0..1], 1)\,.
	\]
	Compute permutation polynomial $\p{z}(X)$
	\begin{multline*}
		\p{z}(X) = (b_7 X^2 + b_8 X + b_9)\p{Z_H}(X) + \lag_1(X) + \\
			+ \sum_{i = 1}^{\noofc - 1} 
			\left(\lag_{i + 1} (X) \prod_{j = 1}^{i} 
			\frac{
			(\wit_j +\beta \omega^{j - 1} + \gamma)(\wit_{\noofc + j} + \beta k_1 \omega^{j - 1} + \gamma)(\wit_{2 \noofc + j} +\beta k_2 \omega^{j- 1} + \gamma)}
			{(\wit_j+\sigma(j) \beta + \gamma)(\wit_{\noofc + j} + \sigma(\noofc + j)\beta + \gamma)(\wit_{2 \noofc + j} + \sigma(2 \noofc + j)\beta + \gamma)}\right)
	\end{multline*}
	Output polynomial commitment $\gone{\p{z}(\chi)}$
		
	\item[Round 3]
	Get the challenge $\alpha = \ro(\zkproof[0..2])$, compute the quotient polynomial 
	\begin{align*}
	& \p{t}(X)  = \\
	& (\p{a}(X) \p{b}(X) \selmulti(X) + \p{a}(X) \selleft(X) + 
	\p{b}(X)\selright(X) + \p{c}(X)\seloutput(X) + \pubinppoly(X) + \selconst(X)) 
	\frac{1}{\p{Z_H}(X)} +\\
	& + ((\p{a}(X) + \beta X + \gamma) (\p{b}(X) + \beta k_1 X + \gamma)(\p{c}(X) 
	+ \beta k_2 X + \gamma)\p{z}(X)) \frac{\alpha}{\p{Z_H}(X)} \\
	& - (\p{a}(X) + \beta \p{S_{\sigma 1}}(X) + \gamma)(\p{b}(X) + \beta 
	\p{S_{\sigma 2}}(X) + \gamma)(\p{c}(X) + \beta \p{S_{\sigma 3}}(X) + 
	\gamma)\p{z}(X \omega))  \frac{\alpha}{\p{Z_H}(X)} \\
	& + (\p{z}(X) - 1) \lag_1(X) \frac{\alpha^2}{\p{Z_H}(X)}
	\end{align*}
	Split $\p{t}(X)$ into degree less then $\noofc$ polynomials $\p{t_{lo}}(X), \p{t_{mid}}(X), \p{t_{hi}}(X)$, such that
	\[
		\p{t}(X) = \p{t_{lo}}(X) + X^{\noofc} \p{t_{mid}}(X) + X^{2 \noofc} \p{t_{hi}}(X)\,.
	\]
	Output $\gone{\p{t_{lo}}(\chi), \p{t_{mid}}(\chi), \p{t_{hi}}(\chi)}$.
	
	\item[Round 4]
	Get the challenge $\chz \in \FF_p$, $\chz = \ro(\zkproof[0..3])$.
	Compute opening evaluations
	\begin{align*}
      \p{a}(\chz), \p{b}(\chz), \p{c}(\chz), \p{S_{\sigma 1}}(\chz), \p{S_{\sigma 2}}(\chz), \p{t}(\chz), \p{z}(\chz \omega),
	\end{align*}
	Compute the linearisation polynomial
	\[
		\p{r}(X) = 
		\begin{aligned}
          & \p{a}(\chz) \p{b}(\chz) \selmulti(X) + \p{a}(\chz) \selleft(X) + \p{b}(\chz) \selright(X) + \p{c}(\chz) \seloutput(X) + \selconst(X) \\
          & + \alpha \cdot \left( (\p{a}(\chz) + \beta \chz + \gamma) (\p{b}(\chz) + \beta k_1 \chz + \gamma)(\p{c}(\chz) + \beta k_2 \chz + \gamma) \cdot \p{z}(X)\right) \\
          & - \alpha \cdot \left( (\p{a}(\chz) + \beta \p{S_{\sigma 1}}(\chz) + \gamma) (\p{b}(\chz) + \beta \p{S_{\sigma 2}}(\chz) + \gamma)\beta \p{z}(\chz\omega) \cdot \p{S_{\sigma 3}}(X)\right) \\
          & + \alpha^2 \cdot \lag_1(\chz) \cdot \p{z}(X)
		\end{aligned}
	\]
	Output $\p{a}(\chz), \p{b}(\chz), \p{c}(\chz), \p{S_{\sigma 1}}(\chz), \p{S_{\sigma 2}}(\chz), \p{t}(\chz), \p{z}(\chz \omega), \p{r}(\chz).$
	
	\item[Round 5]
	Compute the opening challenge $v \in \FF_p$, $v = \ro(\zkproof[0..4])$.
	Compute the openings for the polynomial commitment scheme 
	\begin{align*}
	& \p{W_\chz}(X) = \frac{1}{X - \chz} \left(
	\begin{aligned}
		& \p{t_{lo}}(X) + \chz^\noofc \p{t_{mid}}(X) + \chz^{2 \noofc} \p{t_{hi}}(X) - \p{t}(\chz)\\
		& + v(\p{r}(X) - \p{r}(\chz)) \\
		& + v^2 (\p{a}(X) - \p{a}(\chz))\\
		& + v^3 (\p{b}(X) - \p{b}(\chz))\\
		& + v^4 (\p{c}(X) - \p{c}(\chz))\\
		& + v^5 (\p{S_{\sigma 1}}(X) - \p{S_{\sigma 1}}(\chz))\\
		& + v^6 (\p{S_{\sigma 2}}(X) - \p{S_{\sigma 2}}(\chz))
	\end{aligned}
	\right)\\
	& \p{W_{\chz \omega}}(X) = \frac{\p{z}(X) - \p{z}(\chz \omega)}{X - \chz \omega}
\end{align*}
	Output $\gone{\p{W_{\chz}}(\chi), \p{W_{\chz \omega}}(\chi)}$.
\end{description}

\ncase{$\plonk$ verifier $\verifier(\REL, \srs, \inp, \zkproof)$}\ \newline
The \plonk{} verifier works as follows
\begin{description}
	\item[Step 1] Validate all obtained group elements.
	\item[Step 2] Validate all obtained field elements.
	\item[Step 3] Validate the instance
      $\inp = \smallset{\wit_i}_{i = 1}^\instsize$.
	\item[Step 4] Compute challenges $\beta, \gamma, \alpha, \chz, v,
      u$ from the transcript.
	\item[Step 5] Compute zero polynomial evaluation
      $\p{Z_H} (\chz) =\chz^\noofc - 1$.
	\item[Step 6] Compute Lagrange polynomial evaluation
      $\lag_1 (\chz) = \frac{\chz^\noofc -1}{\noofc (\chz - 1)}$.
	\item[Step 7] Compute public input polynomial evaluation
      $\pubinppoly (\chz) = \sum_{i \in \range{1}{\instsize}} \wit_i
      \lag_i(\chz)$.
	\item[Step 8] Compute quotient polynomials evaluations
	\begin{multline*}
      \p{t} (\chz) = \frac{1}{\p{Z_H}(\chz)} \Big(
      \p{r} (\chz) + \pubinppoly(\chz) - (\p{a}(\chz) + \beta \p{S_\sigma 1}(\chz) + \gamma) (\p{b}(\chz) + \beta \p{S_\sigma 2}(\chz) + \gamma) \\
      (\p{c}(\chz) + \gamma)\p{z}(\chz \omega) \alpha - \lag_1 (\chz) \alpha^2
      \Big) \,.
	\end{multline*}
	\item[Step 9] Compute batched polynomial commitment
	$\gone{D} = v \gone{r} + u \gone {z}$ that is
	\begin{align*}
		\gone{D} & = v
		\left(
		\begin{aligned}
          & \p{a}(\chz)\p{b}(\chz) \cdot \gone{\selmulti} + \p{a}(\chz)  \gone{\selleft} + \p{b}  \gone{\selright} + \p{c}  \gone{\seloutput} + \\
          & + (	(\p{a}(\chz) + \beta \chz + \gamma) (\p{b}(\chz) + \beta k_1 \chz + \gamma) (\p{c} + \beta k_2 \chz + \gamma) \alpha  + \lag_1(\chz) \alpha^2)  + \\
			% &   \\
          & - (\p{a}(\chz) + \beta \p{S_{\sigma 1}}(\chz) + \gamma) (\p{b}(\chz)
          + \beta \p{S_{\sigma 2}}(\chz) + \gamma) \alpha \beta \p{z}(\chz
          \omega) \gone{\p{S_{\sigma 3}}(\chi)})
		\end{aligned}
		\right) + \\
		& + u \gone{\p{z}(\chi)}\,.
	\end{align*}
	\item[Step 10] Computes full batched polynomial commitment $\gone{F}$:
	\begin{align*}
      \gone{F} & = \left(\gone{\p{t_{lo}}(\chi)} + \chz^\noofc \gone{\p{t_{mid}}(\chi)} + \chz^{2 \noofc} \gone{\p{t_{hi}}(\chi)}\right) + u \gone{\p{z}(\chi)} + \\
               & + v
                 \left(
		\begin{aligned}
			& \p{a}(\chz)\p{b}(\chz) \cdot \gone{\selmulti} + \p{a}(\chz)  \gone{\selleft} + \p{b}(\chz)   \gone{\selright} + \p{c}(\chz)  \gone{\seloutput} + \\
			& + (	(\p{a}(\chz) + \beta \chz + \gamma) (\p{b}(\chz) + \beta k_1 \chz + \gamma) (\p{c}(\chz)  + \beta k_2 \chz + \gamma) \alpha  + \lag_1(\chz) \alpha^2)  + \\
			% &   \\
			& - (\p{a}(\chz) + \beta \p{S_{\sigma 1}}(\chz) + \gamma) (\p{b}(\chz) + \beta \p{S_{\sigma 2}}(\chz) + \gamma) \alpha  \beta \p{z}(\chz \omega) \gone{\p{S_{\sigma 3}}(\chi)})
		\end{aligned}
		\right) \\
		& + v^2 \gone{\p{a}(\chi)} + v^3 \gone{\p{b}(\chi)} + v^4 \gone{\p{c}(\chi)} + v^5 \gone{\p{S_{\sigma 1}(\chi)}} + v^6 \gone{\p{S_{\sigma 2}}(\chi)}\,.
	\end{align*}
	\item[Step 11] Compute group-encoded batch evaluation $\gone{E}$
	\begin{align*}
		\gone{E}  = \frac{1}{\p{Z_H}(\chz)} & \gone{
		\begin{aligned}
			& \p{r}(\chz) + \pubinppoly(\chz) +  \alpha^2  \lag_1 (\chz) + \\
			& - \alpha \left( (\p{a}(\chz) + \beta \p{S_{\sigma 1}} (\chz) + \gamma) (\p{b}(\chz) + \beta \p{S_{\sigma 2}} (\chz) + \gamma) (\p{c}(\chz) + \gamma) \p{z}(\chz \omega) \right)
		\end{aligned}
           }\\
      + & \gone{v \p{r}(\chz) + v^2 \p{a}(\chz) + v^3 \p{b}(\chz) + v^4 \p{c}(\chz) + v^5 \p{S_{\sigma 1}}(\chz) + v^6 \p{S_{\sigma 2}}(\chz) + u \p{z}(\chz \omega) }\,.
	\end{align*}
	\item[Step 12] Check whether the verification $\vereq(\chi)$ equation holds
	\begin{multline}
		\label{eq:ver_eq}
		\left( \gone{\p{W_{\chz}}(\chi)} + u \cdot \gone{\p{W_{\chz
                \omega}}(\chi)} \right) \bullet
		\gtwo{\chi} - %\\
		\left( \chz \cdot \gone{\p{W_{\chz}}(\chi)} + u \chz \omega \cdot
          \gone{\p{W_{\chz \omega}}(\chi)} + \gone{F} - \gone{E} \right) \bullet
        \gtwo{1} = 0\,.
	\end{multline}
  The verification equation is a batched version of the verification equation
  from \cite{AC:KatZavGol10} which allows the verifier to check openings of
  multiple polynomials in two points (instead of checking an opening of a single
  polynomial at one point).
\end{description}

\subsection{Unique opening property of $\PCOMp$}
\begin{lemma}
\label{lem:pcomp_unique_op}
Let $\PCOMp$ be a batched version of a KZG polynomial commitment
\cite{AC:KatZavGol10} as described in \cite{EPRINT:GabWilCio19} then $\PCOMp$
has the unique opening property in the AGM. 
\end{lemma}
\begin{proof}
  Let $\vec{z} = (z, z') \in \FF_p^2$ be the two points the polynomials are
  evaluated at, $k \in \NN$ be the number of the committed polynomials to be
  evaluated at $z$, and $k' \in \NN$ be the number of the committed polynomials
  to be evaluated at $z'$, $\vec{c} \in \GRP^k, \vec{c'} \in \GRP^{k'}$ be the
  commitments, $\vec{s} \in \FF_p^k, \vec{s'} \in \FF_p^{k'}$ the evaluations,
  and $\vec{o} = (o, o') \in \FF_p^2$ be the commitment openings. We need to
  show that for every $\ppt$ adversary $\adv$ the probability
\[
	\Pr
		\left[
			\begin{aligned}
				& \verify(\srs, \vec{c}, \vec{c'}, (z, z'), \vec{s}, \vec{s'}, \vec{o}), \\
				& \verify(\srs, \vec{c}, \vec{c'}, (z, z'), \vec{s}, \vec{s'}, \vec{\tilde{o}}) \\
				& \vec{o} \neq \vec{\tilde{o}}
			\end{aligned}
		\,\left|\,
		\vphantom{\begin{aligned}
			& \verify(\srs, \vec{c}, \vec{c}, \vec{z}, \vec{s}, \vec{s'}, \vec{o}), \\
			& \verify(\srs, \vec{c}, \vec{c}, \vec{z}, \vec{s}, \vec{s'}, \vec{\tilde{o}}) \\
			&\vec{o} \neq \vec{\tilde{o}})
		\end{aligned}}
		\begin{aligned}
			& \srs \gets \kgen(\secparam), \\
			&	(\vec{c}, \vec{c'}, \vec{z}, \vec{s}, \vec{s'}, \vec{o}, \vec{\tilde{o}}) \gets \adv(\srs)
		\end{aligned}
		\right.\right]
	 % \leq \negl.
\]
is at most negligible.

\ncase{Step 1} First, consider a case where the commitment is limited to
commit to multiple polynomials which are evaluated at the same point $z$. As
noted in \cite[Lemma 2.2]{EPRINT:GabWilCio19} it is enough to upper bound the
probability of the adversary succeeding using the idealised verification
equation---which considers equality between polynomials---instead of the real
verification equation---which consider equality of the polynomials' evaluations.
This holds since an adversary that manages to provide a commitment opening that
holds for the real verifier, but does not hold for the idealised verifier can be
used to break the dlog assumption and reveal the secret trapdoor used to produce
the commitment's SRS, cf.~\cite[Lemma 2.2]{EPRINT:GabWilCio19} for more details.

For polynomials $\vec{\p{f}} = \p{f}_1, \ldots, \p{f}_k$, evaluation point $z$, evaluation
result $\vec{s} = s_1, \ldots, s_k$, random $\gamma$, and opening $\p{o}(X)$ the
idealised check verifies that
\begin{equation}
	\sum_{i = 1}^k \gamma^{i - 1} \p{f}_i(X) - \sum_{i = 1}^{k} \gamma^{i - 1} s_i \equiv \p{o}(X) (X - z)\,.
	\label{eq:pcom_idealised_check}
\end{equation}
Since $\p{o}(X)(X - z) \in \FF_p[X]$ then from the uniqueness of polynomial
composition, there is only one $\p{o} (X)$ that fulfils the equation above.

\ncase{Step 2} Second, consider a case when the polynomials are evaluated on two
points $\vec{z} = (z, z')$ and the adversary is asked to provide two openings
$\vec{o} = (o, o')$. Similarly, we analyse the case of the ideal verification.
In that scenario, the verifier checks whether the following equality, for
$\gamma, r'$ picked at random, holds:
\begin{multline}
	\label{eq:ver_eq_poly}
	\sum_{i = 1}^{k} \gamma^{i - 1} \cdot \p{f}_i(X) - \sum_{i = 1}^{k} \gamma^{i - 1} \cdot s_i  + r' \left(\sum_{i = 1}^{k'} \gamma'^{i - 1} \cdot \p{f'}_i(X) - \sum_{i = 1}^{k'} \gamma'^{i - 1} \cdot s'_i \right)\\
	\equiv \p{o}(X)(X - z) + r' \p{o}'(X)(X- z')
\end{multline}
Since $r'$ has been picked at random, probability that \cref{eq:ver_eq_poly} holds while either
\[
	\sum_{i = 1}^{k} \gamma^{i - 1} \cdot \p{f}_i(X) - \sum_{i = 1}^{k}  \gamma^{i - 1} \cdot s_i \not\equiv \p{o}(X)(X - z)
\]
or 
\[
	\sum_{i = 1}^{k'} \gamma'^{i - 1} \cdot \p{f'}_i(X) - \sum_{i = 1}^{k'} \gamma'^{i - 1} \cdot s'_i \not\equiv \p{o'}(X)(X - z')
\]
is negligible~\cite{EPRINT:GabWilCio19}. This brings the proof back to
Step 1 above. \qed
\end{proof}

\subsection{Unique response property}
\begin{lemma}
	\label{lem:plonkprot_ur}
  Let $\PCOMp$ be commitment of knowledge with security $\epsk$,
  $\epsbind$-binding and has unique opening property with security $\epsop$,
  then probability that a $\ppt$ adversary $\adv$ breaks $\plonkprot$'s $\ur{3}$
  property is at most $\epsk + 2\cdot\epsbind +
  \epsop$.% assuming the polynomial commitment scheme $\PCOMp$
%is of knowledge.
\end{lemma}
\begin{proof}[sketch]
  Let
  $\adv(\REL,\srs = (\gone{1, \chi, \ldots, \chi^{\noofc + 2}}, \gtwo{\chi}))$
  be an adversary tasked to break the $\ur{3}$-ness of $\plonkprot$. It is
  sufficient to observe that the first 2 rounds of the protocol determines,
  along with the verifiers challenges, the rest of it.

  In Round 3 the adversary outputs a commitment to polynomial $\pt(X)$ which
  assures that all constraints of the system are fulfilled. Since the commitment
  scheme is deterministic, there is only one value $c$ that is a commitment to
  $\pt(X)$. Assume that $\adv$ outputs $c' \neq c$ and is later able to open
  $c'$ to $y = \pt(\chz)$, where $\chz$ is a random point determined later.
  Using the AGM and arguments similar to \cite{CCS:MBKM19}, we argue that
  $\PCOMp$ is a commitment of knowledge. That is, an AGM adversary $\adv$ that
  outputs a commitment $c'$ which it can later open, knows a polynomial $\p{f}$
  of degree-$(\leq \noofc + 2)$ such that $\gone{\p{f}(\chi)} = c'$. Thus if
  $c' \neq c$ and the commitment scheme is evaluation binding then $\adv$ when
  picking $c'$ picks it as a commitment to $\p{f}$ which evaluates at $\chz$ to
  $\pt(\chz)$. The probability of that is negligible as there can only be
  no more than $\noofc + 2$  overlapping points between $\p{f}(X)$ and $\p{t}(X)$, and
  $\chz$ remains random for $\adv$ when it computes $c'$. Hence, the probability
  that $\adv$ is able to produce two different outputs of Round 3 is
  upper-bounded by $\epsk + \epsbind$.

  In Round 4 the prover is asked to give evaluations of predefined polynomials
  at some point $\chz$. Naturally, for the given polynomials only one value at
  $\chz$ is correct. Assume $\adv$ is able to produce two different outputs in
  that round: $\vec{r_4} = (\ev{\p{a}}, \ev{\p{b}}, \ev{\p{c}}, \ev{\p{S_{\sigma
        1}}}, \ev{\p{S_{\sigma 2}}}, \ev{\p{r}}, \ev{\p{z}})$ and $\vec{r_4} =
  (\ev{\p{a}}', \ev{\p{b}}', \ev{\p{c}}', \ev{\p{S_{\sigma 1}}}',
  \ev{\p{S_{\sigma 2}}}', \ev{\p{r}}', \ev{\p{z}}')$ which suppose to be
  evaluations at $\chz$ of polynomials $\p{a}, \p{b}, \p{c}, \p{S_{\sigma 1}},
  \p{S_{\sigma 2}}, \p{r}$ and an evaluation at $\chz \omega$ of $\p{z}$.
  Clearly, at least one of $\vec{r_4}$, $\vec{r'_4}$ has to be incorrect, thus
  if both evaluations are acceptable by the $\PCOMp.\verify$ then the evaluation
  binding property of $\PCOMp$ is broken. This happens with probability
  upper-bounded by $\epsbind$.

  In the last round of the protocol the prover provides openings for the
  polynomial commitment evaluations done before. Assume $\adv$ is able to
  produce two different polynomial commitment openings pairs: $\vec{r_5} =
  (\ev{\p{W_\chz}}, \ev{\p{W_{\chz \omega}}})$ and $\vec{r'_5} =
  (\ev{\p{W_\chz}}', \ev{\p{W_{\chz \omega}}}')$.
% Since \cref{lem:pcomp_unique_op}, 
  Since $\PCOMp$ has unique opening property, one of the openings has to be
  incorrect and should be rejected by the polynomial commitment verifier. This
  happens except with probability $\epsop$.

  Hence, the probability that after fixing the two first rounds, the adversary
  is able to produce two different outputs in one of the following rounds is
  upper-bounded by
\[
	\epsk + 2 \cdot \epsbind + \epsop\,.
\]
\qed


\end{proof}
\COMMENT{
\begin{lemma}
\label{lem:plonkprot_ur}
If a polynomial commitment scheme $\PCOM$ is evaluation binding with parameter
$\epsbind$ and has unique openings property with parameter $\epsop$, then
$\plonkprot$ is $\ur{2}$. However, in the case presented here, less is required.
with parameter $\epsur \leq \epsbind + \epsop$.
\end{lemma}
\begin{proof}
  Let $\adv$ be an adversary that breaks $\ur{2}$-ness of $\plonkprot$.  We
  consider two cases, depending on which round $\adv$ is able to provide at
  least two different outputs such that the resulting transcripts are
  acceptable.  For the first case we show that $\adv$ breaks the evaluation
  binding property of $\PCOM$, while for the second case we show that it breaks
  the unique opening property of $\PCOM$.

\case{1} In Round 4 the prover is asked to give evaluations of predefined
polynomials at some point $\chz$. Naturally, for the given polynomials only one
value at $\chz$ is correct. Assume $\adv$ is able to produce two different
outputs in that round: $\vec{r_4} = (\ev{\p{a}}, \ev{\p{b}}, \ev{\p{c}},
\ev{\p{S_{\sigma 1}}}, \ev{\p{S_{\sigma 2}}}, \ev{\p{r}}, \ev{\p{z}})$ and
$\vec{r_4} = (\ev{\p{a}}', \ev{\p{b}}', \ev{\p{c}}', \ev{\p{S_{\sigma 1}}}',
\ev{\p{S_{\sigma 2}}}', \ev{\p{r}}', \ev{\p{z}}')$ which suppose to be
evaluations at $\chz$ of polynomials $\p{a}, \p{b}, \p{c}, \p{S_{\sigma 1}},
\p{S_{\sigma 2}}, \p{r}$ and an evaluation at $\chz \omega$ of $\p{z}$. Clearly,
at least one of $\vec{r_4}$, $\vec{r'_4}$ has to be incorrect, thus if both
evaluations are acceptable by the $\PCOM.\verify$ then the evaluation binding
property of $\PCOM$ is broken. This happens with probability upper-bounded by
$\epsbind$.

\case{2} In the last round of the protocol the prover provides openings for the
polynomial commitment evaluations done before. Assume $\adv$ is able to produce
two different polynomial commitment openings pairs: $\vec{r_5} =
(\ev{\p{W_\chz}}, \ev{\p{W_{\chz \omega}}})$ and $\vec{r'_5} =
(\ev{\p{W_\chz}}', \ev{\p{W_{\chz \omega}}}')$.
% Since \cref{lem:pcomp_unique_op},
Since $\PCOM$ has unique opening property, one of the openings has to be
incorrect and should be rejected by the polynomial commitment verifier. This
happens except with probability $\epsop$

\conclude Hence the probability that $\adv$ breaks $\ur{2}$-property of
$\plonkprot$ is upper-bounded by $\epsbind + \epsop$. \qed
\end{proof}
}

\subsection{Computational special soundness}
\begin{lemma}
\label{lem:plonkprot_ss}
Let $\adv$ be a $\ppt$ algebraic adversary. The probability $\epsss$ that $\adv$
breaks $(1, 1, \noofc + 3, 1)$-computational special soundness
of $\plonkprot$ is upper-bounded as
 \[
	\epsss \leq \epsbatch + \epsdlog\,,
 \] 
 where $\epsbatch$ is the (negligible) probability that $\plonkprot$'s idealised
 verification equation $\vereq(X)$ accepts an invalid proof because of batching
 and $\epsdlog$ is a probability that a $\ppt$ algorithm can break the
 $(\noofc + 2, 1)$-$\dlog$ assumption.
\end{lemma}
\begin{proof}
  Let $\srs$ be $\plonkprot$'s SRS and denote by $\srs_1$ all SRS's
  $\GRP_1$-elements; that is, $\srs_1 = \gone{1, \chi, \ldots,
    \chi^{\noofc + 2}}$. Let $\adv$ be an algebraic adversary that
  for a statement $\inp$ produces a $(1, 1, 1, \noofc + 3, 1)$-tree
  of acceptable transcripts
  $\tree$. % with non-negligible probability $\eta_\tree$.
  Note that in all transcripts the instance $\inp$, proof elements
  $\gone{\p{a}(\chi), \p{b}(\chi), \p{c}(\chi), \p{z}(\chi), \p{t}(\chi)}$ and
  challenges $\alpha, \beta, \gamma$ are common as the transcripts share the
  first three rounds.

  We consider two mutually disjunctive events. First, $\event{E}$ holds when all
  of the transcripts are acceptable by the idealised verification equation,
  i.e.~$\vereq(X) = 0$, cf.~\cref{eq:ver_eq}. Second, $\nevent{E}$ holds when
  there is a transcript that is acceptable, yet for some transcript
  $\vereq(\chi) = 0$, but $\vereq(X) \neq 0$. We build a special extractor
  $\extss$ which given the tree of transcripts $\tree$ reveals the witness with
  an overwhelming probability when $\event{E}$ happens. We show a reduction
  $\rdvdlog$ that, when $\nevent{E}$ happens, breaks the $\dlog$ assumption.

  \ncase{When $\event{E}$ happens} Since the protocol $\plonkprot$, instantiated
  with the idealised verification equation, is perfectly sound, except with
  probability of batching failure $\epsbatch$, for a valid proof $\zkproof$ of a
  statement $\inp$ there exists a witness $\wit$, such that $\REL(\inp, \wit)$
  holds. Note that the polynomials $\p{a}(X), \p{b}(X), \p{c}(X)$, which contain
  witness in their coefficients, have degree $(\noofc + 2)$ and
  since $\adv$ answered honestly on $(\noofc + 3)$ different
  challenges $\chz$ then $(\noofc + 3)$ evaluations of these
  polynomials (at different points) are known. The extractor $\extss$
  interpolates the polynomials and reveals the corresponding witness $\wit$.

  \ncase{When $\nevent{E}$ happens} Consider a transcript such that
  $\vereq(X) \neq 0$, but $\vereq(\chi) = 0$. Since the adversary is algebraic,
  all group elements included in the tree of transcripts are extended by their
  representation as a combination of the input $\GRP_1$-elements i.e.~$\gone{1,
    \chi, \ldots, \chi^{\noofc + 2}}$. Hence all coefficients of
  the verification equation polynomial $\vereq(X)$ are known and $\rdvdlog$ can
  find its zero points. Since $\vereq(\chi) = 0$, the targeted discrete log
  value $\chi$ is among them. \qed
\end{proof}

\subsection{Honest verifier zero-knowledge}
\begin{lemma}
  $\plonk$ is honest verifier zero-knowledge and its simulator $\simulator$ does
  not require a SRS trapdoor.\footnote{The simulator works as a simulator for
    proofs that are zero-knowledge in the standard model. However, we do not say
    that $\plonk$ is HVZK in the standard model as proof of that \emph{requires}
    the SRS simulator.} More precisely, assume that $\plonk$'s SRS simulator
  $\simulator_\chi$ produces a proof that is at most $\epszk$ far from a real
  proof, and $(\pR, \pS, \pT, \pf)$-uber assumption for $\pR, \pS, \pT, \pf$ as
  defined in \cref{eq:uber} is $\epsuber$-secure. Then for any $\ppt$ adversary
  $\adv$ its advantage in telling a proof produced by $\simulator$ from a real
  proof is upper-bounded by $\epszk + \epsuber$.
\end{lemma}
\begin{proof}
  The proof goes by game-hoping. The environment that controls the games
  provides the adversary with a SRS $\srs$, then the adversary outputs an
  instance--witness pair $(\inp, \wit)$ and, depending on the game, is provided
  with either real or simulated proof for it. In the end of the game the
  adversary outputs either $0$ if it believes that the proof it saw was provided
  by the simulator and $1$ in the other case.

  \ngame{0} In this game $\adv(\REL, \srs)$ picks an instance--witness pair
  $(\inp, \wit)$ and gets a real proof $\zkproof$ for it.

  \ngame{1} In this game for $\adv(\REL, \srs)$ picks an instance--witness pair
  $(\inp, \wit)$ and gets a proof $\zkproof$ that is simulated by a simulator
  $\simulator_\chi$ which utilises for the simulation the SRS trapdoor and
  proceeds as follows. In the first round the simulator $\simulator_\chi$ picks
  randomisers $b_1, \ldots b_9$, sets $\wit_i = 0$, for $i \in \range{1}{3
    \noofc}$, computes polynomials $\pa(X), \pb(X), \pc(X)$ and
  outputs $\gone{\pa(\chi), \pb(\chi), \pc(\chi)}$. Then it picks Round 1
  challenges $\beta, \gamma$ honestly.

  In the second round $\simulator_\chi$ computes the polynomial $\pz(X)$ and
  outputs $\gone{\pz(\chi)}$. Then it picks randomly Round 2 challenge $\alpha$.

  In the third round the simulator computes polynomial $\pt(X)$ and evaluates it
  at $\chi$, then outputs $\gone{\ptlo(\chi), \ptmid(\chi), \pthi(\chi)}$. Note
  that this evaluation is feasible (in the polynomial time with non-negligible
  probability) only since $\simulator_\chi$ knows the trapdoor.

  In the last two rounds the simulator proceeds as an honest prover would
  proceed and picks corresponding challenges at random as an honest verifier
  would.

  \ncase{$\game{0} \mapsto \game{1}$} Since $\plonk$ is zero-knowledge,
  probability that $\adv$ outputs a different bit in both games is negligible.
  Hence
  \[
	\abs{\prob{\game{0}} - \prob{\game{1}}} \leq \epszk.
\]

\ngame{2} In this game $\adv(\REL, \srs)$ picks an instance--witness pair
$(\inp, \wit)$ and gets a proof $\zkproof$ simulated by the simulator
$\simulator$ which proceeds as follows.

First, since the simulator $\simulator$ does not know a witness $\wit$ for the proven
statement $\inp$, it cannot compute the output of Round 1 accordingly to the
protocol. Instead, it picks randomly both the randomisers $b_1, \ldots, b_6$ and
sets $\wit_i = 0$ for $i \in \range{1}{3\noofc}$. Then $\simulator$
outputs $\gone{\p{a}(\chi), \p{b}(\chi), \p{c}(\chi)}$. For the first round
challenge, the simulator picks permutation argument challenges $\beta, \gamma$
randomly.

In the second round, the simulator computes $\p{z}$ from
the newly picked randomisers $b_7, b_8, b_9$ and coefficients of polynomials
$\p{a}, \p{b}, \p{c}$. Then it evaluates $\p{z}$ honestly and outputs
$\gone{\p{z}(\chi)}$. Challenge $\alpha$ that should be sent by the verifier
after Round 2 is picked by the simulator at random.

The next round starts by the simulator picking at random a challenge $\chz$,
which in the real proof comes as a challenge from the verifier sent \emph{after} Round
3. Then $\simulator$ computes evaluations \(\p{a}(\chz), \p{b}(\chz),
\p{c}(\chz), \p{S_{\sigma 1}}(\chz), \p{S_{\sigma 2}}(\chz), \pubinppoly(\chz),
\lag_1(\chz), \p{Z_H}(\chz),\allowbreak \p{z}(\chz\omega)\) and computes
$\p{t}(X)$ honestly. Since for a random $\p{a}, \p{b}, \p{c}, \p{z}$ the
constraint system is (with overwhelming probability) not satisfied and the
constraints-related polynomials are not divisible by $\p{Z_H}$, $\p{t}(X)$ is a
rational function rather than a polynomial. Then, the simulator evaluates $\p{t}(X)$ at
$\chz$ and picks randomly a degree-$(\noofc + 2)$ polynomial
$\p{\tilde{t}}(X)$ such that $\p{t}(\chz) = \p{\tilde{t}}(\chz)$ and publishes a
commitment $\gone{\p{\tilde{t}}(\chi)}$. After this round the simulator outputs
$\chz$ as a challenge.

In the next round, the simulator computes polynomial $\p{r}(X)$ as an honest
prover would, cf.~\cref{sec:plonk_explained} and evaluates $\p{r}(X)$ at $\chz$.

The rest of the evaluations are already computed, thus 
$\simulator$ simply outputs 
\(
	\p{a}(\chz), \p{b}(\chz), \p{c}(\chz), \p{S_{\sigma 1}}(\chz), \p{S_{\sigma 2}}(\chz), \p{t}(\chz), \p{z}(\chz \omega)\,.
\)
After that it picks randomly the challenge $v$, proceeds in the last round as an
honest prover would proceed and outputs the final challenge, $u$, by picking it
at random as well.

\ncase{$\game{1} \mapsto \game{2}$} We now describe the reduction $\rdv$ which
relies on the $(\pR, \pS, \pT, \pf)$-uber assumption where $\pR, \pS, \pT, \pf$
are polynomials over variables $\vB = B_1, \ldots B_9$ and are defined as
follows. Let $E = \smallset{\smallset{1, 2}, \smallset{3, 4}, \smallset{5, 6},
  \smallset{7, 8, 9}}$. Let
\begin{align}
\label{eq:uber}
\pR(\vB) & = \smallset{B_i \mid i \in A,\ A \in E} \cup \smallset{B_i B_j \mid i \in
A, j \in B,\ A \neq B, A, B \in E} \cup \\ 
		& \smallset{B_i B_j B_k \mid i \in A,\ j \in
B,\ k \in C,\
		A, B, C \text{ all different and in } E} \cup \notag \\
	& \smallset{B_i B_j B_k B_l \mid i \in A,\ j \in B,\ k \in C,\ l \in D,\
		A, B, C, D \text{ all different and in } E} \notag \\
	& \setminus \smallset{B_1
B_3 B_5 B_7}\,,\notag \\
\pS(\vB) & = \emptyset \notag\,, \\
\pT(\vB) & = \emptyset \notag\,, \\
\pf(\vB) & = B_1 B_3 B_5 B_7 \notag\,.
\end{align}
That is, the elements of $\pR$ are all singletons, pairs, triplets and
quadruplets of $B_i$ variables that occur in polynomial $\pt(\vB)$ except the
challenge element $\pf(\vB) = B_1 B_3 B_5 B_7$. Variables $\vB$ are evaluated to
randomly picked $\vb = b_1, \ldots b_9$.

The reduction $\rdv$ learns $\gone{\pR}$ and challenge $\gone{w}$ where $w$ is
either $\pf(\vb) = b_1 b_3 b_5 b_7$ or a random value. Then it picks $\chi$,
$\chz$ and computes the SRS $\srs$ from $\chi$. Elements $b_i$ are interpreted as
polynomials in $X$ that are evaluated at $\chi$, i.e. $b_i = b_i(\chi)$. Next,
$\rdv$ sets
\[
  \gone{\p{\tb}_i}(X) =
(X - \chz)(X - \ochz) \gone{b_i}(X) + \xi_i (X - \chz) \gone{1} +
\zeta_i (X - \ochz) \gone{1}, % \text{ for } i \in % \range{1}{9}, u_1
\] 
for $i \in \range{1}{9}$ and $\xi_i, \zeta_i \sample \FF_p$. Denote by $\tb_i$
evaluations of $\p{\tb}_i$ at $\chi$.
The reduction computes all $\gone{\tb_i \tb_j}, \gone{\tb_i \tb_j \tb_k},
\gone{\tb_i \tb_j \tb_k \tb_l}$ such that
$\gone{B_i B_j, B_i B_j B_k, B_i B_j B_k B_l} \in \pR$.
This is possible since $\rdv$ knows all singletons $\gone{b_1, \ldots,
b_9}$ and pairs $\gone{b_i b_j} \in \pR$ which can be used to compute all
required pairs $\gone{\tb_i \tb_j}$: 
\begin{align*}
\gone{\tb_i \tb_j} 
& = ((\chi - \chz)(\chi - \ochz)\gone{b_i} + \xi_i (\chi - \chz)\gone{1} +
\zeta_i (\chi - \ochz) \gone{1}) 
\cdot \\
 & ((\chi - \chz)(\chi - \ochz)\gone{b_j} + \xi_j (\chi - \chz)\gone{1} +
\zeta_j (\chi - \ochz) \gone{1}) = \\
 & ((\chi - \chz)(\chi - \ochz))^2 \gone{b_i b_j} +  ((\chi - \chz)(\chi -
 \ochz)\gone{b_i} (\xi_j (\chi - \chz) \gone{1} + \zeta_j (\chi - \ochz)
 \gone{1}) + \\
 & ((\chi - \chz)(\chi -
 \ochz)\gone{b_j} (\xi_i (\chi - \chz) \gone{1} + \zeta_i (\chi - \ochz)
 \gone{1}) + \psi,
\end{align*}
where $\psi$ compounds of $\xi_i, \xi_j, \zeta_i, \zeta_j, \chz, \ochz, \chi$ which
are all known by $\rdv$ and no $b_i$ nor $b_j$.
Analogously for the triplets and quadruplets. 

For the challenge $\gone{w}$, $\rdv$ sets $\gone{\tw} = \gone{\tb_1 \tb_3
\tb_5 \tb_7}$, where $\gone{b_1 b_3 b_5 b_7}$ is substituted by $\gone{w}$.
Next it runs the adversary $\adv(\REL, \srs)$ and obtains from $\adv$ an
instance--witness pair $(\inp, \wit)$.  $\rdv$ now prepares a simulated proof as follows:
\begin{description} 
\item[Round 1] $\rdv$ computes $\gone{\pa(\chi)}$ using as
randomisers $\gone{\tb_1}, \gone{\tb_2}$ and setting $\wit_i = 0$, for $i
\in \range{1}{3 \noofc}$. Similarly it computes
$\gone{\pb(\chi)}, \gone{\pc(\chi)}$.  $\rdv$ publishes the obtained values
and picks a Round 1 challenge $\beta, \gamma$ at random.  
\item[Round 2]
$\rdv$ computes $\gone{\pz(\chi)}$ using $\tb_7, \tb_8, \tb_9$ and publishes
it. Then it picks randomly the challenge $\alpha$.  
\item[Round 3] The
reduction computes $\gone{\pt(\chi)}$ using $\tw$ as it was equal $\tb_1
\tb_3 \tb_5 \tb_7$. That is, if $w = b_1 b_3 b_5 b_7$ then $\pt(\chi)$ is as
computed by the simulator $\simulator_\chi$, otherwise, if $w$ is random
then $\pt(\chi)$ is random as well, thus it is computed as $\simulator$
would compute. The reduction computes and outputs $\gone{\ptlo(\chi),
\ptmid(\chi), \pthi(\chi)}$ such that $\pt(X) = \ptlo(X) +
X^\noofc \ptmid(X) + X^{2\noofc} \pthi(X)$.
Eventually, $\rdv$ outputs $\chz$.  
\item[Round 4] The reduction outputs
$\pa(\chz), \pb(\chz), \pc(\chz), \p{S_{\sigma 1}}(\chz), \p{S_{\sigma 2}
	(\chz)}, \pt(\chz), \pz(\ochz)$.  For the sake of concreteness, denote by
	$S = \smallset{\pa, \pb, \pc, \pt, \pz}$. Although for a polynomial $\p{p}
	\in S$, reduction $\rdv$ does not know $\p{p}(\chi)$ or even do not know
	all the coefficients of $\p{p}$, the polynomials in $S$ was computed such
	that the reduction always knows their evaluation at $\chz$ and $\ochz$.
\item[Round 5] $\rdv$ computes the openings of the polynomial commitments
assuring that evaluations at $\chz$ it provided were computed honestly.
\end{description} Is the adversary $\adv$'s output distribution differ in
Game $\game{1}$ and $\game{2}$ then the reduction uses it to distinguish
between $w = b_1 b_3 b_5 b_7$ and $w$ being random, thus \(
\abs{\prob{\game{1}} - \prob{\game{2}}} \leq \epsuber.  \) Eventually, \(
\abs{\prob{\game{0}} - \prob{\game{2}}} \leq \epszk + \epsuber.  \) \qed
\end{proof}

\subsection{From special-soundness and unique response property to simulation
  extractability of $\plonkprotfs$}

Since \cref{lem:plonkprot_ur,lem:plonkprot_ss} hold, $\plonkprot$ is $\ur{3}$
and computationally special sound. We now 
make use of \cref{thm:se} and show that
$\plonkprot_\fs$ is simulation-extractable as defined in \cref{def:simext}.

% \michals{13.10}{The theorem below is going to have a much shorter proof -- for
% it being just a corollary of Sec.~4}
\begin{corollary}[Simulation extractability of $\plonkprot_\fs$]
\label{thm:plonkprotfs_se}
Assume that $\plonkprot$ is $\ur{3}$ with security $\epsur(\secpar)$, and
computational special-sound with security $\epsss(\secpar)$. Let $\ro\colon
\bin^* \to \bin^\secpar$ be a random oracle. Let $\advse$ be a $\ppt$ adversary
that can make up to $q$ random oracle queries and outputs an acceptable proof
for $\plonkprotfs$ with probability at least $\accProb$. Then $\plonkprotfs$ is
simulation-extractable with extraction error $\eta = \epsur$. The extraction
probability $\extProb$ is at least
\[
	\extProb \geq \frac{1}{q^{\noofc + 2}} (\accProb - \epsur)^{\noofc + 3} -\eps\,,
\]
for some negligible $\eps$ and $\noofc$ being the number of
contrains in the proven circuit.
\end{corollary}

\section{Simulation extractability of $\sonicprotfs$}
\label{sec:sonic}
\subsection{\sonic{} protocol rolled out}
In this section we present $\sonic$'s constraint system and algorithms. Reader
familiar with them may jump directly to the next section.

\subsubsection{The constraint system}
\label{sec:sonic_constraint_system}
\sonic's system of constraints composes of three $\multconstr$-long vectors
$\va, \vb, \vc$ which corresponds to left and right inputs to multiplication
gates and their outputs. It hence holds $\va \cdot \vb = \vc$.

There is also $\linconstr$ linear constrains of the form
\[
  \va \vec{u_q} + \vb \vec{v_q} + \vc \vec{w_q} = k_q,
\]
where $\vec{u_q}, \vec{v_q}, \vec{w_q}$ are vectors for the $q$-th linear
constraint with instance value $k_q \in \FF_p$. Furthermore define polynomials
\begin{equation}
  \begin{split}
    \p{u_i}(Y) & = \sum_{q = 1}^\linconstr Y^{q + \multconstr} u_{q, i}\,,\\
    \p{v_i}(Y) & = \sum_{q = 1}^\linconstr Y^{q + \multconstr} v_{q, i}\,,\\
  \end{split}
  \qquad
  \begin{split}
    \p{w_i}(Y) & = -Y^i - Y^{-i} + \sum_{q = 1}^\linconstr Y^{q +
      \multconstr} w_{q, i}\,,\\
    \p{k}(Y) & = \sum_{q = 1}^\linconstr Y^{q + \multconstr} k_{q}.
  \end{split}
\end{equation}
In \sonic{} we will use commitments to the following polynomials.
\begin{align*}
  \pr(X, Y) & = \sum_{i = 1}^{\multconstr} \left(a_i X^i Y^i + b_i X^{-i} Y^{-i}
              + c_i X^{-i - \multconstr} Y^{-i - \multconstr}\right) \\
  \p{s}(X, Y) & = \sum_{i = 1}^{\multconstr} \left( u_i (Y) X^{-i} +
                v_i(Y) X^i + w_i(Y) X^{i + \multconstr}\right)\\
  \pr'(X, Y) & = \pr(X, Y) + \p{s}(X, Y) \\
  \pt(X, Y) & = \pr(X, 1) \pr'(X, Y) - \p{k}(Y)\,.
\end{align*}
	
\subsubsection{Algorithms rolled out}
\paragraph{$\sonic$ SRS generation $\kgen(\REL)$.} The SRS generating algorithm picks
randomly $\alpha, \chi \sample \FF_p$ and outputs
	\[
      \srs = \left( \gone{\smallset{\chi^i}_{i = -\dconst}^{\dconst},
          \smallset{\alpha \chi^i}_{i = -\dconst, i \neq 0}^{\dconst}},
        \gtwo{\smallset{\chi^i, \alpha \chi^i}_{i = - \dconst}^{\dconst}},
        \gtar{\alpha} \right)
	\]
\paragraph{$\sonic$ prover $\prover(\REL, \srs, \inp, \wit=\va, \vb, \vc)$.}
\begin{description}
\item[Round 1] The prover picks randomly randomisers
  $c_{\multconstr + 1}, c_{\multconstr + 2}, c_{\multconstr + 3}, c_{\multconstr
    + 4} \sample \FF_p$. Set
  $\pr(X, Y) \gets \pr(X, Y) + \sum_{i = 1}^4 c_{\multconstr + i} X^{- 2
    \multconstr - i}$. Commits to $\pr(X, 1)$ and outputs
  $\gone{r} \gets \com(\srs, \multconstr, \pr(X, 1))$.  Then it gets challenge $y$ from
  the verifier.
\item[Round 2] $\prover$ commits to $\pt(X, y)$ and outputs
  $\gone{t} \gets \com(\srs, \dconst, \pt(X, y))$. Then it gets a challenge $z$ from
  the verifier.
\item[Round 3] The prover computes commitment openings. That is, it outputs
  \begin{align*}
    \gone{o_a} & = \open(\srs, z, \pr(z, 1), \pr(X, 1)) \\
    \gone{o_b} & = \open(\srs, yz, \pr(yz, 1), \pr(X, 1)) \\
    \gone{o_t} & = \open(\srs, z, \pt(z, y), \pt(X, y)) 
  \end{align*}
  along with evaluations $a' = \pr(z, 1), b' = \pr(y, z), t' = \pt(z, y)$.  Then it
  engages in the signature of correct computation playing the role of the
  helper, i.e.~it commits to $\p{s}(X, y)$ and sends the commitment $\gone{s}$.  Then
  it obtains a challenge $u$ from the verifier.
\item[Round 4] In the next round the prover computes
  $\gone{c} \gets \com(\srs, \dconst, \p{s}(u, x)) \cdot \gone{\p{s}(u, x)}$ and
  compute commitments' openings
  \begin{align*}
    \gone{w} & = \open(\srs, u, \p{s}(X, y)), \\
    \gone{q_y} & = \open(\srs, y, \p{s}(u, Y)),
  \end{align*}
  and returns $\gone{w}, \gone{q_y}, \p{s}(u, y)$. Eventually the prover gets the last challenge
  from the verifier---$z$.
\item[Round 5] In the final round, $\prover$ computes opening
  $\gone{q_z} = \open(\srs, z, \p{s}(u, X))$ and outputs $\gone{q_z}$ and $\p{s}(u, z)$.
\end{description}

\paragraph{$\sonic$ verifier $\verifier(\REL, \srs, \inp, \zkproof)$.} The verifier
in \sonic{} runs as subroutines the verifier for the polynomial commitment. That
is it sets $t' = a'(b' + s) - \p{k}(y)$ and checks the following:
\begin{equation*}
  \begin{split}
    &\PCOMs.\verifier(\srs, \multconstr, \gone{r}, z, a', \gone{o_a}), \\
    &\PCOMs.\verifier(\srs, \multconstr, \gone{r}, yz, b', \gone{o_b}),\\
    &\PCOMs.\verifier(\srs, \dconst, \gone{t}, z, t', \gone{o_t}),\\
  \end{split}
  \qquad
  \begin{split}
    &\PCOMs.\verifier(\srs, \dconst, \gone{s}, u, s, \gone{w}),\\
    &\PCOMs.\verifier(\srs, \dconst, \gone{c}, y, s, \gone{q_y}),\\
    &\PCOMs.\verifier(\srs, \dconst, \gone{c}, z, s_z, \gone{q_z}),
  \end{split}
\end{equation*}
and accepts the proof iff all the checks holds.

\subsection{Unique opening property of $\PCOMs$}
\begin{lemma}
\label{lem:pcoms_unique_op}
$\PCOMs$ has the unique opening property in the AGM. 
\end{lemma}
\begin{proof}
Let 
$z \in \FF_p$ be the attribute the polynomial is evaluated at,
$\gone{c} \in \GRP$ be the commitment,  
$s \in \FF_p$ the evaluation value, and 
$o \in \GRP$ be the commitment opening. 
We need to show that for every $\ppt$ adversary $\adv$ probability
\[
  \Pr \left[
    \begin{aligned}
      & \verify(\srs, \gone{c}, z, s, \gone{o}) = 1, \\
      & \verify(\srs, \gone{c}, z, \tilde{s}, \gone{\tilde{o}}) = 1
    \end{aligned}
    \,\left|\, \vphantom{\begin{aligned}
          & \verify(\srs, \gone{c}, z, s, \gone{o}),\\
          & \verify(\srs, \gone{c}, z, \tilde{s}, \gone{\tilde{o}}) \\
          &o \neq \tilde{o})
		\end{aligned}}
      \begin{aligned}
        & \srs \gets \kgen(\secparam), \\
        & (\gone{c}, z, s, \tilde{s}, \gone{o}, \gone{\tilde{o}}) \gets \adv(\srs)
      \end{aligned}
    \right.\right]
  % \leq \negl.
\]
is at most negligible.

As noted in \cite[Lemma 2.2]{EPRINT:GabWilCio19} it is enough to upper bound the
probability of the adversary succeeding using the idealised verification
equation---which considers equality between polynomials---instead of the real
verification equation---which considers equality of the polynomials' evaluations.

For a polynomial $f$, its degree upper bound $\maxconst$, evaluation point $z$,
evaluation result $s$, and opening $\gone{o(X)}$ the idealised check verifies that
\begin{equation}
  \alpha (X^{\dconst - \maxconst}f(X) \cdot X^{-\dconst + \maxconst} -  s) \equiv \alpha \cdot o(X) (X - z)\,,
\end{equation}
what is equivalent to 
\begin{equation}
	f(X) -  s \equiv o(X) (X - z)\,.
	\label{eq:pcoms_idealised_check}
\end{equation}
Since $o(X)(X - z) \in \FF_p[X]$ then from the uniqueness of polynomial
composition, there is only one $o(X)$ that fulfils the equation above.
\qed
\end{proof}


\subsection{Unique response property}
The unique response property of $\sonicprot$ follows from the unique opening
property of the used polynomial commitment scheme $\PCOMs$.
\begin{lemma}
\label{lem:sonicprot_ur}
If a polynomial commitment scheme $\PCOMs$ is evaluation binding with
parameter $\epsbind$ and has unique openings property with parameter
$\epsop$, then $\sonicprot$ is $\ur{2}$ with parameter $\epsur \leq
\epsbind + \epsop$.  
\end{lemma}
\begin{proof}
Let $\adv$ be an adversary that breaks $\ur{2}$-ness of $\sonicprot$. 
We consider two cases, depending on which round $\adv$ is able to provide at
least two different outputs such that the resulting transcripts are
acceptable.  For the first case we show that $\adv$ breaks the evaluation
binding property of $\PCOMs$, while for the
second case we show that it breaks the unique opening property of $\PCOMs$.

The proof goes similarly to the proof of \cref{lem:plonkprot_ur} thus we
provide only draft of it here. 
In each Round $i$, for $i > 1$, the prover either commits to some
well-defined polynomials (deterministically), evaluates these on 
randomly picked points, or shows that the evaluations were performed
correctly. 
Naturally for a committed polynomial $\p{p}$ evaluated at point $x$ only one
value $y = \p{p}(x)$ is correct. If the adversary was able to provide two
different values $y$ and $\tilde{y}$ that would be accepted as an evaluation
of $\p{p}$ at $x$ then the $\PCOMs$'s evaluation binding would be broken.
Alternatively, if $\adv$ was able to provide two openings $\p{W}$ and
$\p{\tilde{W}}$ for $y = \p{p}(x)$ then the unique opening property would be
broken.
%
Hence the probability that $\adv$ breaks $\ur{2}$-property of $\PCOMs$ is
upper-bounded by $\epsbind + \epsop$. 
\qed

\end{proof}

\subsection{Computational special soundness}
\begin{lemma}
	\label{lem:sonicprot_ss}
	Let $\adv$ be a $\ppt$ algebraic adversary. The probability $\epsss$ that
  $\adv$ breaks computational special soundness of $\sonicprot$ is upper-bounded
  as
	\[
			\epsss \leq \epss + \epsldlog\,,
	\]
	where $\epss$ is a soundness error of the protocol, and $\epsldlog$ is a
  probability that a $\ppt$ algorithm can break the $(\dconst,
  \dconst)$-$\ldlog$ assumption.
\end{lemma}
\begin{proof}
  The proof goes similarly to the proof of \cref{lem:plonkprot_ss}.
%
  Let $\adv$ be an adversary that produces a
  $(1, 1, \multconstr + \linconstr + 1, 1, 1)$-tree of acceptable transcripts
  $\tree$ for a statement $\inp$. We consider two disjunctive events $\event{E}$
  and $\nevent{E}$. The first corresponds to a case when all transcripts in
  $\tree$ are acceptable for the ideal verifier,
  i.e.~$\vec{\vereq}(X) = \vec{0}$. In that case we show an extractor $\extss$
  that from $\tree$ extracts a valid witness $\wit$. The second, corresponds to
  a case when $\tree$ contains a transcript that is acceptable by the real
  verifier but is not acceptable by the ideal verifier.  In that case we show a
  reduction $\rdvldlog$ that uses $\adv$ to break the
  $(\dconst, \dconst)$-$\ldlog$ assumption.

  \ncase{When $\event{E}$ happens} Since $\sonicprot$ is perfectly sound
  regarding the ideal verifier, for an acceptable proof $\zkproof$ for a
  statement $\inp$ there exists a witness $\wit$ such that $\REL(\inp, \wit)$
  holds and the polynomial $\p{r}(X, y)$ contains witness at its coefficients.
  Note that the witness-carrying polynomial $\p{r}(X, y)$ has degree at most
  $\multconstr + \linconstr$ and since $\adv$ answered correctly on
  $(\multconstr + \linconstr + 1)$ different challenges $z$ (for the sake of
  concreteness let us call them $z_1, \ldots, z_{\multconstr + \linconstr + 1}$)
  then $(\multconstr + \linconstr + 1)$ evaluations
  $\p{r}(z_1, y), \ldots, \p{r}(z_{\multconstr + \linconstr + 1}, y)$ of
  $\p{r}(X, y)$ are known. The extractor $\extss$ interpolates $\p{r}(X, y)$ and
  reveals the corresponding witness $\wit$.

  \ncase{When $\nevent{E}$ happens} Consider a transcript such that for some
  verification equation $\vereq_i(X) \neq 0$, but $\vereq_i(\chi) = 0$. Since
  the adversary is algebraic, all group elements included in the tree of
  transcripts are extended by their representation as a combination of the input
  $\GRP_1$ or $\GRP_2$-elements. Hence all coefficients of the verification
  equation polynomial $\vereq_i(X)$ are known and $\rdvdlog$ can find its zero
  points. Since $\vereq_i(\chi) = 0$, the targeted discrete log value $\chi$ is
  among them. \qed
\end{proof}

\subsection{Honest verifier zero-knowledge}
\begin{lemma}
\label{lem:sonic_hvzk}
$\sonic$ is honest verifier
zero-knowledge.	
\end{lemma}
% Here we show that $\sonic$ is trapdoor-less zero-knowledge.
\begin{proof}
The simulator proceeds as follows.
In the first round, it picks randomly vectors $\vec{a}$, $\vec{b}$ and sets
\begin{equation}
	\label{eq:ab_eq_c}
	\vec{c} = \vec{a} \cdot \vec{b}. 
\end{equation}
Then it pick randomisers $c_{\multconstr + 1}, \ldots, c_{\multconstr + 4}$,
honestly computes polynomials $\p{r}(X, Y), \p{r'}(X, Y), \p{s}(X, Y)$ and
$\p{t}(X, Y)$ and concludes the first round as an honest prover would. Because
of the randomisers the polynomial $\pr(X, Y)$ computed by the simulator is
indistinguishable from a polynomial provided by an honest user for an adversary
that only learns $\pR,\pT,\pa,\pb$ from the proof (the other proof elements are
fixed by these elements and the public information,
see~\cref{sec:sonic_constraint_system}).

Next, $\simulator$ computes the first verifier's challenge $y$ such that
$\pt(X, y)$ is a polynomial that has $0$ as a coefficient next to $X^0$.
I.e.~$\pt(0, y) = 0$. By the definition of $\pt(X)$, the coefficient next to
$X^0$ in $\p{t}(X, Y)$ equals
\begin{equation}
	\label{eq:x0}
	\pt(0,Y) = 
	\vec{a} \cdot \vec{\p{u}}(Y) + 
	\vec{b} \cdot \vec{\p{v}}(Y) + 
	\vec{c} \cdot \vec{\p{w}}(Y) +	
	\sum_{i = 1}^{\multconstr} a_i b_i (Y^i + Y^{-i}) - \p{k}(Y), 
\end{equation}
for public $\vec{\p{u}}(Y), \vec{\p{v}}(Y), \vec{\p{w}}(Y), \p{k}(Y)$ as defined
in \cref{sec:sonic_constraint_system} (Vectors $\vec{u_q}, \vec{v_q}, \vec{w_q}$
are $\multconstr$-elements long and correspond to $Q$ linear constrains of
the proof system. Field element $k_{q}$ is the instance value). When the proven
instance is correct, $\pt(0,Y)$ is a zero polynomial. See \cite{CCS:MBKM19} for
details. Also, when \cref{eq:ab_eq_c} holds, that polynomial simplifies to
\begin{equation}
	\label{eq:x0_simpl}
	t(0,Y) = \vec{a} \cdot \vec{\p{u}}(Y) + 
	\vec{b} \cdot \vec{\p{v}}(Y) + 
	\vec{c} \cdot \vec{\p{\tilde{w}}}(Y)	
	- \p{k}(Y), 
\end{equation}
where $\vec{\p{\tilde{w}}}(Y)$ is defined as
\[
	\p{\tilde{w}_i}(Y) = \sum_{q = 1}^\linconstr Y^{q +
	\multconstr} w_{q, i}\,.
\]
Note that $\pt(0, Y)$ is a ``classical'', i.e.~non-Laurent, polynomial. Also,
it is a polynomial of degree $\linconstr + \multconstr$ with $0$
coefficients for $Y^{i}$, for $i \in \range{0}{\multconstr}$. Also, since
$\vec{a}, \vec{b}$ were picked at random, $\pt(0,Y) / Y^{\multconstr + 1}$ is a
degree-$(\linconstr - 1)$ polynomial of random coefficients. Recall, that the
view of the adversary is independent of $\vec{a}, \vec{b}$ because of
randomisers $c_{\multconstr + 1}, \ldots, c_{\multconstr + 4}$.

The probability that a random degree-$(\linconstr - 1)$ polynomial over
$\FF_p[Y]$ has a root is at least $\infrac{1}{(\linconstr - 1) !}$, see
\cref{lem:root_prob} for a proof of that bound. Since we assume that $\linconstr
= \poly$, we can say that the polynomial $\pt(0,Y)$ as computed by the simulator
has roots with non-negligible probability. Furthermore, these roots can be found
and the simulator picks fresh $\vec{a}, \vec{b}$ until $\pt(0,Y)$ has a root. As
the roots of a random polynomial are themselves random $\FF_p$ elements, the
challenge $y$ picked by the simulator comes from the same distribution as if it
was picked by an honest verifier.

The simulator continues building the transcript by honestly computing the
prover's messages and by picking verifier's challenges at random. This and the
fact that $\pt(0,y) = 0$ guarantees that the transcript provided by the simulator is
acceptable and comes from the same distribution as a transcript of an honest
prover and verifier. \qed
\end{proof}

\begin{remark} 
  As noted in \cite{CCS:MBKM19}, $\sonic$ is statistically subversion-zero
  knowledge (Sub-ZK). As noted in \cite{AC:ABLZ17}, one way to achieve
  subversion zero knowledge is to utilise an extractor that extracts a SRS
  trapdoor from a SRS-generator. Unfortunately, a NIZK made subversion
  zero-knowledge by this approach cannot achieve perfect Sub-ZK as one has to
  count in the probability of extraction failure. However, with the simulation
  presented in \cref{lem:sonic_hvzk}, the trapdoor is not required for the
  simulator as it is able to simulate the execution of the protocol just by
  picking appropriate (honest) verifier's challenges. This result transfers to
  $\sonicprotfs$, where the simulator can program the random oracle to provide
  challenges that fits it.
\end{remark}

\subsection{From special-soundness and unique response property to simulation extractability of $\sonicprotfs$}
Since \cref{lem:sonicprot_ur,lem:sonicprot_ss} hold, $\sonicprot$ is $\ur{2}$
and computationally special sound. We now make use
of \cref{thm:se} and show that $\sonicprotfs$ is simulation-extractable as defined in \cref{def:simext}.

\begin{corollary}[Simulation extractability of $\sonicprotfs$]
  \label{thm:sonicprotfs_se}
  Assume that $\sonicprot$ is $\ur{2}$ with security $\epsur(\secpar)$, and
  computational special-sound with security $\epsss(\secpar)$. Let $\ro\colon
  \bin^* \to \bin^\secpar$ be a random oracle. Let $\advse$ be a $\ppt$
  adversary that can make up to $q$ random oracle queries and outputs an
  acceptable proof for $\sonicprotfs$ with probability at least $\accProb$. Then
  $\sonicprotfs$ is simulation-extractable with extraction error $\eta =
  \epsur$. The extraction probability $\extProb$ is at least
\[
		\extProb  \geq \frac{1}{q^{\multconstr + \linconstr}} (\accProb - \epsur)^{\multconstr +
		\linconstr + 1} - \eps.
	\]
	for some negligible $\eps$, $\multconstr$ and $\linconstr$ being,
  respectively, the number of multiplicative and linear constrains of the system.
\end{corollary}


% \markulf{03.11.2020}{Comment again at this being huge? Maybe mention
% simulation-soundness result?}

%\begin{proof}
% 	The theorem holds as a corollary to \cref{thm:se}. The proof goes by game
%  hoping. The games are controlled by an environment $\env$ that internally
%  runs a simulation extractability adversary $\advse$, simulates its with
%  access to a random oracle and simulator, and when necessary rewinds it. The
%  games differ by various breaking points, i.e.~points where the environment
%  decides to abort the game.
% 
% Denote by $\zkproof_{\advse}, \zkproof_{\simulator}$ proofs
% returned by the adversary and the simulator respectively. We use $\zkproof[i]$
% to denote prover's message in the $i$-th round of the proof, $\zkproof[i].\ch$
% to denote the challenge that is given to the prover after $\zkproof[i]$, and
% $\zkproof\range{i}{i'}$ to denote all messages of the proof exchanged between rounds $i$ and $i'$, i.e.~$\zkproof[i], \zkproof[i].\ch, \ldots, \zkproof[i']$.
% 
% Without loss of generality, we assume that whenever the accepting proof
% contains a response to a challenge from a random oracle, we assume that the
% adversary queried the oracle to get it. It is straightforward to transform any
% adversary that violates this condition into an adversary that makes these
% additional queries to the random oracle and wins with the same probability.
% 
% \ngame{0}
% This is a simulation extraction game played between an adversary $\advse$ who
% has given access to a random oracle $\ro$ and simulator
% $\plonkprotfs.\simulator$.  There is also an extractor $\ext$ that, from the
% proof $\zkproof_\advse$ for instance $\inp_\advse$ output by the adversary and
% from a transcripts of $\advse$'s operations, is tasked to extract a witness
% $\wit_\advse$ such that $\REL(\inp_\advse, \wit_\advse)$ holds. $\advse$ wins
% if it manages to produce an acceptable proof and the extractor fails to reveal
% the corresponding witness. In the following game hops we upper-bound
% probability of this happening.
% 
% \ngame{1}
% This is identical to $\game{0}$ except that now the game is aborted if there
% is a simulated proof $\zkproof_\simulator$ such that
% $\zkproof_\simulator\range{1}{3} = \zkproof_\advse\range{1}{3}$. That is, the
% adversary in its final proof reuses a part of a simulated proof it saw before
% and the proof is acceptable. Denote that event by $\event{\errur}$.
% 
% \ncase{$\game{0} \mapsto \game{1}$}	
% We have, 
% \[
% 	\prob{\game{0} \land \nevent{\errur}} = \prob{\game{1} \land \nevent{\errur}}
% \]
% and, from \cref{lem:difference_lemma}
% \[
% 	\abs{\prob{\game{0}} - \prob{\game{1}}} \leq \event{\errur}\,.
% \]
% Thus, to show that the transition from one game to another introduces only
% minor change in probability of $\advse$ winning it should be shown that
% $\prob{\event{\errur}}$ is small.
% 
% Assume that $\advse$ queried the simulator on $\inp_{\advse}$---the instance
% which $\advse$ outputs. We show a reduction $\rdvur$ that utilises $\advse$,
% who outputs a valid proof for $\inp_\advse$, to break the $\ur{3}$ property of
% $\plonkprot$.
% 
% Consider an algorithm $\rdvur$ that runs $\advse$ internally as a black-box:
% \begin{itemize} \item The reduction answers both queries to the simulator
% $\plonkprotfs.\simulator$ and to the random oracle.  It also keeps lists $Q$,
% for the simulated proofs, and $Q_\ro$ for the random oracle queries.  \item
% When $\advse$ outputs a fake proof $\zkproof_{\advse}$ for  $\inp_\advse$,
% $\rdvur$ looks through lists $Q$ and $Q_\ro$ until it finds
% $\zkproof_{\simulator}\range{1}{3}$ such that $\zkproof_{\advse}\range{1}{3} =
% \zkproof_{\simulator}\range{1}{3}$ and a random oracle query
% $\zkproof_{\simulator}[3].\ch$ on $\zkproof_{\simulator}\range{1}{3}$. \item
% $\rdvur$ returns two proofs for $\inp_\advse$: \begin{align*} \zkproof_1 =
% (\zkproof_{\simulator}\range{1}{3}, \zkproof_{\simulator}[3].\ch,
% \zkproof_{\simulator}\range{4}{5})\\ \zkproof_2 =
% (\zkproof_{\simulator}\range{1}{3}, \zkproof_{\simulator}[3].\ch,
% \zkproof_{\advse}\range{4}{5}) \end{align*} \end{itemize}   If $\zkproof_1 =
% \zkproof_2$, then $\advse$ fails to break simulation extractability, as
% $\zkproof_2 \in Q$. On the other hand, if the proofs are not equal, then
% $\rdvur$ breaks $\ur{3}$-ness of $\plonkprot$, what may happen with some
% negligible probability $\epsur$ only, hence 
% \[ 
% \prob{\event{\errur}} \leq \epsur\,. 
% \]
% 
% \ngame{2} This is identical to $\game{1}$ except that now the environment
% aborts also when it fails to build a $(1, 1, 1, \noofc + 3,
% 1)$-tree of accepting transcripts $\tree$ by rewinding $\advse$. Denote that
% event by $\event{\errfrk}$. Note that for every acceptable proof
% $\zkproof_{\advse}$, we may assume that whenever $\advse$ outputs in Round
% $i$, for $i \in \smallset{4, 5}$, a message $\zkproof_{\advse}[i]$, then
% $\zkproof_{\advse}\range{1}{i}$ is a query to the random oracle that was made
% by the adversary, not the simulator\footnote{\cite{INDOCRYPT:FKMV12} calls
% these queries \emph{fresh}.}. That is, assume that is not true and for some
% query $\zkproof_{\advse}\range{1}{i}$ holds $\zkproof_{\advse}\range{1}{i} =
% \zkproof_\simulator\range{1}{i'}$, for $i, i' \in \smallset{4, 5}$, the proof
% is acceptable and not in $Q$, then the unique response property would be
% broken.
% 
% \ncase{$\game{1} \mapsto \game{2}$}	
% As previously, 
% \[
% 	\abs{\prob{\game{1}} - \prob{\game{2}}} \leq \event{\errfrk}\,.
% \]
% Denote by $\accProb$ the probability that $\advse$ outputs a proof which is
% acceptable and does not break $\ur{3}$-ness of $\plonkprot$. From the
% generalised forking lemma, cf.~\cref{lem:generalised_forking_lemma},
% \[
%   \prob{\event{\errfrk}} \leq 1 - \left(\frac{\accProb^{\noofc +
%     3}}{q^{\noofc + 2}} - \frac{\accProb \cdot
%     (\noofc + 2)}{2^\secpar}\right)\,.
% \]
% 
% \ngame{3} This is identical to $\game{2}$ except that now the environment uses
% the tree $\tree$ to extract the witness for the proven statement and aborts
% when it fails. Denote that event by $\event{\errss}$.
% 
% \ncase{$\game{2} \mapsto \game{3}$}	
% As previously, 
% \[
% 	\abs{\prob{\game{2}} - \prob{\game{3}}} \leq \event{\errss}\,.
% \]
% Since $\plonkprot$ is special-sound the probability that $\env$ fails in
% extracting the witness is upper-bounded by some negligible $\eps_\ss$.
% 
% In the last game, Game $\game{3}$, the environment aborts when it fails to
% extract the correct witness, hence the adversary $\advse$ cannot win. Thus, by
% the game-hoping argument,
% \[
%   \abs{\prob{\game{0}} - \prob{\game{3}}} \leq 1 -
% 	 \left(\frac{\accProb^{\noofc + 3}}{q^{\noofc +
%     2}} - \frac{\accProb \cdot (\noofc + 2)}{2^\secpar}\right) +
% 	 \epsur + \epsss\,.
% \]
% Thus the probability that extractor $\extss$ succeeds is at least
% \[
%   \left(\frac{\accProb^{\noofc + 3}}{q^{\noofc + 2}}
%     - \frac{\accProb \cdot (\noofc + 2)}{2^\secpar}\right) -
%   \epsur - \epsss\,.
% \]
% Since $\accProb$ is probability of $\advse$ outputting acceptable transcript
% that does not break $\ur{3}$-ness of $\plonkprot$, then $\accProb \leq
% \accProb + \epsur$, where $\accProb$ is the probability of $\advse$ outputing
% an acceptable proof as defined in \cref{def:simext}. It thus holds
% \[
% 	\label{eq:frk}
% 	\extProb \geq \frac{(\accProb - \epsur)^{\noofc +
%  3}}{q^{\noofc + 2}} - \underbrace{\frac{(\accProb - \epsur)
%  \cdot (\noofc + 2)}{2^\secpar} - \epsur - \epsss}_{\eps}\,.
% \]
% Note that the part of \cref{eq:frk} denoted by $\eps$ is negligible and 
% \[
%   \extProb \geq \frac{1}{q^{\noofc + 2}} (\accProb -
% 	 \epsur)^{\noofc + 3} -\eps\,.
% \] 
% thus
% $\plonkprot_\fs$ is simulation extractable with extraction error $\epsur$.
% 	\qed
% \end{proof}


\section{Further work}
We identify a number of problems which we left as further work. First of all,
the generalised version of the forking lemma presented in this paper can be
generalised even further to include protocols where $(n_1, \ldots,
n_\mu)$-special soundness holds for more than one $n_j > 1$. I.e.~to include
protocols that for witness extraction require transcripts that branch at more
than one point.

Although we picked $\plonk$ and $\sonic$ as examples for our framework, it is
not limited to SRS-based NIZKs. Thus, it would be interesting to apply it to
known so-called transparent zkSNARKs like Bulletproofs \cite{SP:BBBPWM18},
Aurora \cite{EC:BCRSVW19} or AuroraLight \cite{EPRINT:Gabizon19a}.

Since the rewinding technique and the forking lemma used to show simulation
extractability of $\plonkprotfs$ and $\sonicprotfs$ come with security loss,
it would be interesting to show SE of these protocols directly in the
algebraic group model.

Although we focused here only on zkSNARKs, it is worth to
investigating other protocols that may benefit from our framework, like
e.g.~identification schemes.

Last, but not least, this paper would benefit greatly if a more tight version
of the generalised forking lemma was provided. However, we have to note here
that some of the inequalities used in the proof are already tight, i.e.~for
specific adversaries, some of the inequalities are already equalities.
\bibliographystyle{abbrv}
\bibliography{cryptobib/abbrev1,cryptobib/crypto,additional_bib}
\clearpage
\appendix
%{\Huge{Supplementary Material}} 

\section{Simulation-extractability of sigma protocols and forking lemma}
\label{sec:forking_lemma}
\begin{theorem}[Simulation extractability of the Fiat--Shamir transform
  \cite{INDOCRYPT:FKMV12}]
	Let $\sigmaprot = (\prover, \verifier, \simulator)$ be a non-trivial sigma
  protocol with unique responses for a language $\LANG \in \npol$. In the random
  oracle model, the NIZK proof system $\sigmaprot_\fs = (\prover_\fs,
  \verifier_\fs, \simulator_{\fs})$ resulting by applying the Fiat--Shamir
  transform to $\sigmaprot$ is simulation extractable with extraction error
  $\eta = q/h$ for the simulator $\simulator$. Here, $q$ is the number of random
  oracle queries and $h$ is the number of elements in the range of $\ro$.
\end{theorem}

The theorem relies on the following \emph{general forking lemma} \cite{JC:PoiSte00}.

\begin{lemma}[General forking lemma, cf.~\cite{INDOCRYPT:FKMV12,CCS:BelNev06}]
	\label{lem:forking_lemma}
	Fix $q \in \ZZ$ and a set $H$ of size $h > 2$. Let $\zdv$ be a $\ppt$
  algorithm that on input $y, h_1, \ldots, h_q$ returns $(i, s)$, where $i
  \in\range{0}{q}$ and $s$ is called a \emph{side output}. Denote by $\ig$ a
  randomised instance generator. We denote by $\accProb$ the probability
	\[
		\condprob{i > 0}{y \gets \ig; h_1, \ldots, h_q \sample H; (i, s) \gets
		\zdv(y, h_1, \ldots, h_q)}\,.
	\]
	Let $\forking_\zdv(y)$ denote the algorithm described in
  \cref{fig:forking_lemma}, then the probability $\frkProb$ defined as $
  \frkProb := \condprob{b = 1}{y \gets \ig; (b, s, s') \gets \forking_{\zdv}(y)}
  $ holds
	\[
		\frkProb \geq \accProb \brak{\frac{\accProb}{q} - \frac{1}{h}}\,.
	\]
	%
	\begin{figure}[t]
		\centering
		\fbox{
		\procedure{$\forking_\zdv (y)$}
		{
			\rho \sample \RND{\zdv}\\
			h_1, \ldots, h_q \sample H\\
			(i, s) \gets \zdv(y, h_1, \ldots, h_q; \rho)\\
			\pcif i = 0\ \pcreturn (0, \bot, \bot)\\
			h'_{i}, \ldots, h'_{q} \sample H\\
			(i', s') \gets \zdv(y, h_1, \ldots, h_{i - 1}, h'_{i}, \ldots,  h'_{q};
			\rho)\\
			\pcif (i = i') \land (h_{i} \neq h'_{i})\ \pcreturn (1, s, s')\\
			\pcind \pcelse \pcreturn (0, \bot, \bot)
		}}
		\caption{Forking algorithm $\forking_\zdv$}
		\label{fig:forking_lemma}
\end{figure}
\end{lemma}



\section{Omitted lemmas and proofs}
\begin{lemma}\label{lem:jensen}
	Let $\RND{\zdv}$ denote the set from which $\zdv$ picks its coins at random.
	For each $\iota \in \range{1}{q}$ let $X_\iota \colon \RND{\zdv} \times
	H^{\iota - 1} \to [0, 1]$ be defined by setting $X_\iota(\rho, h_1, \ldots,
h_{\iota - 1})$ to 
\[
  \condprob{i = \iota}{h_\iota, \ldots, h_q \sample H; (i, s) \gets \zdv(y, h_1,
    \ldots, h_q; \rho)}
	\] 
	for all $\rho \in \RND{\zdv}$ and $h_1, \ldots, h_{\iota - 1} \in H$. Consider
  $X_\iota$ as a random variable over the uniform distribution on its domain.
  Then $\expected{X_\iota^m} \geq \expected{X_\iota}^m$.
\end{lemma}
\begin{proof}
	First we recall the Jensen inequality \cite{W:Weissten20}, if for some random
  variable $X$ holds $\abs{\expected{X}} \leq \infty$ and $f$ is a Borel convex
  function then
	\[
		f(\expected{X}) \leq \expected{f(X)}\,.
	\] 
	Finally, we note that $\abs{\expected{X}} \leq \infty$ and taking to the
  $m$-th power is a Borel convex function on $[0, 1]$ interval. \qed
\end{proof}

\begin{lemma}[H\"older's inequality. Simplified.]\label{lem:holder}
	Let $x_i, y_i$, for $i \in \range{1}{q}$, and $p, q$ be real numbers such that
  $1/p + 1/q = 1$. Then
	\[
		\sum_{i = 1}^{q} x_i y_i \leq \left(\sum_{i = 1}^{q}
      x_i^p\right)^{\frac{1}{p}} \cdot \left(\sum_{i = 1}^{q}
      y_i^p\right)^{\frac{1}{q}}\,.
	\]
\end{lemma}

\begin{remark}[Tightness of the H\"older inequality]
	In is important to note that Inequality (\ref{eq:tightness}) is tight. More
  precisely, for $\expected{X_i} = x$, $i \in \range{1}{q}$ we have
	\begin{gather*}
		\sum_{i = 1}^q x = \left(\sum_{i = 1}^{q} x^m\right)^\frac{1}{m} \cdot \left(\sum_{i = 1}^{q} 1^{\frac{m}{m - 1}}\right)^{\frac{m - 1}{m}} \\
		qx = \left(qx^m\right)^\frac{1}{m} \cdot q^{\frac{m - 1}{m}} \\
		(qx)^m = qx^m \cdot q^{m - 1} \\
		(qx)^m = (qx)^m\,.
	\end{gather*}
\end{remark}

\begin{lemma}
  \label{lem:root_prob}
  Let $\p{f}(X)$ be a random degree-$d$ polynomial over $\FF_p[X]$. Then the
  probability that $\p{f}(X)$ has roots in $\FF_p$ is at least $\infrac{1}{d!}$.
\end{lemma}
\begin{proof}
  First observe that there is $p^{d}$ canonical polynomials in $\FF_p[X]$.  Each
  of the polynomials may have up to $d$ roots. Consider polynomials which are
  reducible to polynomials of degree $1$, i.e.~polynomials that have all $d$
  roots. The roots can be picked in $\bar{C}^{p}_{d}$ ways, where
  $\bar{C}^{n}_{k}$ is the number of $k$-elements combinations with repetitions
  from $n$-element set. That is,
  \[
    \bar{C}^n_k = \binom{n + k - 1}{k}\,.
  \]
  Thus, the probability that a randomly picked polynomial has all $d$ roots is
  \begin{multline*}
    p^{-d} \cdot \bar{C}^p_d = p^{-d} \cdot \binom{p + d - 1}{d} =
    p^{-d} \cdot \frac{(p + d - 1)!}{(p + d - 1 - d)! \cdot d!} = \\
    p^{-d} \cdot \frac{(p + d - 1) \cdot \ldots \cdot p \cdot (p - 1)!}{(p - 1)!
      \cdot d!} = p^{-d} \cdot \frac{(p + d - 1)\cdot
      \ldots \cdot p}{d!} \\
    \geq p^{-d} \cdot {\frac{p^d}{d!}} = \frac{1}{d!}
  \end{multline*}
  \qed
\end{proof}

\section{$\plonk$ protocol rolled out}
\subsection{Algorithms rolled out}
\label{sec:plonk_explained}
\ncase{$\plonk$ SRS generating algorithm $\kgen(\REL)$}
For the sake of simplicity of security reductions presented in this paper, we
describe here a simplified SRS generating algorithm which produces only
these SRS elements that cannot be computed without knowing the secret trapdoor
$\chi$. The rest of the preprocessed input, as it is called in
\cite{EPRINT:GabWilCio19}, can be computed using these SRS elements thus we leave
them to be computed by the prover, verifier, and simulator. 

The SRS generating algorithm picks at random $\chi \sample \FF_p$, computes
and outputs
\[
	\srs = \left(\gone{\smallset{\chi^i}_{i = 0}^{\noofc + 2}},
	\gtwo{\chi} \right).
\]

\ncase{$\plonk$ prover $\prover(\REL, \srs, \inp, \wit = (\wit_i)_{i \in \range{1}{3 \cdot \noofc}})$}
\begin{description}
	\item[Round 1] 
	Sample $b_1, \ldots, b_9 \sample \FF_p$; compute $\p{a}(X), \p{b}(X), \p{c}(X)$ as 
	\begin{align*}
		\p{a}(X) &= (b_1 X + b_2)\p{Z_H}(X) + \sum_{i = 1}^{\noofc} \wit_i \lag_i(X) \\
		\p{b}(X) &= (b_3 X + b_4)\p{Z_H}(X) + \sum_{i = 1}^{\noofc} \wit_{\noofc + i} \lag_i(X) \\
		\p{c}(X) &= (b_5 X + b_6)\p{Z_H}(X) + \sum_{i = 1}^{\noofc} \wit_{2 \cdot \noofc + i} \lag_i(X) 
	\end{align*}
	Output $\gone{\p{a}(\chi), \p{b}(\chi), \p{c}(\chi)}$.
	
	\item[Round 2]
	Get challenges $\beta, \gamma \in \FF_p$
	\[
		\beta = \ro(\zkproof[0..1], 0)\,, \qquad \gamma = \ro(\zkproof[0..1], 1)\,.
	\]
	Compute permutation polynomial $\p{z}(X)$
	\begin{multline*}
		\p{z}(X) = (b_7 X^2 + b_8 X + b_9)\p{Z_H}(X) + \lag_1(X) + \\
			+ \sum_{i = 1}^{\noofc - 1} 
			\left(\lag_{i + 1} (X) \prod_{j = 1}^{i} 
			\frac{
			(\wit_j +\beta \omega^{j - 1} + \gamma)(\wit_{\noofc + j} + \beta k_1 \omega^{j - 1} + \gamma)(\wit_{2 \noofc + j} +\beta k_2 \omega^{j- 1} + \gamma)}
			{(\wit_j+\sigma(j) \beta + \gamma)(\wit_{\noofc + j} + \sigma(\noofc + j)\beta + \gamma)(\wit_{2 \noofc + j} + \sigma(2 \noofc + j)\beta + \gamma)}\right)
	\end{multline*}
	Output $\gone{\p{z}(\chi)}$
		
	\item[Round 3]
	Get the challenge $\alpha = \ro(\zkproof[0..2])$, compute the quotient polynomial 
	\begin{align*}
	& \p{t}(X)  = \\
	& (\p{a}(X) \p{b}(X) \selmulti(X) + \p{a}(X) \selleft(X) + 
	\p{b}(X)\selright(X) + \p{c}(X)\seloutput(X) + \pubinppoly(X) + \selconst(X)) 
	\frac{1}{\p{Z_H}(X)} +\\
	& + ((\p{a}(X) + \beta X + \gamma) (\p{b}(X) + \beta k_1 X + \gamma)(\p{c}(X) 
	+ \beta k_2 X + \gamma)\p{z}(X)) \frac{\alpha}{\p{Z_H}(X)} \\
	& - (\p{a}(X) + \beta \p{S_{\sigma 1}}(X) + \gamma)(\p{b}(X) + \beta 
	\p{S_{\sigma 2}}(X) + \gamma)(\p{c}(X) + \beta \p{S_{\sigma 3}}(X) + 
	\gamma)\p{z}(X \omega))  \frac{\alpha}{\p{Z_H}(X)} \\
	& + (\p{z}(X) - 1) \lag_1(X) \frac{\alpha^2}{\p{Z_H}(X)}
	\end{align*}
	Split $\p{t}(X)$ into degree less then $\noofc$ polynomials $\p{t_{lo}}(X), \p{t_{mid}}(X), \p{t_{hi}}(X)$, such that
	\[
		\p{t}(X) = \p{t_{lo}}(X) + X^{\noofc} \p{t_{mid}}(X) + X^{2 \noofc} \p{t_{hi}}(X)\,.
	\]
	Output $\gone{\p{t_{lo}}(\chi), \p{t_{mid}}(\chi), \p{t_{hi}}(\chi)}$.
	
	\item[Round 4]
	Get the challenge $\chz \in \FF_p$, $\chz = \ro(\zkproof[0..3])$.
	Compute opening evaluations
	\begin{align*}
			\p{a}(\chz), \p{b}(\chz), \p{c}(\chz), \p{S_{\sigma 1}}(\chz), \p{S_{\sigma 2}}(\chz), \p{t}(\chz), \p{z}(\chz \omega),
	\end{align*}
	Compute the linearisation polynomial
	\[
		\p{r}(X) = 
		\begin{aligned}
			& \p{a}(\chz) \p{b}(\chz) \selmulti(X) + \p{a}(\chz) \selleft(X) + \p{b}(\chz) \selright(X) + \p{c}(\chz) \seloutput(X) + \selconst(X) \\
			& + \alpha \cdot \left( (\p{a}(\chz) + \beta \chz + \gamma) (\p{b}(\chz) + \beta k_1 \chz + \gamma)(\p{c}(\chz) + \beta k_2 \chz + \gamma) \cdot \p{z}(X)\right) \\
			& - \alpha \cdot \left( (\p{a}(\chz) + \beta \p{S_{\sigma 1}}(\chz) + \gamma) (\p{b}(\chz) + \beta \p{S_{\sigma 2}}(\chz) + \gamma)\beta \p{z}(\chz\omega) \cdot \p{S_{\sigma 3}}(X)\right) \\
			& + \alpha^2 \cdot \lag_1(\chz) \cdot \p{z}(X)
		\end{aligned}
	\]
	Output $\p{a}(\chz), \p{b}(\chz), \p{c}(\chz), \p{S_{\sigma 1}}(\chz), \p{S_{\sigma 2}}(\chz), \p{t}(\chz), \p{z}(\chz \omega), \p{r}(\chz).$
	
	\item[Round 5]
	Compute the opening challenge $v \in \FF_p$, $v = \ro(\zkproof[0..4])$.
	Compute the openings for the polynomial commitment scheme 
	\begin{align*}
	& \p{W_\chz}(X) = \frac{1}{X - \chz} \left(
	\begin{aligned}
		& \p{t_{lo}}(X) + \chz^\noofc \p{t_{mid}}(X) + \chz^{2 \noofc} \p{t_{hi}}(X) - \p{t}(\chz)\\
		& + v(\p{r}(X) - \p{r}(\chz)) \\
		& + v^2 (\p{a}(X) - \p{a}(\chz))\\
		& + v^3 (\p{b}(X) - \p{b}(\chz))\\
		& + v^4 (\p{c}(X) - \p{c}(\chz))\\
		& + v^5 (\p{S_{\sigma 1}}(X) - \p{S_{\sigma 1}}(\chz))\\
		& + v^6 (\p{S_{\sigma 2}}(X) - \p{S_{\sigma 2}}(\chz))
	\end{aligned}
	\right)\\
	& \p{W_{\chz \omega}}(X) = \frac{\p{z}(X) - \p{z}(\chz \omega)}{X - \chz \omega}
\end{align*}
	Output $\gone{\p{W_{\chz}}(\chi), \p{W_{\chz \omega}}(\chi)}$.
\end{description}

\ncase{$\plonk$ verifier $\verifier(\REL, \srs, \inp, \zkproof)$}\ \newline
The \plonk{} verifier works as follows
\begin{description}
	\item[Step 1] Validate all obtained group elements.
	\item[Step 2] Validate all obtained field elements.
	\item[Step 3] Validate the instance $\inp = \smallset{\wit_i}_{i = 1}^\instsize$.
	\item[Step 4] Compute challenges $\beta, \gamma, \alpha, \alpha', \chz, v, u$ from the transcript.
	\item[Step 5] Compute zero polynomial evaluation $\p{Z_H} (\chz)  =\chz^\noofc - 1$.
	\item[Step 6] Compute Lagrange polynomial evaluation $\lag_1 (\chz) = \frac{\chz^\noofc -1}{\noofc (\chz - 1)}$.
	\item[Step 7] Compute public input polynomial evaluation $\pubinppoly (\chz) = \sum_{i \in \range{1}{\instsize}} \wit_i \lag_i(\chz)$.
	\item[Step 8] Compute quotient polynomials evaluations
	\begin{multline*}
		\p{t} (\chz)  = \frac{1}{\p{Z_H}(\chz)}
		\Big(
			\p{r} (\chz) + \pubinppoly(\chz) - (\p{a}(\chz) + \beta \p{S_\sigma 1}(\chz) + \gamma) (\p{b}(\chz) + \beta \p{S_\sigma 2}(\chz) + \gamma) \\
			(\p{c}(\chz) +
			\gamma)\p{z}(\chz \omega) \alpha - \lag_1 (\chz) \alpha^2
		\Big) \,.
	\end{multline*}
	\item[Step 9] Compute batched polynomial commitment
	$\gone{D} = v \gone{r} + u \gone {z}$ that is
	\begin{align*}
		\gone{D} & = v
		\left(
		\begin{aligned}
			& \p{a}(\chz)\p{b}(\chz) \cdot \gone{\selmulti} + \p{a}(\chz)  \gone{\selleft} + \p{b}  \gone{\selright} + \p{c}  \gone{\seloutput} + \\
			& + (	(\p{a}(\chz) + \beta \chz + \gamma) (\p{b}(\chz) + \beta k_1 \chz + \gamma) (\p{c} + \beta k_2 \chz + \gamma) \alpha  + \lag_1(\chz) \alpha^2)  + \\
			% &   \\
			& - (\p{a}(\chz) + \beta \p{S_{\sigma 1}}(\chz) + \gamma) (\p{b}(\chz) + \beta \p{S_{\sigma 2}}(\chz) + \gamma) \alpha  \beta \p{z}(\chz \omega) \gone{\p{S_{\sigma 3}}(\chi)})
		\end{aligned}
		\right) + \\
		& + u \gone{\p{z}(\chi)}\,.
	\end{align*}
	\item[Step 10] Computes full batched polynomial commitment $\gone{F}$:
	\begin{align*}
		\gone{F} & = \left(\gone{\p{t_{lo}}(\chi)} + \chz^\noofc \gone{\p{t_{mid}}(\chi)} + \chz^{2 \noofc} \gone{\p{t_{hi}}(\chi)}\right) + u \gone{\p{z}(\chi)} + \\
		& + v
		\left(
		\begin{aligned}
			& \p{a}(\chz)\p{b}(\chz) \cdot \gone{\selmulti} + \p{a}(\chz)  \gone{\selleft} + \p{b}(\chz)   \gone{\selright} + \p{c}(\chz)  \gone{\seloutput} + \\
			& + (	(\p{a}(\chz) + \beta \chz + \gamma) (\p{b}(\chz) + \beta k_1 \chz + \gamma) (\p{c}(\chz)  + \beta k_2 \chz + \gamma) \alpha  + \lag_1(\chz) \alpha^2)  + \\
			% &   \\
			& - (\p{a}(\chz) + \beta \p{S_{\sigma 1}}(\chz) + \gamma) (\p{b}(\chz) + \beta \p{S_{\sigma 2}}(\chz) + \gamma) \alpha  \beta \p{z}(\chz \omega) \gone{\p{S_{\sigma 3}}(\chi)})
		\end{aligned}
		\right) \\
		& + v^2 \gone{\p{a}(\chi)} + v^3 \gone{\p{b}(\chi)} + v^4 \gone{\p{c}(\chi)} + v^5 \gone{\p{S_{\sigma 1}(\chi)}} + v^6 \gone{\p{S_{\sigma 2}}(\chi)}\,.
	\end{align*}
	\item[Step 11] Compute group-encoded batch evaluation $\gone{E}$
	\begin{align*}
		\gone{E}  = \frac{1}{\p{Z_H}(\chz)} & \gone{
		\begin{aligned}
			& \p{r}(\chz) + \pubinppoly(\chz) +  \alpha^2  \lag_1 (\chz) + \\
			& - \alpha \left( (\p{a}(\chz) + \beta \p{S_{\sigma 1}} (\chz) + \gamma) (\p{b}(\chz) + \beta \p{S_{\sigma 2}} (\chz) + \gamma) (\p{c}(\chz) + \gamma) \p{z}(\chz \omega) \right)
		\end{aligned}
		}\\
		 + & \gone{v \p{r}(\chz) + v^2 \p{a}(\chz) + v^3 \p{b}(\chz) + v^4 \p{c}(\chz) + v^5 \p{S_{\sigma 1}}(\chz) + v^6 \p{S_{\sigma 2}}(\chz) + u \p{z}(\chz \omega) }\,.
	\end{align*}
	\item[Step 12] Check whether the verification $\vereq(\chi)$ equation holds
	\begin{multline}
		\label{eq:ver_eq}
		\left(
		\gone{\p{W_{\chz}}(\chi)} + u \cdot \gone{\p{W_{\chz \omega}}(\chi)}
		\right) \bullet
		\gtwo{\chi} - \\
		\left(
			\chz \cdot \gone{\p{W_{\chz}}(\chi)} + u \chz \omega \cdot \gone{\p{W_{\chz \omega}}(\chi)} + \gone{F} - \gone{E}
		\right) \bullet
		\gtwo{1} = 0\,.
	\end{multline}
  The verification equation is a batched version of the verification equation
  from \cite{AC:KatZavGol10} which allows the verifier to check openings of
  multiple polynomials in two points (instead of checking an opening of a single
  polynomial at one point).
\end{description}

Since the original paper \cite{EPRINT:GabWilCio19} lacks of explanation how the
simulator of \plonk{} works, it is presented here.

\COMMENT{
\ncase{$\plonk$ simulator $\simulator_\chi(\REL, \srs, \td, \inp)$}
% \paragraph{Simulation in \plonk.}
% The simulator $\simulator$ in $\plonk$ proceeds according to the following steps:
\begin{description}
\item[Round 1] Since the simulator does not know a witness $\wit$ for the proven
  statement $\inp$, $\simulator_\chi$ cannot compute the output of this round
  accordingly to the protocol. Instead, it picks randomly both the randomisers
  $b_1, \ldots, b_6$ and evaluations of polynomials $\p{a}, \p{b}, \p{c}$ by
  picking their coefficients randomly and outputting $\gone{\p{a}(\chi),
    \p{b}(\chi), \p{c}(\chi)}$.
\item[Round 2] The simulator takes permutation argument challenges $\beta,
  \gamma$ as a random oracle output in the ongoing proof. Similarly as in the
  previous round, the simulator cannot evaluate the requested polynomial $\p{z}$
  honestly as it does not know the witness, picks its coefficients randomly and
  outputs $\gone{\p{z}(\chi)}$.
\item[Round 3] In this round the simulator starts by picking at random a
  challenge $\chz$ that will be later used to program a random oracle. Then it
  computes evaluations $\p{a}(\chz), \p{b}(\chz), \p{c}(\chz), \p{S_{\sigma
      1}}(\chz), \p{S_{\sigma 2}}(\chz), \pubinppoly(\chz), \lag_1(\chz),
  \p{Z_H}(\chz),\allowbreak \p{z}(\chz\omega)$
	
	Given the evaluations $\simulator_\chi$ computes polynomial $\p{r}(X)$ honestly, i.e.
	\[
		\p{r}(X) = 
		\begin{aligned}
			& \p{a}(\chz) \p{b}(\chz) \selmulti(X) + \p{a}(\chz) \selleft(X) + \p{b}(\chz) \selright(X) + \p{c}(\chz) \seloutput(X) + \selconst(X) \\
			& + \alpha \cdot \left( (\p{a}(\chz) + \beta \chz + \gamma) (\p{b}(\chz) + \beta k_1 \chz + \gamma)(\p{c}(\chz) + \beta k_2 \chz + \gamma) \cdot \p{z}(X)\right) \\
			& - \alpha \cdot \left( (\p{a}(\chz) + \beta \p{S_{\sigma 1}}(\chz) + \gamma) (\p{b}(\chz) + \beta \p{S_{\sigma 2}}(\chz) + \gamma)\beta \p{z}(\chz\omega) \cdot \p{S_{\sigma 3}}(X)\right) \\
			& + \alpha^2 \cdot \lag_1(\chz) \cdot \p{z}(X)
		\end{aligned}
	\]
	and evaluates $\p{r}(X)$ at $\chz$.
	
	In the next step the simulator computes $\ev{t}$ as the verifier would compute
  in Step 8. Next, $\simulator_\chi$ picks randomly a polynomial $\p{t}$ such
  that $\p{t} (\chz) = \ev{t}$. The simulator concludes this round as an honest
  prover would by dividing $\p{t}$ into $\p{t_{hi}}, \p{t_{mid}}, \p{t_{lo}}$
  and outputting $\gone{\p{t_{hi}}(\chi), \p{t_{mid}}(\chi), \p{t_{lo}}(\chi)}$.
\item[Round 4] The simulator program random oracle to return $\chz$ when queried
  on the current state of the transcript. Since the necessary evaluations at
  $\chz$ are already computed, $\simulator_\chi$ simply outputs
	\[
		\p{a}(\chz), \p{b}(\chz), \p{c}(\chz), \p{S_{\sigma 1}}(\chz), \p{S_{\sigma 2}}(\chz), \p{t}(\chz), \p{z}(\chz \omega)\,.
	\]
	\item[Round 5]
	In this round the simulator proceeds as an honest prover would.
	\end{description}
}
\COMMENT{
	\section{$\sonic$ protocol rolled out}
	\label{sec:sonic_explained}
	\subsection{The constraint system}
	\label{sec:sonic_constraint_system}
	\sonic's system of constraints composes of three $\multconstr$-long vectors
  $\va, \vb, \vc$ which corresponds to left and right inputs to multiplication
  gates and their outputs. It hence holds $\va \cdot \vb = \vc$.

	There is also $\linconstr$ linear constrains of the form 
	\[
		\va \vec{u_q} + \vb \vec{v_q} + \vc \vec{w_q} = k_q,
	\]
	where $\vec{u_q}, \vec{v_q}, \vec{w_q}$ are vectors for the $q$-th linear
	constraint with instance value $k_q \in \FF_p$. Furthermore define polynomials
\begin{equation}
	\begin{split}
		\p{u_i}(Y) & = \sum_{q = 1}^\linconstr Y^{q + \multconstr} u_{q, i}\,,\\
		\p{v_i}(Y) & = \sum_{q = 1}^\linconstr Y^{q + \multconstr} v_{q, i}\,,\\
	\end{split}
	\qquad
	\begin{split}
		\p{w_i}(Y) & = -Y^i - Y^{-i}  + \sum_{q = 1}^\linconstr Y^{q +
		\multconstr} w_{q, i}\,,\\
		\p{k}(Y) & = \sum_{q = 1}^\linconstr Y^{q + \multconstr} k_{q}.
	\end{split}
\end{equation}
In \sonic{} we will use commitments to the following polynomials.
\begin{align*}
	\pr(X, Y) & = \sum_{i = 1}^{\multconstr} \left(a_i X^i Y^i + b_i X^{-i} Y^{-i}
	+ c_i X^{-i - \multconstr} Y^{-i - \multconstr}\right) \\
	\p{s}(X, Y) & = \sum_{i = 1}^{\multconstr} \left( u_i (Y) X^{-i} +
	v_i(Y) X^i + w_i(Y) X^{i + \multconstr}\right)\\
		\pr'(X, Y) & = \pr(X, Y) + \p{s}(X, Y) \\
		\pt(X, Y) & = \pr(X, 1) \pr'(X, Y) - \p{k}(Y)\,.
\end{align*}
	
	\subsection{Algorithms rolled out}
	\ncase{$\sonic$ SRS generation $\kgen(\REL)$}
	The SRS generating algorithm picks randomly $\alpha, \chi \sample \FF_p$ and
	outputs 
	\[
		\srs = \left( \gone{\smallset{\chi^i}_{i = -\dconst}^{\dconst},
			\smallset{\alpha \chi^i}_{i = -\dconst, i \neq 0}^{\dconst}},
			\gtwo{\smallset{\chi^i, \alpha \chi^i}_{i = - \dconst}^{\dconst}},
		\gtar{\alpha} \right)
	\]
	\ncase{$\sonic$ prover $\prover(\REL, \srs, \inp, \wit=\va, \vb,
	\vc)$}
\begin{description}
	\item[Round 1]
		The prover picks randomly randomisers $c_{\multconstr + 1}, c_{\multconstr + 2},
		c_{\multconstr + 3}, c_{\multconstr + 4} \sample \FF_p$. Set $\pr(X, Y)
		\gets \pr(X, Y) + \sum_{i = 1}^4 c_{\multconstr + i} X^{- 2 \multconstr -
		i}$. Commits to $\pr(X, 1)$ and outputs $R \gets \com(\srs, \multconstr,
		\pr(X, 1))$.
		Then it gets challenge $y$ from the verifier.
      \item[Round 2] $\prover$ commits to $\pt(X, y)$ and outputs
        $T \gets \com(\srs, \dconst, \pt(X, y))$. Then it gets a challenge $z$
        from the verifier.
	\item[Round 3] The prover computes commitment openings. That is, it outputs
		\begin{align*}
			W_a & = \open(\srs, z, \pr(z, 1), \pr(X, 1)) \\
			W_b & = \open(\srs, yz, \pr(yz, 1), \pr(X, 1)) \\
			W_t & = \open(\srs, z, \pt(z, y), \pt(X, y)) 
		\end{align*}
		along with evaluations $a = \pr(z, 1), b = \pr(y, z), t = \pt(z, y)$.
		Then it engages in the signature of correct computation playing the role of
		the helper, i.e.~it commits to $\p{s}(X, y)$ and sends the commitment $S$.
		Then it obtains a challenge $u$ from the verifier.
	\item[Round 4] In the next round the prover computes $C \gets \com(\srs,
		\dconst, \p{s}(u, x)) \cdot \gone{\p{s}(u, x)}$ and compute commitments'
		openings 
		\begin{align*}
			W & = \open(\srs, u, \p{s}(X, y)), \\
			Q & = \open(\srs, y, \p{s}(u, Y)),
		\end{align*}
		and returns $W, Q, \p{s}(u, y)$. Eventually the prover gets the last
		challenge from the verifier---$z$.
	\item[Round 5] In the final round, $\prover$ computes opening $Q_z =
		\open(\srs, z, \p{s}(u, X))$ and outputs $Q_z$ and $\p{s}(u, z)$. 
\end{description}

	\ncase{$\sonic$ verifier $\verifier(\REL, \srs, \inp, \zkproof)$}
	The verifier in \sonic{} runs as subroutines the verifier for the polynomial
	commitment. That is it sets $t = a(b + s) - \p{k}(y)$ and checks the following:
	\begin{equation*}
		\begin{split}
		&\PCOMs.\verifier(\srs, \multconstr, R, z, a, W_a), \\
		&\PCOMs.\verifier(\srs, \multconstr, R, yz, b, W_b),\\
		&\PCOMs.\verifier(\srs, \dconst, T, z, t, W_t),\\	
		\end{split}
		\qquad
		\begin{split}
		&\PCOMs.\verifier(\srs, \dconst, S, u, s, W),\\
		&\PCOMs.\verifier(\srs, \dconst, C, y, s, Q),\\
		&\PCOMs.\verifier(\srs, \dconst, C, z, s_z, Q_z),
		\end{split}
      \end{equation*}
      and accepts the proof iff all the checks holds.
 }

	\section{Generalised forking lemma for multiple branching points}
    \subsection{Generalised forking lemma.}
First of all, although dubbed ``general'', \cref{lem:forking_lemma} is not
general enough for our purpose as it is useful only for protocols where witness
can be extracted from just two transcripts. To be able to extract a witness
from, say, an execution of $\plonkprot$ we need to obtain at least
$\noofc + 3$ valid proofs. Here we propose a generalisation of the
general forking lemma that given probability of producing an accepting
transcript $\accProb$ lower-bounds the probability of generating a \emph{tree
  of accepting transcripts} $\tree$, which allows to extract a witness.

\begin{definition}[Tree of accepting transcripts, cf.~{\cite{EC:BCCGP16}}]
	\label{def:tree_of_accepting_transcripts}
	Consider a $(2\mu + 1)$-message interactive proof system $\ps$. An $(n_1,
  \ldots, n_\mu)$-tree of accepting transcript is a tree where each node on
  depth $i$, for $i \in \range{1}{\mu}$, is an $i$-th prover's message in an
  acceptable transcript; edges between the nodes are labeled with verifier's
  challenges, such that no two edges on the same depth have the same
  label; and each node on depth $i$ has $(n_{i} - 1)$ siblings and $n_{i +
    1}$ children. Altogether, the tree consists of $m = \prod_{i = 1}^\mu n_i$
  branches, which makes $m$ acceptable transcripts. We require $m = \poly$.
\end{definition}


\begin{lemma}[General forking lemma II]
	\label{lem:generalised_forking_lemma_gen}
	Fix $q \in \ZZ$ and set $H$ of size $h \geq m m'$. Let $\zdv$ be a $\ppt$
  algorithm that on input $y, h_1, \ldots, h_q$ returns $(\vec{i}, s)$ where
  $\vec{i} = (\vec{i}[1], \ldots, \vec{i}[m']) \in
  \range{0}{q}^{m'}$ and $s$ is called a side output. Denote by $\ig$ a randomised
  instance generator. We denote by $\accProb$ the probability
	\[
      \condprob{\forall k \in \range{1}{m'} : \vec{i}[k] \neq 0}{ y \gets \ig;\
        h_1, \ldots, h_q \sample H;\ (\vec{i}, s) \gets \zdv(y, h_1, \ldots,
        h_q)}\,.
	\]
	Let $\genforking_{\zdv}^{m}$ denote the algorithm described in
  \cref{fig:genforking_lemma} then the probability $\frkProb := \condprob{b =
    1}{y \gets \ig;\ h_1, \ldots, h_{q} \sample H;\ (b, \vec{s}) \gets
    \genforking_{\zdv}^{m}(y, h_1, \ldots, h_q)}$ is at least
	\[
		\frac{\accProb^m}{q^{m'(m - 1)}} - \accProb \cdot \left(1 -
    \frac{h!}{(h - m m')! \cdot h^{m m'}}\right).
	\]
		
	\begin{figure}[t]
      \centering \fbox{
        \procedure{$\genforking_{\zdv}^{m} (y,h_1^{1}, \ldots, h_{q}^{1})$} {
          \rho \sample \RND{\zdv}\\
          (\vec{i}, s_1) \gets \zdv(y, h_1^{1}, \ldots, h_{q}^{1}; \rho)
          \pccomment{$\vec{i} = i[1], \ldots, i[m']$}\\
          \vec{i_1} \gets \vec{i}\\
          \pcif (\exists k \in \range{1}{m'} : \vec{i}[k] = 0)\ \pcreturn (0, \bot)\\
          \pcfor j \in \range{2}{m}\\
          \pcind h_{1}^{j}, \ldots, h_{i[1] - 1}^{j} \gets h_{1}^{j - 1},
          \ldots,
          h_{i[1] - 1}^{j - 1}\\
          \pcind h_{i[1]}^{j}, \ldots, h_{q}^{j} \sample H\\
          \pcind (\vec{i_j}, s_j) \gets \zdv(y, h_1^{j}, \ldots, h_{q}^{j}; \rho)\\
          \pcind \pcif (\exists k \in \range{1}{m'} : \vec{i_j}[k] = 0) \lor \vec{i_j} \neq \vec{i}\ \pcreturn (0, \bot)\\
          \pcif \exists (j, j') \in \range{1}{m}^2, k \in \range{1}{m'} :
          (h_{i[k]}^{j} = h_{i[k]}^{j'})\
          \pcreturn (0, \bot)\\
          \pcelse \pcreturn (1, \vec{s}) }}
	\caption{Generalised forking algorithm $\genforking_{\zdv}^{m}$}
	\label{fig:genforking_lemma_gen}
\end{figure}
\end{lemma}
\begin{proof}
First let denote by $\accProb(y)$ and $\frkProb(y)$ the following probabilities
\begin{align*}
  \accProb(y) & =  \condprob{\forall k \in \range{1}{m'} : \vec{i}[k] \neq 0}{h_1, \ldots, h_q \sample H;\ (\vec{i}, s)
                \gets \zdv(y, h_1, \ldots, h_q)}\,.\\
  \frkProb(y) & = \condprob{b = 1}{(b, \vec{s}) \gets
                \genforking_{\zdv}^{m}(y, h_1, \ldots, h_q)}\,.
\end{align*}

We start by claiming that for all $y$ 
\begin{equation}\label{eq:frkProb_y_gen}
	\frkProb(y) \geq 
	\frac{\accProb(y)^m}{q^{m'(m - 1)}} - \accProb(y) \cdot \left(1 -
  \frac{h!}{(h - m m')! \cdot h^{m m'}}\right)
	\end{equation}
Then with the expectation taken over $y \sample \ig$, we have
\begin{align}
	\frkProb & = \expected{\frkProb(y)} \geq
	\expected{\frac{\accProb(y)^m}{q^{m' (m - 1)}} -  \accProb(y) \cdot \left(1 -
  \frac{h!}{(h - m m')! \cdot h^{m m'}}\right)} \label{eq:use_eq1_gen}\\
	& \geq \frac{\expected{\accProb(y)}^m}{q^{m'(m - 1)}} -
	\expected{\accProb(y)} \cdot \left(1 - \frac{h!}{(h - m m')! \cdot
  h^{m m'}}\right) \label{eq:by_lemma_jensen_gen}\\
	& = \frac{\accProb^m}{q^{m'(m - 1)}} -  \accProb \cdot \left(1 -
  \frac{h!}{(h - m m')! \cdot h^{m m'}}\right)\label{eq:by_accProb_gen}\,.
\end{align}
Where \cref{eq:use_eq1_gen} comes from \cref{eq:frkProb_y_gen};
\cref{eq:by_lemma_jensen_gen} comes from linearity of expected value and \cref{lem:jensen}; and
\cref{eq:by_accProb_gen} holds by the fact that $\expected{\accProb(y)} =
\accProb$.

We now show \cref{eq:frkProb_y}.
Denote by $J = \range{1}{m}^2 \setminus \smallset{(j, j)}_{j \in \range{1}{m}}$. 
For any input $y$, with probabilities taken over the coin tosses of
$\genforking_{\zdv}^{m}$ we have
\begin{align*}
  \frkProb (y) & = \prob{\vec{i_j} = \vec{i_{j'}} \land \vec{i_j}[k] \neq 0 \land
                 h_{\vec{i_j}[l]}^{j} \neq h_{\vec{i_{j'}}[l]}^{j'} \text{ for
                 all } (j, j') \in J, k, l \in \range{1}{m'}}	\\
               & \geq \prob{\vec{i_j} = \vec{i_{j'}} \land  \vec{i_j}[k] \neq 0
                 \text{ for all } (j, j') \in J,  k \in \range{1}{m'}} \\
               & \quad - \prob{\vec{i_j}[k] \neq 0 \land
                 h_{\vec{i_j}[l]}^{j} = h_{\vec{i_{j'}}[l]}^{j'} \text{ for all
                 } k \in \range{1}{m'} \text{ and for some
                 } (j, j') \in J, l \in \range{1}{m'}}\\
               & = \prob{\vec{i_j} = \vec{i_{j'}} \land \vec{i_j}[k] \neq 0
                 \text{ for all } (j, j') \in
                 J,  k \in \range{1}{m'}} - \\
               & \quad  \prob{\vec{i_j}[k] \neq 0 \text{ for all } k \in \range{1}{m'}} \cdot 
                 \left(1 - \frac{h!}{(h - m m')! \cdot h^{m m'}}\right) \\ 
               & = \prob{\vec{i_j} = \vec{i_{j'}} \land
                 \vec{i_j}[k] \neq 0 \text{ for all 
                 } (j, j') \in J,  k \in \range{1}{m'}} - \\
               & \quad \accProb(y) \cdot \left(1 -
                 \frac{h!}{(h - m m')! \cdot h^{m m'}}\right)\,.
\end{align*}

Probability that for some $(j, j') \in J$, $k \in \range{1}{m'}$ and $\vec{i_j} = \vec{i_{j'}}$ holds
$h_{\vec{i_j}[k]}^{j} \neq h_{\vec{i_{j'}}[k]}^{j'}$, equals 
\[
  \frac{h \cdot (h - 1)
\cdot \ldots \cdot (h - m m' - 1)}{h^{m m'}} = \frac{h!}{(h - m m')! \cdot h^{m m'}}.
\]
That is, it equals the number
of all $m m'$-element strings where each element is different divided by
the number of all $m m'$-element strings, where elements are taken from a
set of size $h$. 

It remains to show that
\[
  \prob{\vec{i_j} = \vec{i_{j'}} \land \vec{i_j}[k] \neq 0 \text{ for all } (j,
    j') \in J, k \in \range{1}{m'}} \geq \infrac{\accProb(y)^m}{q^{m - 1}}.
[]\]
Let $\RND{\zdv}$ denote the set from which $\zdv$ picks its coins at random. For
each $\vec{\iota} \in \range{1}{q}^{m'}$ let $X_{\vec{\iota}} \colon \RND{\zdv}
\times H^{\vec{\iota}[1] - 1} \to [0, 1]$ be defined by setting
$X_{\vec{\iota}}(\rho, h_1, \ldots, h_{\vec{\iota}[1] - 1})$ to
\[
  \condprob{\vec{i} = \vec{\iota}}{h_{\vec{\iota}[1]}, \ldots, h_q \sample H;
    (\vec{i}, s) \gets \zdv(y, h_1, \ldots, h_q; \rho)}
\] 
for all $\rho \in \RND{\zdv}$ and $h_1, \ldots, h_{\vec{\iota}[1] - 1} \in
H$. Consider $X_{\vec{\iota}}$ as a random variable over the uniform
distribution on its domain.  Then
\begin{align*}
  & \prob{\vec{i_j} = \vec{i_{j'}} \land \vec{i_j}[k] \neq 0 \text{ for
    all } (j, j') \in J, k \in \range{1}{m'}} 
    = \sum_{\vec{\iota} \in \range{1}{q}^{m'}} \prob{\vec{i_1} = \vec{\iota} \land \ldots \land \vec{i_m} = \vec{\iota}} \\
  & = \sum_{\vec{\iota} \in \range{1}{q}^{m'}} \prob{\vec{i_1} = \vec{\iota}} \cdot \condprob{\vec{i_2} = \vec{\iota}}{\vec{i_1} = \vec{\iota}} \cdot \ldots \cdot \condprob{\vec{i_m} = \vec{\iota}}{\vec{i_1} = \ldots = \vec{i_{m - 1}} = \vec{\iota}} \\
  & = \sum_{\vec{\iota} \in \range{1}{q}^{m'}} \sum_{\rho, h_1, \ldots, h_{\vec{\iota}[1] - 1}} X_{\vec{\iota}}
    (\rho, h_1, \ldots, h_{\vec{\iota}[1] - 1})^{m} \cdot \frac{1}{\abs{\RND{\zdv}} \cdot \abs{H}^{\vec{\iota}[1] - 1}}
    = \sum_{\vec{\iota} \in \range{1}{q}^{m'}} \expected{X_{\vec{\iota}}^m} \,.
\end{align*}
Importantly, $\sum_{\vec{\iota} \in \range{1}{q}^{m'}} \expected{X_{\vec{\iota}}} = \accProb(y)$.

By \cref{lem:jensen} we get
\[
	\sum_{\vec{\iota} \in \range{1}{q}^{m'}} \expected{X_{\vec{\iota}}^m} \geq \sum_{\vec{\iota} \in \range{1}{q}^{m'}} \expected{X_\iota}^m\,.
\]
Note that for e.g.~$X_{\vec{i}} = 1$, $\vec{i} \in \range{1}{q}^{m'}$ the
inequality becomes equality, that is, it is tight.
 
We now use the H\"older inequality, cf.~\cref{lem:holder}, for
$x_{\vec{i}} = \expected{X_{\vec{i}}}$, $y_{\vec{i}} = 1$, $p = m$, and
$q = m/(m - 1)$ obtaining
\begin{gather}
  \left(\sum_{\vec{\iota} \in \range{1}{q}^{m'}} \expected{X_{\vec{i}}}\right)^{m}  \leq \left(\sum_{\vec{\iota} \in \range{1}{q}^{m'}} \expected{X_{\vec{i}}}^m\right) \cdot q^{m'(m - 1)}\\
  \frac{1}{q^{m'(m - 1)}} \cdot \accProb(y)^{m} \leq \sum_{\vec{\iota} \in
    \range{1}{q}^{m'}} \expected{X_{\vec{i}}}^m\,.
\end{gather}
Finally, we get
\[
	\frkProb(y) \geq \frac{\accProb(y)^m}{q^{m'(m - 1)}} - 
	 \accProb(y) \cdot \left(1 - \frac{h!}{(h - m m')! \cdot h^{m m'}}\right)\,.
\]
\qed
\end{proof}

To highlight importance of the generalised forking lemma we describe how we use
it in our simulation-extractability proof.  Let $\proofsystem$ be a
computationally special sound proof system where for an instance $\inp$ the
corresponding witness can be extracted from an $(n_1, \ldots, n_\mu)$-tree of
accepting transcripts, where $m'$ elements $n_j$, $j \in \range{1}{\mu}$, are
bigger than $1$. Denote the set of indices $j$ such that $n_j > 1$ by $B$. Let
$\advse$ be the simulation-extractability adversary that outputs an acceptable
proof with probability at least $\accProb$. (Although we use the same $\accProb$
to denote probability of $\zdv$ outputing a $\vec{i}$ which has no $0$ entry and
probability of $\advse$ outputing an acceptable proof we claim that these
probabilities are exactly the same what comes from how we define $\zdv$.)  Let
$\advse$ produce an acceptable proof $\zkproof_{\advse}$ for instance
$\inp_{\advse}$; $r$ be $\advse$'s randomness; $Q$ the list of queries submitted
by $\advse$ along with simulator's $\simulator_\zkproof$ answers; and $Q_\ro$ be
the list of all random oracle queries made by $\advse$.  All of these are given
to the extractor $\ext$ that internally runs the forking algorithm
$\genforking_\zdv$.  Algorithm $\zdv$ takes $(\REL, \srs, \advse, Q, r)$ as
input $y$ and $Q_\ro$ as input $h_1^1, \ldots, h_q^1$.  (For the sake of
completeness, we allow $\genforking_\zdv$ to pick $h^1_{k + 1}, \ldots, h^1_q$
responses if $Q_\ro$ has only $k < q$ elements.)

Next, $\zdv$ runs internally $\advse(\REL, \srs; r)$ and responds to its random
oracle and simulator queries by using $Q_\ro$ and $Q$. Note that $\advse$ makes
the same queries as it did before it output $(\inp_{\advse}, \zkproof_{\advse})$
as it is run on the same random tape and with the same answers from the
simulator and random oracle. After $\advse$ finishes its acceptable proof
$\zkproof_{\advse}$, algorithm $\zdv$ outputs $(\vec{i}, \zkproof_{\advse})$,
where $\vec{i}$ is the vector of indices of random oracle queries that make
challenges after $k$-th messages from the prover, for $k \in B$. Then, after the
first run of $\advse$ is done, the extractor runs $\zdv$ again, but this time it
provides a fresh random oracle responses $h^2_{\vec{i}[1]}, \ldots, h^2_q$. Note
that this is equivalent to rewinding $\advse$ to a point just before $\advse$ is
about to ask its $\vec{i}[1]$-th random oracle query. (We rewind to a point
where the tree of accepting transcripts branches for the first time.)
Probability that the adversary produces an acceptable transcript with the fresh
random oracle responses is at least $\accProb$. This continues until the
required number of transcripts is obtained.

We note that in the original forking lemma the forking algorithm $\forking$
cf.~\cref{fig:genforking_lemma_gen} gets only as input $y$ and elements $h^1_1, \ldots,
h^1_q$ are randomly picked from $H$ internally by $\forking$. However, assuming
that $h^1_1, \ldots, h^1_q$ are random oracle responses, thus are random, makes
the change only notational.

We also note that the general forking lemma proposed in
\cref{lem:generalised_forking_lemma_gen} is more general than we actually need to
show simulation extractability of $\plonk$ and $\sonic$. More precisely, it
would be enough to show its variant which allows for extraction from
$(1, \ldots, 1, n_k, 1, \ldots, 1)$-tree of acceptable transcripts, i.e.~for $m'
= 1$. Setting $m' = 1$ also considerably improves the probability bounds for
$\frkProb$, which becomes
\[
  \frkProb = \frac{\accProb^m}{q^{m - 1}} - \accProb \cdot \left(1 -
    \frac{h!}{(h - m)! \cdot h^m}\right)\enspace.
\]

\section{Simulation-extractability---the even more general result}

\begin{theorem}[Simulation-extractable multi-message protocols]
\label{thm:se}
Let $\ps = (\kgen, \prover, \verifier, \simulator)$ be an interactive
$(2 \mu + 1)$-message proof system for $\RELGEN(\secparam)$ that is honest
verifier zero-knowledge in the standard model\footnote{Crucially, we require
  that one can provide an indistinguishable simulated proof without any
  additional knowledge, as e.g~knowledge of a SRS trapdoor.}, has $\ur{k}$
property with security $\epsur$, is $(n_1, \ldots, n_\mu)$-special
sound and $m'$ elements from $\smallset{n_1, \ldots, n_\mu}$ are not $1$.
Let $\ro\colon \bin^{*} \to \bin^{\secpar}$ be a random oracle. 
Then $\psfs$ is simulation-extractable with extraction error $\epsur$
against $\ppt$ algebraic adversaries that makes up to $q$ random oracle queries and
returns an acceptable proof with probability at least $\accProb$. 
The extraction probability $\extProb$ is at least
\[
	\extProb \geq \frac{1}{q^{m'(m - 1)}} (\accProb - \epsur)^{m' m} -\eps\,,
\]
for $m = \prod_{k = 1}^{\mu} n_k$ and some negligible $\eps$.	
\end{theorem}
\begin{proof}		
  The proof goes by game hoping. The games are controlled by an environment
  $\env$ that internally runs a simulation extractability adversary $\advse$,
  provides it with access to a random oracle and simulator, and when necessary
  rewinds it. The games differ by various breaking points, i.e.~points where the
  environment decides to abort the game.

  Denote by $\zkproof_{\advse}, \zkproof_{\simulator}$ proofs returned by the
  adversary and the simulator respectively. We use $\zkproof[i]$ to denote
  prover's message in the $i$-th round of the proof, i.e.~$(2i - 1)$-th message
  exchanged in the protocol. $\zkproof[i].\ch$ denotes the challenge that is
  given to the prover after $\zkproof[i]$, and $\zkproof\range{i}{j}$ to denote
  all messages of the proof including challenges between rounds $i$ and $j$, but
  not challenge $\zkproof[j].\ch$. When it is not explicitly stated we denote
  the proven instance $\inp$ by $\zkproof[0]$ (however, there is no following
  challenge $\zkproof[0].\ch$).

  Without loss of generality, we assume that whenever the accepting proof
  contains a response to a challenge from a random oracle, we assume that the
  adversary queried the oracle to get it. It is straightforward to transform any
  adversary that violates this condition into an adversary that makes these
  additional queries to the random oracle and wins with the same probability.

  \ngame{0} This is a simulation extraction game played between an adversary
  $\advse$ who has given access to a random oracle $\ro$ and simulator
  $\psfs.\simulator$. There is also an extractor $\ext$ that, from the proof
  $\zkproof_{\advse}$ for instance $\inp_{\advse}$ output by the adversary and
  from a transcript of $\advse$'s operations, is tasked to extract a witness
  $\wit_{\advse}$ such that $\REL(\inp_{\advse}, \wit_{\advse})$ holds. $\advse$
  wins if it manages to produce an acceptable proof and the extractor fails to
  reveal the corresponding witness. In the following game hops we upper-bound
  the probability that this happens.

  \ngame{1} This is identical to $\game{0}$ except that now the game is aborted
  if there is a simulated proof $\zkproof_\simulator$ for $\inp_{\advse}$ such
  that $(\inp_{\advse}, \zkproof_\simulator\range{1}{k}) = (\inp_{\advse},
  \zkproof_{\advse}\range{1}{k})$. That is, the adversary in its final proof
  reuses a part of a simulated proof it saw before and the proof is acceptable.
  Denote that event by $\event{\errur}$.

  \ncase{$\game{0} \mapsto \game{1}$} We have, \( \prob{\game{0} \land
    \nevent{\errur}} = \prob{\game{1} \land \nevent{\errur}} \) and, from the
  difference lemma, cf.~\cref{lem:difference_lemma} \( \abs{\prob{\game{0}} -
    \prob{\game{1}}} \leq \prob{\event{\errur}}\,. \) Thus, to show that the
  transition from one game to another introduces only minor change in
  probability of $\advse$ winning it should be shown that
  $\prob{\event{\errur}}$ is small.

  Assume that $\advse$ queried the simulator on the instance it wishes to output
  $\inp_{\advse}$. We show a reduction $\rdvur$ that utilises $\advse$, who
  outputs a valid proof for $\inp_{\advse}$, to break the $\ur{k}$ property of
  $\ps$. Let $\rdvur$ run $\advse$ internally as a black-box:
  \begin{itemize}
  \item The reduction answers both queries to the simulator $\psfs.\simulator$
    and to the random oracle. It also keeps lists $Q$, for the simulated proofs,
    and $Q_\ro$ for the random oracle queries.
  \item When $\advse$ outputs a fake proof $\zkproof_{\advse}$ for
    $\inp_{\advse}$, $\rdvur$ looks through lists $Q$ and $Q_\ro$ until it finds
    $\zkproof_{\simulator}[0..k]$ such that $\zkproof_{\advse}[0..k] =
    \zkproof_{\simulator}[0..k]$ and a random oracle query
    $\zkproof_{\simulator}[k].\ch$ on $\zkproof_{\simulator}[0..k]$.
  \item $\rdvur$ returns two proofs for $\inp_{\advse}$:
    \begin{align*}
      \zkproof_1 = (\zkproof_{\simulator}[1..k],
      \zkproof_{\simulator}[k].\ch, \zkproof_{\simulator}[k + 1..\mu + 1])\\
      \zkproof_2 = (\zkproof_{\simulator}[1..k],
      \zkproof_{\simulator}[k].\ch, \zkproof_{\advse}[k + 1..\mu + 1])
    \end{align*}
  \end{itemize}
  If $\zkproof_1 = \zkproof_2$, then $\advse$ fails to break simulation
  extractability, as $\zkproof_2 \in Q$. On the other hand, if the proofs are
  not equal, then $\rdvur$ breaks $\ur{k}$-ness of $\ps$, what may happen with
  some negligible probability $\epsur$ only, hence \( \prob{\event{\errur}} \leq
  \epsur\,. \)
	
  \ngame{2} This is identical to $\game{1}$ except that now the environment
  aborts also when it fails to build a $(n_1, \ldots, n_\mu)$-tree of accepting
  transcripts $\tree$ by rewinding $\advse$. Denote that event by
  $\event{\errfrk}$. Note that for every acceptable proof $\zkproof_{\advse}$,
  we may assume that whenever $\advse$ outputs in Round $k$, a message
  $\zkproof_{\advse}[k]$, then $(\inp_{\advse}, \zkproof_{\advse}[1..k])$ random
  oracle query that was made by the adversary, not the
  simulator\footnote{\cite{INDOCRYPT:FKMV12} calls these queries \emph{fresh}.},
  i.e.~there is no simulated proof $\zkproof_\simulator$ on $\inp_\simulator$
  such that $(\inp_{\advse}, \zkproof_{\advse} [1..k]) = (\inp_\simulator,
  \zkproof_\simulator[1..k])$. Otherwise, the game would be already interrupted
  by the error event in Game $\game{1}$.

  \ncase{$\game{1} \mapsto \game{2}$} As previously, \( \abs{\prob{\game{1}} -
    \prob{\game{2}}} \leq \prob{\event{\errfrk}}\,. \) Denote by $\waccProb$ the
  probability that $\advse$ outputs a proof that is accepted and does not break
  $\ur{k}$-ness of $\ps$. Assuming that $\advse$ does not break the unique
  response property is necessary to be able to use the forking lemma in which
  $\waccProb'$ corresponds to the probability of an algorithm $\zdv$ producing
  an accepting proof with a fresh challenge in round $k$. Otherwise, if
  $\ur{k}$-ness does not hold and the adversary could, say, rerandomise a proof
  $\zkproof$ it obtained from the simulator then it could force $\zdv$ to never
  accept. The unique response property takes care of that. If an adversary would
  like to reuse a simulated proof it saw, it would end up with exactly the same
  proof which can also can not break simulation extractability, thus
  $\waccProb=\waccProb'$.

  We describe our extractor here. The extractor takes as input relation $\REL$,
  SRS $\srs$, $\advse$'s code, its randomness $r$, the output instance
  $\inp_{\advse}$ and proof $\zkproof_{\advse}$, as well as the list $Q$ of
  simulated proofs (and their instances) and the list of random oracle queries
  and responses $Q_\ro$. Then, $\ext$ starts a forking algorithm
  $\genforking^{N}_\zdv(y,h_1, \ldots, h_q)$ for $y = (\REL, \srs, \advse, r,
  \inp_{\advse}, \zkproof_{\advse}, Q)$ where we set $h_1, \ldots, h_q$ to be
  the consecutive queries from list $Q_\ro$. We run $\advse$ internally in
  $\zdv$.

  To assure that in the first execution of $\zdv$ the adversary $\advse$ produce
  the same $\inp_{\advse}, \zkproof_{\advse}$ as in the extraction game, $\zdv$
  provides $\advse$ with the same randomness $r$ and answers queries to the
  random oracle and simulator with responses pre-recorded responses in $Q_\ro$
  and $Q$.
%
  Note, that since the view of the adversary run inside $\zdv$ is the same as
  its view with access to real random oracle and simulator, it produces exactly
  the same output. After the first run, $\zdv$ outputs a vector $\vec{i}$ of
  indices of random oracle queries used by $\advse$ to compute challenges
  $\zkproof_{\advse}[i[j]].\ch = \ro(\zkproof_{\advse}[0..i[j]])$, for $j \in
  B$, and adversary's transcript, in $\genforking$ description denoted by $s_1$.
  Then new random oracle responses are picked and the adversary is rewound to
  the point just prior the response to RO query $(\zkproof_{\advse}[0..i[1]])$.
  The adversary gets a random oracle response from a new set of responses
  $h^2_{i[1]}, \ldots, h^2_q$. If the adversary requests a simulated proof after
  seeing $h^2_{i[1]}$ then $\zdv$ computes the simulated proof on its own.
  Eventually, $\zdv$ outputs another vector $\vec{i'}$ of queries used by the
  adversary to compute $\zkproof_{\advse}[i'[j]].\ch =
  \ro(\zkproof_{\advse}[0..i'[j]])$, for $j \in B$, and a new transcript form
  the adversary $s_2$. This way, $\zdv$ is run with different random oracle
  responses till $m$ valid transcripts are collected.

If the unique response property does not hold, then in the $l$-th execution of
$\zdv$ the adversary could reuse a challenge that it learnt from observing
proofs in $Q$. In that case, $\zdv$ would not be able to output $\vec{i}$ with
no zero entries, what would make extractor fail.

From the generalised forking lemma, cf.~\cref{lem:generalised_forking_lemma_gen},
\[ \prob{\event{\errfrk}} \leq 1 - \waccProb \cdot \left(\frac{\waccProb^{m -
        1}}{q^{m'(m - 1)}} + \frac{(2^\secpar) !}{(2^\secpar - m' m)! \cdot
      (2^\secpar)^{m' m}} - 1\right)\,.
\]
For the sake of simplicity we loose this approximation a bit and state
\[
	\prob{\event{\errfrk}} \leq 1 -
	\left(\frac{\waccProb^{m}}{q^{m' (m - 1)}} +
		\waccProb \cdot \left(\frac{2^\secpar - m' m}{2^\secpar}\right)^{m' m} -
	\waccProb\right)\,.
\]
\ngame{3} This is identical to $\game{2}$ except that now the environment uses
the tree $\tree$ to extract the witness for the proven statement and aborts when
it fails. Denote that event by $\event{\errss}$.

\ncase{$\game{2} \mapsto \game{3}$}	
As previously, 
\(
	\abs{\prob{\game{2}} - \prob{\game{3}}} \leq \event{\errss}\,.
	\)
Since $\ps$ is special-sound the probability that $\env$ fails in extracting
the witness is upper-bounded by some negligible $\eps_\ss$.

In the last game, Game $\game{3}$, the environment aborts when it fails to
extract the correct witness, hence the adversary $\advse$ cannot win.  Thus,
by the game-hoping argument, 
\[
	\abs{\prob{\game{0}} - \prob{\game{3}}} \leq 1 -
	\left(\frac{\waccProb^{m}}{q^{m' (m - 1)}} + \waccProb \cdot
	\left(\frac{2^\secpar - mm'}{2^\secpar}\right)^{m m'} - \waccProb\right) + \epsur + \epsss\,.
\]
Thus the probability that extractor $\extss$ succeeds is at least
\[
	\frac{\waccProb^{m}}{q^{m' (m - 1)}} + 
	\waccProb \cdot
	\left( \frac{2^\secpar - m' m}{2^\secpar}\right)^{m' m} -
\waccProb - \epsur - \epsss\,.
\]
Since $\waccProb$ is probability of $\advse$ outputting acceptable transcript
that does not break $\ur{k}$-ness of $\ps$, then $\waccProb \geq \accProb -
\epsur$, where $\accProb$ is the probability of $\advse$ outputing an acceptable
proof as defined in \cref{def:simext}. It thus holds
\[
	\label{eq:frk}
	\extProb \geq \frac{(\accProb - \epsur)^{m}}{q^{m' (m - 1)}} -
	\underbrace{(\accProb - \epsur) \cdot \left( 1 -
		\left(\frac{2^\secpar - m' m}{2^\secpar}\right)^{m' m}\right)
- \epsur - \epsss}_{\eps}\,.
\]
Note that the part of \cref{eq:frk} denoted by $\eps$ is negligible as
$\epsur, \epsss$ are negligible, and $\left(\infrac{(2^\secpar
- m' m)}{2^\secpar}\right)^{m' m}$ is overwhelming.
Thus, 
\[
	\extProb \geq \frac{1}{q^{m' (m - 1)}} (\accProb - \epsur)^{m} -\eps\,.
\] 
thus
$\psfs$ is simulation extractable with extraction error $\epsur$.
\qed
\end{proof}

\section{On Bootle et al.~\cite{EC:BCCGP16}}
\newcommand{\treefind}{\pcalgostyle{tFind}}
\newcommand{\findwitness}{\pcalgostyle{wFind}}
\newcommand{\validtree}{\pcalgostyle{VT}}

\begin{figure}
  \centering
  \fbox{
    \procedure{$\ext_\adv(\REL, \srs, \inp; r)$}{
      h_1, \ldots, h_1 \sample H^{q}\\
      (\inp, \zkproof, \tree) \gets \treefind(1, (h_1, \ldots, h_q))\\
      \pcif \validtree(\tree) = 0 \pcthen \\
      \pcind \pcreturn (\inp, \zkproof, \bot)\\
      \pcelse \wit \gets \findwitness(\REL, \srs, \inp, \tree) \\
      \pcreturn \wit
    }
  }
  \fbox{
    \procedure[linenumbering]{$\treefind(i, (h_1, \ldots, h_q))$}{
      \pcif i = \mu + 1 \pcthen\\
      \pcind (\inp, \zkproof) \gets \adv^{\ro}(\REL, \srs)\\
      \pcind \pcif \verifier(\REL, \srs, \inp, \zkproof) = 1 \pcthen\\
      \pcind \pcind \pcreturn (\inp, \zkproof, \tree)\\
      \pcind \pcelse \pcreturn (\inp, \zkproof, \bot)\\
      %
      \text{run $\adv$ up to and including its $(i + 1)$-th message}\\
       (\inp, \zkproof, \tree) \gets \treefind(i + 1, (h_1, \ldots, h_q))\\
      \pcif \tree = \bot \pcthen \pcreturn (\inp, \zkproof, \bot)\\
      \counter \gets 1\\
      \pcwhile \counter < n_i\\
      \pcind \text{rewind $\adv$ just before it computes the $i$-th challenge}\\
      \pcind \text{let $j$ be index of a random oracle query used by $\adv$ to
        compute the $i$-th challenge}\\
      \pcind (h'_{j}, \ldots, h'_q) \sample H^{q - j + 1}\\ 
      \label{line:branching}\pcind(\inp, \zkproof', \tree') \gets \treefind(i + 1, (h_1, \ldots, h_{j
        - 1}, h'_j, \ldots, h'_q))\\
      \pcind \pcif \tree' \neq \bot \pcthen \\
      \pcind \pcind \tree \gets (\tree, \tree') \pccomment{append $\tree'$ to $\tree$}\\
      \pcind \pcind \counter \gets \counter + 1\\
      \pcreturn (\inp, \zkproof, \tree)
    }
  }
  \caption{Extractor and tree finding algorithm. $\validtree(\tree)$ is
    an algorithm that takes an $(n_1, \ldots, n_\mu)$ tree of transcripts and
    returns $1$ if it is valid and $0$ otherwise. $\findwitness(\tree)$ is an
    algorithm that given a valid $(n_1, \ldots, n_\mu)$-tree of accepting
    transcripts for statements $\inp$ extracts a witness $\wit$ such that
    $\REL(\inp, \wit) = 1$.
  }
  \label{fig:extractor_and_tree_finding_algorithm}
\end{figure}
In \cite{EC:BCCGP16} Bootle et al.~proposed a novel inner-product argument. In
its security proof the authors shown protocol's witness-extended emulation
property. To that end, their proposed a new version of forking lemma, where they
analysed probability that a tree finding algorithm is able to produce a required
tree of acceptable transcripts by rewinding a conversation between a
(potentially malicious) prover and verifier.

Although the result of presented in that paper is dubbed ``forking lemma'' it
differs from forking lemmas known from e.g.~\cite{JC:PoiSte00,CCS:BelNev06}.
First of all, the forking lemmas in these papers analyse probability of building
a tree of acceptable transcripts for Fiat--Shamir based non-interactive proof
systems, while the protocol presented by Bootle et al.~is intended to work for
interactive proof systems.

Importantly, it is not obvious how the result of Bootle et al.~can be used to
show security of non-interactive protocols. At
\cref{fig:extractor_and_tree_finding_algorithm} we present a ``natural''
approach of making Bootle et al.'s extractor and tree finder work for a
non-interactive proof system.

Unfortunately, there is a crucial problem with such approach. Namely, the
adversary is not fixed to show a proof for any concrete instance and can change
the proven instance on the fly, e.g.~after seeing a number of random oracle
responses. More precisely, at line \ref{line:branching} of
\cref{fig:extractor_and_tree_finding_algorithm} the first branch of the tree is
already built and the algorithm tries to find another one by rewinding the
adversary and presenting it a new set of challenges. The issue is that when
$\adv$ sees a challenge $h'_j$ then it may decide to start the proof over
\emph{with a new instance}. In that case the resulting proof cannot be joined to
the existing one (to make a tree) simply because it is for a different instance.
Obviously, the adversary cannot change the instances indefinitely, so there is a
chance that the tree will be finally build; however, it is far from obvious and
requires additional analysis.

Alternatively, the adversary may not pick a new instance after seeing $h'_j$,
but ``rewinds'' its own proof. Say, that $\adv$ has prepared first $k$ messages
and learns a new $h'_j$. Then it may discard some number of the last messages,
say save the first $k'$, $k' < k$, messages and pick its $(k' + 1)$-th message
differently and continues computation of the proof. In this scenario, the tree
building algorithm fails as well as it was prepared to branch the protocol at
point $k$ while the adversary made a new proof that branches from the previous
at $k'$.

The generalised forking lemma, cf.~\cref{lem:generalised_forking_lemma},
proposed in this paper solves the issues above, however for a specific family of
protocols where witnesses can be extracted from trees of acceptable transcripts
that branch at single point only -- say that the tree branches after adversary's
$k$-th message (where $k \leq \mu$ and the proof can be divided into $\mu$
adversary's messages and corresponging ``challenges''). In the proposed lemma
(following approach of \cite{CCS:BelNev06}) we analyse probability that the
(possibly malicious) prover $\adv$ picks the same index of a random oracle in
all runs of the rewound protocol where in each run $\adv$ is presented with
fresh random oracle responses. This approach secures the required result. Assume
that we guessed correctly that $i$-th random oracle query made by the adversary
was used to compute its $k$-th challenge and denote by $\inp$ the instance which
the adversary is proving and by $\zkproof[1..k]$ a partial proof that the
adversary computed prior to seeing $h_i$. Then an adversary who outputs an
instance $\inp$ and its valid proof $\zkproof$ cannot change the instance or
$\zkproof[1..k]$ after $h_i$ has been seen as that would require finding a
collision in the random oracle. That is, the new instance $\inp'$ and proof
$\zkproof'$ would have to be that $h_i = \ro(\inp, \zkproof[1..k]) = \ro(\inp',
\zkproof'[1..k])$, where either $\inp \neq \inp'$ or $\zkproof[1..k] \neq
\zkproof'[1..k]$. Otherwise, computing $\ro(\inp', \zkproof'[1..k])$ would
require another random oracle query, falsifying the claim that we guessed the
index for the $k$-th proof challenge correctly.


% This problem could be solved as follows. \michals{18.2}{Describe how guessing
%   the first adversary's ro query can solve the problem.} \michals{18.2}{Maybe we
%   could guess only the first ro query?! That would be splendid.}

% % Importantly, the result of Bootle et al.~does not analyse security of
% % Fiat--Shamir transformation applied to the original interactive protocol.
% % Obviously, one could just assume that the Fiat--Shamir transformation is secure,
% % however it remains unclear how much security loss it introduces.

% For example consider a (folklore, e.g.~see~\cite{JC:PoiSte00}) reduction $\rdv$
% showing soundness of a Fiat--Shamir transformed non-interactive proof
% $\proofsystem_\fs$ relying on soundness of its interactive version
% $\proofsystem$. The reduction breaks soundness of $\proofsystem$ by running an
% adversary $\adv$ who successfully breaks $\proofsystem_\fs$. Say that
% $\proofsystem$ compounds of $(2\mu + 1)$ messages and, for an instance $\inp$,
% the corresponding witness can be extracted from a $(n_1, \ldots, n_\mu)$-tree of
% interactive transcripts. Let $q$ be the total number of random oracle queries
% asked by $\adv$. Reduction $\rdv$ proceeds as follows.

% First it guesses indices $(i_1, \ldots, i_\mu)$ of $\adv$'s random oracle queries
% used as challenges to a proof $\zkproof$ output by the adversary. Then it picks
% randomly $q$ random oracle responses $h_1, \ldots, h_q$.

% Next the reduction starts its interaction with $\proofsystem.\verifier$ playing
% the role of a prover, and with the adversary $\adv$ providing it with the random
% oracle responses; that is, on $i$-th random oracle query from the adversary the
% reduction responds with $h_i$, except for $i \in \smallset{i_1, \ldots, i_\mu}$.
% When the adversary provides instance $\inp$ it tries to prove, the same instance
% is provided to $\proofsystem.\verifier$. When the adversary sends its $i_j$-th
% random oracle query the reduction passes the query to the verifier as its $j$-th
% message. The $j$-th challenge from the verifier is sent back to the adversary as
% the random oracle response. In the end, the successful adversary returns a valid
% proof for an invalid statement, that way, the reduction convinces the
% $\proofsystem$'s verifier on a false statement.

\section{(Tight) simulation soundness of $\plonk$}
\begin{theorem}[Simulation soundness]
  Assume that $(\noofc + 2, 1)$-$\dlog$ is
  $\eps_\dlog(\secpar)$-hard, $\plonkprot$ is sound and $\ur{2}$ with security
  $\epss(\secpar)$ and $\epsur(\secpar)$ respectively. Then the probability that
  an algebraic, $\ppt$ adversary $\advss$ breaks simulation soundness of
  $\plonkprotfs$ is upper-bounded by
  \[
    \epsur(\secpar) + q_\ro^6 (\eps_{\dlog}(\secpar) + \epss(\secpar))\,,
  \]
  where $q_\ro$ is the total number of queries required by the adversary
  $\advss$.
\end{theorem}
\begin{proof}
  We proceed by contradiction. Suppose there exists a $\ppt$ adversary $\advss$
  that breaks simulation soundness with non-negligible probability
  \[
    \eps := \Pr \left[
      \begin{aligned}
        & \plonkprotfs.\verifier(\REL, \srs, \inp, \zkproof_{\advss}),\\
        & (\inp_{\advss}, \zkproof_{\advss}) \not\in Q,\\
        & \inp_\advss \not\in \LANG_\REL
      \end{aligned}
      \,\left|\, \vphantom{\begin{aligned}
            & \plonkprotfs.\verifier(\REL, \srs, \inp, \zkproof_{\advss}),\\
            & (\inp_{\advss}, \zkproof_{\advss}) \not\in Q,\\
            & \inp_\advss \not\in \LANG_\REL
          \end{aligned}}
        \begin{aligned}
          & \srs \gets \plonkprotfs.\kgen(\REL, \secparam)\\
          & (\inp_{\advss}, \zkproof_{\advss}) \gets \advss^{\simulator, \ro}
          (\REL, \srs),
        \end{aligned}
      \right.\right].
  \]

  In such case, we are able to build reductions $\rdvs$, $\rdvur$, $\rdvdlog$
  which using $\advss$ as a black-box, violate either the soundness, unique
  response properties of the underlying interactive protocol $\plonkprot$, or
  the $(\noofc + 2, 1)$-$\dlog$ assumption.

  In the following we denote by $\zkproof_{\advss}, \zkproof_{\simulator}$
  proofs returned by the adversary and the simulator respectively. We use
  $\zkproof[i]$ to denote prover's message in the $i$-th round of the proof,
  $\zkproof[i].\ch$ to denote the challenge that is given to the prover after
  $\zkproof[i]$, and $\zkproof\range{i}{j}$ to denote all messages of the proof
  including challenges between rounds $i$ and $j$.

  Without loss of generality, we assume that whenever the accepting proof
  contains a response to a challenge from a random oracle, we assume that the
  adversary queried the oracle to get it. It is straightforward to transform any
  adversary that violates this condition into an adversary that makes these
  additional queries to the random oracle and wins with the same probability.

  A crucial observation is that the adversary $\advss$ may have learned
  $\zkproof_{\advss}\range{1}{3}$ by querying the simulator on $\inp_{\advss}$
  or might have computed it itself. We denote the first event by $\event{E}$ and
  the second by $\nevent{E}$.
	%
  Additionally, we divide event $\nevent{E}$ into two disjunctive subevents:
  $\nevent{E}_0$ and $\nevent{E}_1$. Event $\nevent{E}_0$ considers a case when
  the final proof provided by the adversary $\advss$ is accepted by the
  idealised verification equation, i.e.~for that proof $\vereq(X) = 0$.
  Alternatively, event $\nevent{E}_1$ covers a case when for $\zkproof_\advss$
  it holds that $\vereq(\chi) = 0$, but $\vereq(X) \neq 0$, where $\chi$ is
  $\plonkprotfs$'s trapdoor.
	%
  As all these events are mutually exclusive and exhaustive, we have
  \[
    \eps = \prob{\advss \text{ wins}} = \prob{\advss \text{ wins}, \event{E}} +
    \prob{\advss \text{ wins}, \nevent{E}_0} + \prob{\advss \text{ wins},
      \nevent{E}_1}\,.
  \]

  Before analysing the events, we make the following observation. First of all,
  we allow reductions $\rdv_\dlog, \rdvur, \rdvs$ to simulate the random oracle
  and simulator for the adversary $\advss$. We argue that since the reductions
  in their simulation behaves as real random oracle or simulator would, the
  chances the adversary breaks simulation soundness does not change.

  Furthermore, we note that since $\advss$ is algebraic, it outputs a proof
  $\zkproof_\advss$ that can be written as
  \[
    \zkproof_\advss = \vec{M} \cdot (\underbrace{1 \| \chi \| \ldots \|
      \chi^{\noofc + 2}}_{\srs} \|
    \vec{\tilde{\zkproof}_\simulator}_1^{\top} \| \ldots \|
    \vec{\tilde{\zkproof}_\simulator}_{q_\simulator}^\top)^\top\,,
  \]
  where $\gone{1, \chi, \ldots, \chi^{\noofc + 2}}$ are all
  $\GRP_1$-elements from the SRS of $\plonkprotfs$, $\vec{M}$ is a matrix of
  coefficients output by $\advss$ aside the proof,
  $\vec{\tilde{\zkproof}_\simulator}_i$ denote all $\GRP_1$-elements from the
  simulated proof ${\zkproof_\simulator}_i$, and the adversary makes
  $q_\simulator$ queries to the simulator.  Since the reduction itself provides
  the simulated proofs, it knows a matrix $\vec{M'}$ such that
  \begin{equation}
    \label{eq:M_prim}
    \zkproof_\advss = \vec{M'} \cdot (1 \| \chi \| \ldots \| \chi^{\noofc + 2})^\top\,.
  \end{equation}
  We use this property when analysing the success probability of reductions
  $\rdvs$ and $\rdvdlog$.

  Also note that a proof $\zkproof$ could be accepted only if the verification
  equation $\vereq(\chi)$ holds. That is, the verifier plugs-in elements of
  $\zkproof$ into $\vereq(\chi)$ and checks whether it equals $0$. That is what
  is called a \emph{real check} in \cite{EPRINT:GabWilCio19}.  On the other hand
  there is an \emph{idealised check}, which verifies whether $\vereq(X) = 0$
  \emph{as a polynomial}---with proof elements being polynomials as well.

  \ncase{When $\event{E}$ happens} We assume that $\inp_{\advss}$ is submitted
  to the simulator $\simulator$.  We show how $\rdvur$ utilizes $\advss$, that
  makes use of $\inp_\advss, \zkproof_{\advss}\range{1}{3}$, to break the
  $\ur{2}$ property of $\plonkprot$.  This way we bound the probability
  $\prob{\adv \text{ wins}, \event{E}}$ by the probability of $\rdvur$ being
  able to win in the $\ur{2}$ game.

  Consider an algorithm $\rdvur$ that runs $\advss$ internally as a black-box:
  \begin{itemize}
  \item The reduction answers both queries to the simulator
    $\plonkprotfs.\simulator$ and to the random oracle. It also keeps lists $Q$,
    for the simulated proofs, and $Q_\ro$ for the random oracle queries.
  \item When $\advss$ outputs a fake proof $\zkproof_{\advss}$ for
    $\inp_\advss$, $\rdvur$ looks through lists $Q$ and $Q_\ro$ until it finds
    $\zkproof_{\simulator}\range{1}{3}$ such that
    $\zkproof_{\advss}\range{1}{2} = \zkproof_{\simulator}\range{1}{3}$ and a
    random oracle query $\zkproof_{\simulator}[3].\ch$ on
    $\zkproof_{\simulator}\range{1}{3}$.
  \item $\rdvur$ returns two proofs for $\inp_\advss$:
    \begin{align*}
      \zkproof_1 = (\zkproof_{\simulator}\range{1}{3}, \zkproof_{\simulator}[3].\ch, \zkproof_{\simulator}\range{4}{5})\\
      \zkproof_2 = (\zkproof_{\simulator}\range{1}{3}, \zkproof_{\simulator}[3].\ch, \zkproof_{\advss}\range{4}{5})
    \end{align*}
  \end{itemize}
  If $\zkproof_1 = \zkproof_2$, then $\advss$ fails to break simulation
  extractability, as $\zkproof_2 \in Q$.  On the other hand, if the proofs are
  not equal, then $\rdvur$ breaks $\ur{2}$-ness of $\plonkprot$. Thus
  \[
    \prob{\advss \text{ wins}, \event{E}} \leq \epsur(\secpar).
  \]

  \ncase{When $\nevent{E}_0$ happens} In this case the reduction $\rdvs$ uses
  $\advss$ to break soundness of $\plonkprot$ with probability
  $\epss / q_{\ro}^6$, where $q_\ro$ is the number of total random oracle
  queries performed by the adversary or by $\rdvs$ on behalf of the
  simulator. As previously, $\rdvs$ runs $\advss$ internally and simulates its
  environment by answering to its queries to $\plonkprotfs.\simulator$ and
  $\ro$. The reduction works as follows:
  \begin{itemize}
  \item It guesses indices $i_1, \ldots, i_6$ such that random oracle queries
    $h_{i_1}, \ldots, h_{i_6}$ are the queries used in $\zkproof_\advss$. This
    is done with probability at least $1/q_{\ro}^6$ (since there are $6$
    challenges from the verifier in $\plonkprot$).
  \item On input $h$ for the $i$-th,
    $i \not\in \smallset{{i_1}, \ldots, {i_6}}$, random oracle query, $\rdvs$
    returns randomly picked $ y$, sets $\ro(h) = y $ and stores $(h, y)$ in
    $Q_\ro$ if $h$ is sent to $\ro$ the first time. If that is not the case,
    $\rdv$ finds $h$ in $Q_\ro$ and returns the corresponding $y$.
  \item On input $h_{i_j}$ for the $i_j$-th,
    $i_j \in \smallset{{i_1}, \ldots, {i_6}}$, random oracle query, $\rdvs$
    parses $h_{i_j}$ as a partial proof transcript $\zkproof[1..j]$ and runs
    $\plonkprot$ using $\zkproof[j]$ as a $\plonkprot.\prover$'s $j$-th message
    to $\plonkprot.\verifier$. The verifier responds with a challenge
    $\zkproof[j].\ch$. The reduction sets $\ro(h_{i_j}) = \zkproof[j].\ch$.
  \item On query $\inp_\simulator$ to $\simulator$ it runs a simulator
    $\plonkprotfs.\simulator$ internally and returns $\zkproof_\simulator$. If
    the random oracle query with input $\zkproof_\simulator[j]$,
    $1 \leq j \leq 2$, of the simulator is the $i_j$-th query, generate
    $\zkproof_\simulator[j].\ch$ by invoking $\plonkprot.\verifier$ on
    $\zkproof_\simulator[j]$ and programming
    $\ro(h_{i_j}) = \zkproof_\simulator[j].\ch$.
  \item Answers $\plonkprot.\verifier$'s challenge $\zkproof[j].\ch$ using the
    answer given by $\advss$, i.e.~$\zkproof_\advss[j + 1]$.
  \end{itemize}

  Assume that the $\plonkprot.\verifier$ accepts $\zkproof_\advss$. We consider
  a case when the idealised verification equation accepts. (Thus, the real
  verification accepts as well.) In that case $\rdvs$ extracts from $\vec{M'}$
  coefficients of $1, \chi, \ldots, \chi^{\noofc + 2}$ for
  polynomials $\p{a}(X), \p{b}(X)$, and $\p{c}(X)$ and reveals the witness
  $\wit_\advss$ (as it is encoded in theses polynomials' coefficients). If
  $\REL(\inp_\advss, \wit_\advss)$ holds then $\advss$ failed to break
  simulation-soundness of $\plonkprotfs$. On the other hand, if that is not the
  case, then $\rdvs$ breaks soundness of $\plonkprot$.
	%
  Since the reduction guesses queries $h_{i_1}, \ldots, h_{i_6}$ with
  probability $1/q_\ro^6$, then
  \[
    \prob{\rdvs \text{ wins}} = \prob{\advss \text{ wins}, h_{i_1}, \ldots,
      h_{i_6} \text{ are guessed correctly}, \nevent{E}_0}\,.
  \]
  Hence,
  \[
    \prob{\advss \text{ wins}, \nevent{E}_0} \leq q_\ro^6 \cdot \epsss(\secpar).
  \]

  \ncase{When $\nevent{E}_1$ happens} The reduction $\rdvdlog$ runs internally a
  protocol $\plonkprotfs$, which SRS is computed from the challenge
  $\gone{1, \chi, \ldots, \chi^{\noofc + 2}}, \gtwo{\chi}$ from the
  $(\noofc + 2, 1)$-$\dlog$ assumption challenger. Then it proceeds
  as $\rdvs$ does, except in the last part, when the adversary provided its
  proof $\zkproof_\advss$, $\rdvdlog$ uses the fact that the real verification
  equation holds, but the ideal verification equation does not to break the
  $\dlog$ assumption.

  Since $\vereq(X) \neq 0$, but $\vereq(\chi) = 0$ and $\rdvdlog$ knows
  $\vec{M'}$, as defined in \cref{eq:M_prim}, it can recreate all the
  polynomials submitted by $\advss$ as part of the proof and included in
  $\vereq(X)$. This way, it knows all coefficients of $\vereq(X)$. Thus it can
  factorise it and find its roots, one of them is the required $\chi$. Hence it
  holds, by the analogous analysis as in the previous case, that
  \[
    \prob{\advss \text{ wins}, \nevent{E}_1} \leq q_\ro^6 \cdot
    \eps_{\dlog}(\secpar).
  \]

  The proof is concluded by observing that the analysis of events
  $\event{E}, \nevent{E}_0, \nevent{E}_1$ gives
  \[
    \eps \leq \epsur(\secpar) + q_\ro^6 (\eps_{\dlog}(\secpar) +
    \epsss(\secpar))\,,
  \]
  hence $\eps$ is negligible if $\dlog$ is hard and $\plonkprot$ is sound and
  $\ur{2}$.  \qed
\end{proof}

\end{document}

